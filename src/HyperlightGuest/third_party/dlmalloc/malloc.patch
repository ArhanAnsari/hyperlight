diff --git a/src/HyperlightGuest/third_party/dlmalloc/malloc.c b/src/HyperlightGuest/third_party/dlmalloc/malloc.c
index 649cfbc..3b703b5 100644
--- a/src/HyperlightGuest/third_party/dlmalloc/malloc.c
+++ b/src/HyperlightGuest/third_party/dlmalloc/malloc.c
@@ -1,3 +1,32 @@
+/*****************************************************************************************
+
+This version has been modified to make it work with Hyperlight and the MSVC compiler.
+
+The changes are marked in the source file with the comment
+
+// Modified for Hyperlight - <change details>
+
+And they are enabled using the preprocessor symbol HYPERLIGHT
+
+The changes are as follows:
+
+Added function definitions at line 23
+#define for MORECORE and HAVE_MMAP are excluded at line 576
+Call to Windows API GetSystemInfo to determine Page Size is replaced with call to Hyperlight function GetOSPageSize, granularity is set to page size if DEFAULT_GRANULARITY is not set  at line 3151
+Call to GetTickCount replaced with call to GetHyperLightTickCount at line 3213
+
+When we move away from MSVC to either CLang or GCC these changes will need to be reviewed
+
+*****************************************************************************************/
+
+// Modified for Hyperlight - define MORECORE, ABORT and  GetHyperLightTickCount functions
+#ifdef HYPERLIGHT
+extern void* hyperlightMoreCore(size_t);
+extern void dlmalloc_abort();
+extern long  GetHyperLightTickCount();
+extern unsigned int GetOSPageSize();
+#endif /* HYPERLIGHT */
+
 /*
   This is a version (aka dlmalloc) of malloc/free/realloc written by
   Doug Lea and released to the public domain, as explained at
@@ -543,8 +572,14 @@ MAX_RELEASE_CHECK_RATE   default: 4095 unless not HAVE_MMAP
 #define WIN32_LEAN_AND_MEAN
 #include <windows.h>
 #include <tchar.h>
+
+// Modified for Hyperlight - HAVE_MMAP and HAVE_MORECORE are defined in project preprocessor directives for Hyperlight.
+
+#ifndef HYPERLIGHT
 #define HAVE_MMAP 1
 #define HAVE_MORECORE 0
+#endif /* HYPERLIGHT */
+
 #define LACKS_UNISTD_H
 #define LACKS_SYS_PARAM_H
 #define LACKS_SYS_MMAN_H
@@ -761,16 +796,16 @@ MAX_RELEASE_CHECK_RATE   default: 4095 unless not HAVE_MMAP
 #define _STRUCT_MALLINFO
 #define STRUCT_MALLINFO_DECLARED 1
 struct mallinfo {
-  MALLINFO_FIELD_TYPE arena;    /* non-mmapped space allocated from system */
-  MALLINFO_FIELD_TYPE ordblks;  /* number of free chunks */
-  MALLINFO_FIELD_TYPE smblks;   /* always 0 */
-  MALLINFO_FIELD_TYPE hblks;    /* always 0 */
-  MALLINFO_FIELD_TYPE hblkhd;   /* space in mmapped regions */
-  MALLINFO_FIELD_TYPE usmblks;  /* maximum total allocated space */
-  MALLINFO_FIELD_TYPE fsmblks;  /* always 0 */
-  MALLINFO_FIELD_TYPE uordblks; /* total allocated space */
-  MALLINFO_FIELD_TYPE fordblks; /* total free space */
-  MALLINFO_FIELD_TYPE keepcost; /* releasable (via malloc_trim) space */
+    MALLINFO_FIELD_TYPE arena;    /* non-mmapped space allocated from system */
+    MALLINFO_FIELD_TYPE ordblks;  /* number of free chunks */
+    MALLINFO_FIELD_TYPE smblks;   /* always 0 */
+    MALLINFO_FIELD_TYPE hblks;    /* always 0 */
+    MALLINFO_FIELD_TYPE hblkhd;   /* space in mmapped regions */
+    MALLINFO_FIELD_TYPE usmblks;  /* maximum total allocated space */
+    MALLINFO_FIELD_TYPE fsmblks;  /* always 0 */
+    MALLINFO_FIELD_TYPE uordblks; /* total allocated space */
+    MALLINFO_FIELD_TYPE fordblks; /* total free space */
+    MALLINFO_FIELD_TYPE keepcost; /* releasable (via malloc_trim) space */
 };
 #endif /* STRUCT_MALLINFO_DECLARED */
 #endif /* HAVE_USR_INCLUDE_MALLOC_H */
@@ -782,35 +817,35 @@ struct mallinfo {
 */
 
 #ifndef FORCEINLINE
-  #if defined(__GNUC__)
+#if defined(__GNUC__)
 #define FORCEINLINE __inline __attribute__ ((always_inline))
-  #elif defined(_MSC_VER)
-    #define FORCEINLINE __forceinline
-  #endif
+#elif defined(_MSC_VER)
+#define FORCEINLINE __forceinline
+#endif
 #endif
 #ifndef NOINLINE
-  #if defined(__GNUC__)
-    #define NOINLINE __attribute__ ((noinline))
-  #elif defined(_MSC_VER)
-    #define NOINLINE __declspec(noinline)
-  #else
-    #define NOINLINE
-  #endif
+#if defined(__GNUC__)
+#define NOINLINE __attribute__ ((noinline))
+#elif defined(_MSC_VER)
+#define NOINLINE __declspec(noinline)
+#else
+#define NOINLINE
+#endif
 #endif
 
 #ifdef __cplusplus
 extern "C" {
 #ifndef FORCEINLINE
- #define FORCEINLINE inline
+#define FORCEINLINE inline
 #endif
 #endif /* __cplusplus */
 #ifndef FORCEINLINE
- #define FORCEINLINE
+#define FORCEINLINE
 #endif
 
 #if !ONLY_MSPACES
 
-/* ------------------- Declarations of public routines ------------------- */
+    /* ------------------- Declarations of public routines ------------------- */
 
 #ifndef USE_DL_PREFIX
 #define dlcalloc               calloc
@@ -851,565 +886,565 @@ extern "C" {
   maximum supported value of n differs across systems, but is in all
   cases less than the maximum representable value of a size_t.
 */
-DLMALLOC_EXPORT void* dlmalloc(size_t);
-
-/*
-  free(void* p)
-  Releases the chunk of memory pointed to by p, that had been previously
-  allocated using malloc or a related routine such as realloc.
-  It has no effect if p is null. If p was not malloced or already
-  freed, free(p) will by default cause the current program to abort.
-*/
-DLMALLOC_EXPORT void  dlfree(void*);
-
-/*
-  calloc(size_t n_elements, size_t element_size);
-  Returns a pointer to n_elements * element_size bytes, with all locations
-  set to zero.
-*/
-DLMALLOC_EXPORT void* dlcalloc(size_t, size_t);
-
-/*
-  realloc(void* p, size_t n)
-  Returns a pointer to a chunk of size n that contains the same data
-  as does chunk p up to the minimum of (n, p's size) bytes, or null
-  if no space is available.
-
-  The returned pointer may or may not be the same as p. The algorithm
-  prefers extending p in most cases when possible, otherwise it
-  employs the equivalent of a malloc-copy-free sequence.
-
-  If p is null, realloc is equivalent to malloc.
-
-  If space is not available, realloc returns null, errno is set (if on
-  ANSI) and p is NOT freed.
-
-  if n is for fewer bytes than already held by p, the newly unused
-  space is lopped off and freed if possible.  realloc with a size
-  argument of zero (re)allocates a minimum-sized chunk.
-
-  The old unix realloc convention of allowing the last-free'd chunk
-  to be used as an argument to realloc is not supported.
-*/
-DLMALLOC_EXPORT void* dlrealloc(void*, size_t);
+    DLMALLOC_EXPORT void* dlmalloc(size_t);
+
+    /*
+      free(void* p)
+      Releases the chunk of memory pointed to by p, that had been previously
+      allocated using malloc or a related routine such as realloc.
+      It has no effect if p is null. If p was not malloced or already
+      freed, free(p) will by default cause the current program to abort.
+    */
+    DLMALLOC_EXPORT void  dlfree(void*);
 
-/*
-  realloc_in_place(void* p, size_t n)
-  Resizes the space allocated for p to size n, only if this can be
-  done without moving p (i.e., only if there is adjacent space
-  available if n is greater than p's current allocated size, or n is
-  less than or equal to p's size). This may be used instead of plain
-  realloc if an alternative allocation strategy is needed upon failure
-  to expand space; for example, reallocation of a buffer that must be
-  memory-aligned or cleared. You can use realloc_in_place to trigger
-  these alternatives only when needed.
-
-  Returns p if successful; otherwise null.
-*/
-DLMALLOC_EXPORT void* dlrealloc_in_place(void*, size_t);
+    /*
+      calloc(size_t n_elements, size_t element_size);
+      Returns a pointer to n_elements * element_size bytes, with all locations
+      set to zero.
+    */
+    DLMALLOC_EXPORT void* dlcalloc(size_t, size_t);
 
-/*
-  memalign(size_t alignment, size_t n);
-  Returns a pointer to a newly allocated chunk of n bytes, aligned
-  in accord with the alignment argument.
+    /*
+      realloc(void* p, size_t n)
+      Returns a pointer to a chunk of size n that contains the same data
+      as does chunk p up to the minimum of (n, p's size) bytes, or null
+      if no space is available.
 
-  The alignment argument should be a power of two. If the argument is
-  not a power of two, the nearest greater power is used.
-  8-byte alignment is guaranteed by normal malloc calls, so don't
-  bother calling memalign with an argument of 8 or less.
+      The returned pointer may or may not be the same as p. The algorithm
+      prefers extending p in most cases when possible, otherwise it
+      employs the equivalent of a malloc-copy-free sequence.
 
-  Overreliance on memalign is a sure way to fragment space.
-*/
-DLMALLOC_EXPORT void* dlmemalign(size_t, size_t);
+      If p is null, realloc is equivalent to malloc.
 
-/*
-  int posix_memalign(void** pp, size_t alignment, size_t n);
-  Allocates a chunk of n bytes, aligned in accord with the alignment
-  argument. Differs from memalign only in that it (1) assigns the
-  allocated memory to *pp rather than returning it, (2) fails and
-  returns EINVAL if the alignment is not a power of two (3) fails and
-  returns ENOMEM if memory cannot be allocated.
-*/
-DLMALLOC_EXPORT int dlposix_memalign(void**, size_t, size_t);
+      If space is not available, realloc returns null, errno is set (if on
+      ANSI) and p is NOT freed.
 
-/*
-  valloc(size_t n);
-  Equivalent to memalign(pagesize, n), where pagesize is the page
-  size of the system. If the pagesize is unknown, 4096 is used.
-*/
-DLMALLOC_EXPORT void* dlvalloc(size_t);
+      if n is for fewer bytes than already held by p, the newly unused
+      space is lopped off and freed if possible.  realloc with a size
+      argument of zero (re)allocates a minimum-sized chunk.
 
-/*
-  mallopt(int parameter_number, int parameter_value)
-  Sets tunable parameters The format is to provide a
-  (parameter-number, parameter-value) pair.  mallopt then sets the
-  corresponding parameter to the argument value if it can (i.e., so
-  long as the value is meaningful), and returns 1 if successful else
-  0.  To workaround the fact that mallopt is specified to use int,
-  not size_t parameters, the value -1 is specially treated as the
-  maximum unsigned size_t value.
-
-  SVID/XPG/ANSI defines four standard param numbers for mallopt,
-  normally defined in malloc.h.  None of these are use in this malloc,
-  so setting them has no effect. But this malloc also supports other
-  options in mallopt. See below for details.  Briefly, supported
-  parameters are as follows (listed defaults are for "typical"
-  configurations).
-
-  Symbol            param #  default    allowed param values
-  M_TRIM_THRESHOLD     -1   2*1024*1024   any   (-1 disables)
-  M_GRANULARITY        -2     page size   any power of 2 >= page size
-  M_MMAP_THRESHOLD     -3      256*1024   any   (or 0 if no MMAP support)
-*/
-DLMALLOC_EXPORT int dlmallopt(int, int);
+      The old unix realloc convention of allowing the last-free'd chunk
+      to be used as an argument to realloc is not supported.
+    */
+    DLMALLOC_EXPORT void* dlrealloc(void*, size_t);
+
+    /*
+      realloc_in_place(void* p, size_t n)
+      Resizes the space allocated for p to size n, only if this can be
+      done without moving p (i.e., only if there is adjacent space
+      available if n is greater than p's current allocated size, or n is
+      less than or equal to p's size). This may be used instead of plain
+      realloc if an alternative allocation strategy is needed upon failure
+      to expand space; for example, reallocation of a buffer that must be
+      memory-aligned or cleared. You can use realloc_in_place to trigger
+      these alternatives only when needed.
+
+      Returns p if successful; otherwise null.
+    */
+    DLMALLOC_EXPORT void* dlrealloc_in_place(void*, size_t);
 
-/*
-  malloc_footprint();
-  Returns the number of bytes obtained from the system.  The total
-  number of bytes allocated by malloc, realloc etc., is less than this
-  value. Unlike mallinfo, this function returns only a precomputed
-  result, so can be called frequently to monitor memory consumption.
-  Even if locks are otherwise defined, this function does not use them,
-  so results might not be up to date.
-*/
-DLMALLOC_EXPORT size_t dlmalloc_footprint(void);
+    /*
+      memalign(size_t alignment, size_t n);
+      Returns a pointer to a newly allocated chunk of n bytes, aligned
+      in accord with the alignment argument.
 
-/*
-  malloc_max_footprint();
-  Returns the maximum number of bytes obtained from the system. This
-  value will be greater than current footprint if deallocated space
-  has been reclaimed by the system. The peak number of bytes allocated
-  by malloc, realloc etc., is less than this value. Unlike mallinfo,
-  this function returns only a precomputed result, so can be called
-  frequently to monitor memory consumption.  Even if locks are
-  otherwise defined, this function does not use them, so results might
-  not be up to date.
-*/
-DLMALLOC_EXPORT size_t dlmalloc_max_footprint(void);
+      The alignment argument should be a power of two. If the argument is
+      not a power of two, the nearest greater power is used.
+      8-byte alignment is guaranteed by normal malloc calls, so don't
+      bother calling memalign with an argument of 8 or less.
 
-/*
-  malloc_footprint_limit();
-  Returns the number of bytes that the heap is allowed to obtain from
-  the system, returning the last value returned by
-  malloc_set_footprint_limit, or the maximum size_t value if
-  never set. The returned value reflects a permission. There is no
-  guarantee that this number of bytes can actually be obtained from
-  the system.
-*/
-DLMALLOC_EXPORT size_t dlmalloc_footprint_limit();
+      Overreliance on memalign is a sure way to fragment space.
+    */
+    DLMALLOC_EXPORT void* dlmemalign(size_t, size_t);
+
+    /*
+      int posix_memalign(void** pp, size_t alignment, size_t n);
+      Allocates a chunk of n bytes, aligned in accord with the alignment
+      argument. Differs from memalign only in that it (1) assigns the
+      allocated memory to *pp rather than returning it, (2) fails and
+      returns EINVAL if the alignment is not a power of two (3) fails and
+      returns ENOMEM if memory cannot be allocated.
+    */
+    DLMALLOC_EXPORT int dlposix_memalign(void**, size_t, size_t);
 
-/*
-  malloc_set_footprint_limit();
-  Sets the maximum number of bytes to obtain from the system, causing
-  failure returns from malloc and related functions upon attempts to
-  exceed this value. The argument value may be subject to page
-  rounding to an enforceable limit; this actual value is returned.
-  Using an argument of the maximum possible size_t effectively
-  disables checks. If the argument is less than or equal to the
-  current malloc_footprint, then all future allocations that require
-  additional system memory will fail. However, invocation cannot
-  retroactively deallocate existing used memory.
-*/
-DLMALLOC_EXPORT size_t dlmalloc_set_footprint_limit(size_t bytes);
+    /*
+      valloc(size_t n);
+      Equivalent to memalign(pagesize, n), where pagesize is the page
+      size of the system. If the pagesize is unknown, 4096 is used.
+    */
+    DLMALLOC_EXPORT void* dlvalloc(size_t);
+
+    /*
+      mallopt(int parameter_number, int parameter_value)
+      Sets tunable parameters The format is to provide a
+      (parameter-number, parameter-value) pair.  mallopt then sets the
+      corresponding parameter to the argument value if it can (i.e., so
+      long as the value is meaningful), and returns 1 if successful else
+      0.  To workaround the fact that mallopt is specified to use int,
+      not size_t parameters, the value -1 is specially treated as the
+      maximum unsigned size_t value.
+
+      SVID/XPG/ANSI defines four standard param numbers for mallopt,
+      normally defined in malloc.h.  None of these are use in this malloc,
+      so setting them has no effect. But this malloc also supports other
+      options in mallopt. See below for details.  Briefly, supported
+      parameters are as follows (listed defaults are for "typical"
+      configurations).
+
+      Symbol            param #  default    allowed param values
+      M_TRIM_THRESHOLD     -1   2*1024*1024   any   (-1 disables)
+      M_GRANULARITY        -2     page size   any power of 2 >= page size
+      M_MMAP_THRESHOLD     -3      256*1024   any   (or 0 if no MMAP support)
+    */
+    DLMALLOC_EXPORT int dlmallopt(int, int);
+
+    /*
+      malloc_footprint();
+      Returns the number of bytes obtained from the system.  The total
+      number of bytes allocated by malloc, realloc etc., is less than this
+      value. Unlike mallinfo, this function returns only a precomputed
+      result, so can be called frequently to monitor memory consumption.
+      Even if locks are otherwise defined, this function does not use them,
+      so results might not be up to date.
+    */
+    DLMALLOC_EXPORT size_t dlmalloc_footprint(void);
+
+    /*
+      malloc_max_footprint();
+      Returns the maximum number of bytes obtained from the system. This
+      value will be greater than current footprint if deallocated space
+      has been reclaimed by the system. The peak number of bytes allocated
+      by malloc, realloc etc., is less than this value. Unlike mallinfo,
+      this function returns only a precomputed result, so can be called
+      frequently to monitor memory consumption.  Even if locks are
+      otherwise defined, this function does not use them, so results might
+      not be up to date.
+    */
+    DLMALLOC_EXPORT size_t dlmalloc_max_footprint(void);
+
+    /*
+      malloc_footprint_limit();
+      Returns the number of bytes that the heap is allowed to obtain from
+      the system, returning the last value returned by
+      malloc_set_footprint_limit, or the maximum size_t value if
+      never set. The returned value reflects a permission. There is no
+      guarantee that this number of bytes can actually be obtained from
+      the system.
+    */
+    DLMALLOC_EXPORT size_t dlmalloc_footprint_limit();
+
+    /*
+      malloc_set_footprint_limit();
+      Sets the maximum number of bytes to obtain from the system, causing
+      failure returns from malloc and related functions upon attempts to
+      exceed this value. The argument value may be subject to page
+      rounding to an enforceable limit; this actual value is returned.
+      Using an argument of the maximum possible size_t effectively
+      disables checks. If the argument is less than or equal to the
+      current malloc_footprint, then all future allocations that require
+      additional system memory will fail. However, invocation cannot
+      retroactively deallocate existing used memory.
+    */
+    DLMALLOC_EXPORT size_t dlmalloc_set_footprint_limit(size_t bytes);
 
 #if MALLOC_INSPECT_ALL
-/*
-  malloc_inspect_all(void(*handler)(void *start,
-                                    void *end,
-                                    size_t used_bytes,
-                                    void* callback_arg),
-                      void* arg);
-  Traverses the heap and calls the given handler for each managed
-  region, skipping all bytes that are (or may be) used for bookkeeping
-  purposes.  Traversal does not include include chunks that have been
-  directly memory mapped. Each reported region begins at the start
-  address, and continues up to but not including the end address.  The
-  first used_bytes of the region contain allocated data. If
-  used_bytes is zero, the region is unallocated. The handler is
-  invoked with the given callback argument. If locks are defined, they
-  are held during the entire traversal. It is a bad idea to invoke
-  other malloc functions from within the handler.
-
-  For example, to count the number of in-use chunks with size greater
-  than 1000, you could write:
-  static int count = 0;
-  void count_chunks(void* start, void* end, size_t used, void* arg) {
-    if (used >= 1000) ++count;
-  }
-  then:
-    malloc_inspect_all(count_chunks, NULL);
+    /*
+      malloc_inspect_all(void(*handler)(void *start,
+                                        void *end,
+                                        size_t used_bytes,
+                                        void* callback_arg),
+                          void* arg);
+      Traverses the heap and calls the given handler for each managed
+      region, skipping all bytes that are (or may be) used for bookkeeping
+      purposes.  Traversal does not include include chunks that have been
+      directly memory mapped. Each reported region begins at the start
+      address, and continues up to but not including the end address.  The
+      first used_bytes of the region contain allocated data. If
+      used_bytes is zero, the region is unallocated. The handler is
+      invoked with the given callback argument. If locks are defined, they
+      are held during the entire traversal. It is a bad idea to invoke
+      other malloc functions from within the handler.
+
+      For example, to count the number of in-use chunks with size greater
+      than 1000, you could write:
+      static int count = 0;
+      void count_chunks(void* start, void* end, size_t used, void* arg) {
+        if (used >= 1000) ++count;
+      }
+      then:
+        malloc_inspect_all(count_chunks, NULL);
 
-  malloc_inspect_all is compiled only if MALLOC_INSPECT_ALL is defined.
-*/
-DLMALLOC_EXPORT void dlmalloc_inspect_all(void(*handler)(void*, void *, size_t, void*),
-                           void* arg);
+      malloc_inspect_all is compiled only if MALLOC_INSPECT_ALL is defined.
+    */
+    DLMALLOC_EXPORT void dlmalloc_inspect_all(void(*handler)(void*, void*, size_t, void*),
+        void* arg);
 
 #endif /* MALLOC_INSPECT_ALL */
 
 #if !NO_MALLINFO
-/*
-  mallinfo()
-  Returns (by copy) a struct containing various summary statistics:
-
-  arena:     current total non-mmapped bytes allocated from system
-  ordblks:   the number of free chunks
-  smblks:    always zero.
-  hblks:     current number of mmapped regions
-  hblkhd:    total bytes held in mmapped regions
-  usmblks:   the maximum total allocated space. This will be greater
-                than current total if trimming has occurred.
-  fsmblks:   always zero
-  uordblks:  current total allocated space (normal or mmapped)
-  fordblks:  total free space
-  keepcost:  the maximum number of bytes that could ideally be released
-               back to system via malloc_trim. ("ideally" means that
-               it ignores page restrictions etc.)
-
-  Because these fields are ints, but internal bookkeeping may
-  be kept as longs, the reported values may wrap around zero and
-  thus be inaccurate.
-*/
-DLMALLOC_EXPORT struct mallinfo dlmallinfo(void);
+    /*
+      mallinfo()
+      Returns (by copy) a struct containing various summary statistics:
+
+      arena:     current total non-mmapped bytes allocated from system
+      ordblks:   the number of free chunks
+      smblks:    always zero.
+      hblks:     current number of mmapped regions
+      hblkhd:    total bytes held in mmapped regions
+      usmblks:   the maximum total allocated space. This will be greater
+                    than current total if trimming has occurred.
+      fsmblks:   always zero
+      uordblks:  current total allocated space (normal or mmapped)
+      fordblks:  total free space
+      keepcost:  the maximum number of bytes that could ideally be released
+                   back to system via malloc_trim. ("ideally" means that
+                   it ignores page restrictions etc.)
+
+      Because these fields are ints, but internal bookkeeping may
+      be kept as longs, the reported values may wrap around zero and
+      thus be inaccurate.
+    */
+    DLMALLOC_EXPORT struct mallinfo dlmallinfo(void);
 #endif /* NO_MALLINFO */
 
-/*
-  independent_calloc(size_t n_elements, size_t element_size, void* chunks[]);
-
-  independent_calloc is similar to calloc, but instead of returning a
-  single cleared space, it returns an array of pointers to n_elements
-  independent elements that can hold contents of size elem_size, each
-  of which starts out cleared, and can be independently freed,
-  realloc'ed etc. The elements are guaranteed to be adjacently
-  allocated (this is not guaranteed to occur with multiple callocs or
-  mallocs), which may also improve cache locality in some
-  applications.
-
-  The "chunks" argument is optional (i.e., may be null, which is
-  probably the most typical usage). If it is null, the returned array
-  is itself dynamically allocated and should also be freed when it is
-  no longer needed. Otherwise, the chunks array must be of at least
-  n_elements in length. It is filled in with the pointers to the
-  chunks.
-
-  In either case, independent_calloc returns this pointer array, or
-  null if the allocation failed.  If n_elements is zero and "chunks"
-  is null, it returns a chunk representing an array with zero elements
-  (which should be freed if not wanted).
-
-  Each element must be freed when it is no longer needed. This can be
-  done all at once using bulk_free.
-
-  independent_calloc simplifies and speeds up implementations of many
-  kinds of pools.  It may also be useful when constructing large data
-  structures that initially have a fixed number of fixed-sized nodes,
-  but the number is not known at compile time, and some of the nodes
-  may later need to be freed. For example:
-
-  struct Node { int item; struct Node* next; };
-
-  struct Node* build_list() {
-    struct Node** pool;
-    int n = read_number_of_nodes_needed();
-    if (n <= 0) return 0;
-    pool = (struct Node**)(independent_calloc(n, sizeof(struct Node), 0);
-    if (pool == 0) die();
-    // organize into a linked list...
-    struct Node* first = pool[0];
-    for (i = 0; i < n-1; ++i)
-      pool[i]->next = pool[i+1];
-    free(pool);     // Can now free the array (or not, if it is needed later)
-    return first;
-  }
-*/
-DLMALLOC_EXPORT void** dlindependent_calloc(size_t, size_t, void**);
-
-/*
-  independent_comalloc(size_t n_elements, size_t sizes[], void* chunks[]);
-
-  independent_comalloc allocates, all at once, a set of n_elements
-  chunks with sizes indicated in the "sizes" array.    It returns
-  an array of pointers to these elements, each of which can be
-  independently freed, realloc'ed etc. The elements are guaranteed to
-  be adjacently allocated (this is not guaranteed to occur with
-  multiple callocs or mallocs), which may also improve cache locality
-  in some applications.
-
-  The "chunks" argument is optional (i.e., may be null). If it is null
-  the returned array is itself dynamically allocated and should also
-  be freed when it is no longer needed. Otherwise, the chunks array
-  must be of at least n_elements in length. It is filled in with the
-  pointers to the chunks.
-
-  In either case, independent_comalloc returns this pointer array, or
-  null if the allocation failed.  If n_elements is zero and chunks is
-  null, it returns a chunk representing an array with zero elements
-  (which should be freed if not wanted).
-
-  Each element must be freed when it is no longer needed. This can be
-  done all at once using bulk_free.
-
-  independent_comallac differs from independent_calloc in that each
-  element may have a different size, and also that it does not
-  automatically clear elements.
-
-  independent_comalloc can be used to speed up allocation in cases
-  where several structs or objects must always be allocated at the
-  same time.  For example:
-
-  struct Head { ... }
-  struct Foot { ... }
-
-  void send_message(char* msg) {
-    int msglen = strlen(msg);
-    size_t sizes[3] = { sizeof(struct Head), msglen, sizeof(struct Foot) };
-    void* chunks[3];
-    if (independent_comalloc(3, sizes, chunks) == 0)
-      die();
-    struct Head* head = (struct Head*)(chunks[0]);
-    char*        body = (char*)(chunks[1]);
-    struct Foot* foot = (struct Foot*)(chunks[2]);
-    // ...
-  }
-
-  In general though, independent_comalloc is worth using only for
-  larger values of n_elements. For small values, you probably won't
-  detect enough difference from series of malloc calls to bother.
-
-  Overuse of independent_comalloc can increase overall memory usage,
-  since it cannot reuse existing noncontiguous small chunks that
-  might be available for some of the elements.
-*/
-DLMALLOC_EXPORT void** dlindependent_comalloc(size_t, size_t*, void**);
+    /*
+      independent_calloc(size_t n_elements, size_t element_size, void* chunks[]);
+
+      independent_calloc is similar to calloc, but instead of returning a
+      single cleared space, it returns an array of pointers to n_elements
+      independent elements that can hold contents of size elem_size, each
+      of which starts out cleared, and can be independently freed,
+      realloc'ed etc. The elements are guaranteed to be adjacently
+      allocated (this is not guaranteed to occur with multiple callocs or
+      mallocs), which may also improve cache locality in some
+      applications.
+
+      The "chunks" argument is optional (i.e., may be null, which is
+      probably the most typical usage). If it is null, the returned array
+      is itself dynamically allocated and should also be freed when it is
+      no longer needed. Otherwise, the chunks array must be of at least
+      n_elements in length. It is filled in with the pointers to the
+      chunks.
+
+      In either case, independent_calloc returns this pointer array, or
+      null if the allocation failed.  If n_elements is zero and "chunks"
+      is null, it returns a chunk representing an array with zero elements
+      (which should be freed if not wanted).
+
+      Each element must be freed when it is no longer needed. This can be
+      done all at once using bulk_free.
+
+      independent_calloc simplifies and speeds up implementations of many
+      kinds of pools.  It may also be useful when constructing large data
+      structures that initially have a fixed number of fixed-sized nodes,
+      but the number is not known at compile time, and some of the nodes
+      may later need to be freed. For example:
+
+      struct Node { int item; struct Node* next; };
+
+      struct Node* build_list() {
+        struct Node** pool;
+        int n = read_number_of_nodes_needed();
+        if (n <= 0) return 0;
+        pool = (struct Node**)(independent_calloc(n, sizeof(struct Node), 0);
+        if (pool == 0) die();
+        // organize into a linked list...
+        struct Node* first = pool[0];
+        for (i = 0; i < n-1; ++i)
+          pool[i]->next = pool[i+1];
+        free(pool);     // Can now free the array (or not, if it is needed later)
+        return first;
+      }
+    */
+    DLMALLOC_EXPORT void** dlindependent_calloc(size_t, size_t, void**);
+
+    /*
+      independent_comalloc(size_t n_elements, size_t sizes[], void* chunks[]);
+
+      independent_comalloc allocates, all at once, a set of n_elements
+      chunks with sizes indicated in the "sizes" array.    It returns
+      an array of pointers to these elements, each of which can be
+      independently freed, realloc'ed etc. The elements are guaranteed to
+      be adjacently allocated (this is not guaranteed to occur with
+      multiple callocs or mallocs), which may also improve cache locality
+      in some applications.
+
+      The "chunks" argument is optional (i.e., may be null). If it is null
+      the returned array is itself dynamically allocated and should also
+      be freed when it is no longer needed. Otherwise, the chunks array
+      must be of at least n_elements in length. It is filled in with the
+      pointers to the chunks.
+
+      In either case, independent_comalloc returns this pointer array, or
+      null if the allocation failed.  If n_elements is zero and chunks is
+      null, it returns a chunk representing an array with zero elements
+      (which should be freed if not wanted).
+
+      Each element must be freed when it is no longer needed. This can be
+      done all at once using bulk_free.
+
+      independent_comallac differs from independent_calloc in that each
+      element may have a different size, and also that it does not
+      automatically clear elements.
+
+      independent_comalloc can be used to speed up allocation in cases
+      where several structs or objects must always be allocated at the
+      same time.  For example:
+
+      struct Head { ... }
+      struct Foot { ... }
+
+      void send_message(char* msg) {
+        int msglen = strlen(msg);
+        size_t sizes[3] = { sizeof(struct Head), msglen, sizeof(struct Foot) };
+        void* chunks[3];
+        if (independent_comalloc(3, sizes, chunks) == 0)
+          die();
+        struct Head* head = (struct Head*)(chunks[0]);
+        char*        body = (char*)(chunks[1]);
+        struct Foot* foot = (struct Foot*)(chunks[2]);
+        // ...
+      }
 
-/*
-  bulk_free(void* array[], size_t n_elements)
-  Frees and clears (sets to null) each non-null pointer in the given
-  array.  This is likely to be faster than freeing them one-by-one.
-  If footers are used, pointers that have been allocated in different
-  mspaces are not freed or cleared, and the count of all such pointers
-  is returned.  For large arrays of pointers with poor locality, it
-  may be worthwhile to sort this array before calling bulk_free.
-*/
-DLMALLOC_EXPORT size_t  dlbulk_free(void**, size_t n_elements);
+      In general though, independent_comalloc is worth using only for
+      larger values of n_elements. For small values, you probably won't
+      detect enough difference from series of malloc calls to bother.
 
-/*
-  pvalloc(size_t n);
-  Equivalent to valloc(minimum-page-that-holds(n)), that is,
-  round up n to nearest pagesize.
- */
-DLMALLOC_EXPORT void*  dlpvalloc(size_t);
+      Overuse of independent_comalloc can increase overall memory usage,
+      since it cannot reuse existing noncontiguous small chunks that
+      might be available for some of the elements.
+    */
+    DLMALLOC_EXPORT void** dlindependent_comalloc(size_t, size_t*, void**);
+
+    /*
+      bulk_free(void* array[], size_t n_elements)
+      Frees and clears (sets to null) each non-null pointer in the given
+      array.  This is likely to be faster than freeing them one-by-one.
+      If footers are used, pointers that have been allocated in different
+      mspaces are not freed or cleared, and the count of all such pointers
+      is returned.  For large arrays of pointers with poor locality, it
+      may be worthwhile to sort this array before calling bulk_free.
+    */
+    DLMALLOC_EXPORT size_t  dlbulk_free(void**, size_t n_elements);
+
+    /*
+      pvalloc(size_t n);
+      Equivalent to valloc(minimum-page-that-holds(n)), that is,
+      round up n to nearest pagesize.
+     */
+    DLMALLOC_EXPORT void* dlpvalloc(size_t);
+
+    /*
+      malloc_trim(size_t pad);
+
+      If possible, gives memory back to the system (via negative arguments
+      to sbrk) if there is unused memory at the `high' end of the malloc
+      pool or in unused MMAP segments. You can call this after freeing
+      large blocks of memory to potentially reduce the system-level memory
+      requirements of a program. However, it cannot guarantee to reduce
+      memory. Under some allocation patterns, some large free blocks of
+      memory will be locked between two used chunks, so they cannot be
+      given back to the system.
+
+      The `pad' argument to malloc_trim represents the amount of free
+      trailing space to leave untrimmed. If this argument is zero, only
+      the minimum amount of memory to maintain internal data structures
+      will be left. Non-zero arguments can be supplied to maintain enough
+      trailing space to service future expected allocations without having
+      to re-obtain memory from the system.
+
+      Malloc_trim returns 1 if it actually released any memory, else 0.
+    */
+    DLMALLOC_EXPORT int  dlmalloc_trim(size_t);
+
+    /*
+      malloc_stats();
+      Prints on stderr the amount of space obtained from the system (both
+      via sbrk and mmap), the maximum amount (which may be more than
+      current if malloc_trim and/or munmap got called), and the current
+      number of bytes allocated via malloc (or realloc, etc) but not yet
+      freed. Note that this is the number of bytes allocated, not the
+      number requested. It will be larger than the number requested
+      because of alignment and bookkeeping overhead. Because it includes
+      alignment wastage as being in use, this figure may be greater than
+      zero even when no user-level chunks are allocated.
+
+      The reported current and maximum system memory can be inaccurate if
+      a program makes other calls to system memory allocation functions
+      (normally sbrk) outside of malloc.
+
+      malloc_stats prints only the most commonly interesting statistics.
+      More information can be obtained by calling mallinfo.
+    */
+    DLMALLOC_EXPORT void  dlmalloc_stats(void);
 
-/*
-  malloc_trim(size_t pad);
-
-  If possible, gives memory back to the system (via negative arguments
-  to sbrk) if there is unused memory at the `high' end of the malloc
-  pool or in unused MMAP segments. You can call this after freeing
-  large blocks of memory to potentially reduce the system-level memory
-  requirements of a program. However, it cannot guarantee to reduce
-  memory. Under some allocation patterns, some large free blocks of
-  memory will be locked between two used chunks, so they cannot be
-  given back to the system.
-
-  The `pad' argument to malloc_trim represents the amount of free
-  trailing space to leave untrimmed. If this argument is zero, only
-  the minimum amount of memory to maintain internal data structures
-  will be left. Non-zero arguments can be supplied to maintain enough
-  trailing space to service future expected allocations without having
-  to re-obtain memory from the system.
-
-  Malloc_trim returns 1 if it actually released any memory, else 0.
-*/
-DLMALLOC_EXPORT int  dlmalloc_trim(size_t);
+    /*
+      malloc_usable_size(void* p);
 
-/*
-  malloc_stats();
-  Prints on stderr the amount of space obtained from the system (both
-  via sbrk and mmap), the maximum amount (which may be more than
-  current if malloc_trim and/or munmap got called), and the current
-  number of bytes allocated via malloc (or realloc, etc) but not yet
-  freed. Note that this is the number of bytes allocated, not the
-  number requested. It will be larger than the number requested
-  because of alignment and bookkeeping overhead. Because it includes
-  alignment wastage as being in use, this figure may be greater than
-  zero even when no user-level chunks are allocated.
-
-  The reported current and maximum system memory can be inaccurate if
-  a program makes other calls to system memory allocation functions
-  (normally sbrk) outside of malloc.
-
-  malloc_stats prints only the most commonly interesting statistics.
-  More information can be obtained by calling mallinfo.
-*/
-DLMALLOC_EXPORT void  dlmalloc_stats(void);
+      Returns the number of bytes you can actually use in
+      an allocated chunk, which may be more than you requested (although
+      often not) due to alignment and minimum size constraints.
+      You can use this many bytes without worrying about
+      overwriting other allocated objects. This is not a particularly great
+      programming practice. malloc_usable_size can be more useful in
+      debugging and assertions, for example:
 
-/*
-  malloc_usable_size(void* p);
-
-  Returns the number of bytes you can actually use in
-  an allocated chunk, which may be more than you requested (although
-  often not) due to alignment and minimum size constraints.
-  You can use this many bytes without worrying about
-  overwriting other allocated objects. This is not a particularly great
-  programming practice. malloc_usable_size can be more useful in
-  debugging and assertions, for example:
-
-  p = malloc(n);
-  assert(malloc_usable_size(p) >= 256);
-*/
-size_t dlmalloc_usable_size(void*);
+      p = malloc(n);
+      assert(malloc_usable_size(p) >= 256);
+    */
+    size_t dlmalloc_usable_size(void*);
 
 #endif /* ONLY_MSPACES */
 
 #if MSPACES
 
-/*
-  mspace is an opaque type representing an independent
-  region of space that supports mspace_malloc, etc.
-*/
-typedef void* mspace;
-
-/*
-  create_mspace creates and returns a new independent space with the
-  given initial capacity, or, if 0, the default granularity size.  It
-  returns null if there is no system memory available to create the
-  space.  If argument locked is non-zero, the space uses a separate
-  lock to control access. The capacity of the space will grow
-  dynamically as needed to service mspace_malloc requests.  You can
-  control the sizes of incremental increases of this space by
-  compiling with a different DEFAULT_GRANULARITY or dynamically
-  setting with mallopt(M_GRANULARITY, value).
-*/
-DLMALLOC_EXPORT mspace create_mspace(size_t capacity, int locked);
-
-/*
-  destroy_mspace destroys the given space, and attempts to return all
-  of its memory back to the system, returning the total number of
-  bytes freed. After destruction, the results of access to all memory
-  used by the space become undefined.
-*/
-DLMALLOC_EXPORT size_t destroy_mspace(mspace msp);
-
-/*
-  create_mspace_with_base uses the memory supplied as the initial base
-  of a new mspace. Part (less than 128*sizeof(size_t) bytes) of this
-  space is used for bookkeeping, so the capacity must be at least this
-  large. (Otherwise 0 is returned.) When this initial space is
-  exhausted, additional memory will be obtained from the system.
-  Destroying this space will deallocate all additionally allocated
-  space (if possible) but not the initial base.
-*/
-DLMALLOC_EXPORT mspace create_mspace_with_base(void* base, size_t capacity, int locked);
+    /*
+      mspace is an opaque type representing an independent
+      region of space that supports mspace_malloc, etc.
+    */
+    typedef void* mspace;
+
+    /*
+      create_mspace creates and returns a new independent space with the
+      given initial capacity, or, if 0, the default granularity size.  It
+      returns null if there is no system memory available to create the
+      space.  If argument locked is non-zero, the space uses a separate
+      lock to control access. The capacity of the space will grow
+      dynamically as needed to service mspace_malloc requests.  You can
+      control the sizes of incremental increases of this space by
+      compiling with a different DEFAULT_GRANULARITY or dynamically
+      setting with mallopt(M_GRANULARITY, value).
+    */
+    DLMALLOC_EXPORT mspace create_mspace(size_t capacity, int locked);
 
-/*
-  mspace_track_large_chunks controls whether requests for large chunks
-  are allocated in their own untracked mmapped regions, separate from
-  others in this mspace. By default large chunks are not tracked,
-  which reduces fragmentation. However, such chunks are not
-  necessarily released to the system upon destroy_mspace.  Enabling
-  tracking by setting to true may increase fragmentation, but avoids
-  leakage when relying on destroy_mspace to release all memory
-  allocated using this space.  The function returns the previous
-  setting.
-*/
-DLMALLOC_EXPORT int mspace_track_large_chunks(mspace msp, int enable);
+    /*
+      destroy_mspace destroys the given space, and attempts to return all
+      of its memory back to the system, returning the total number of
+      bytes freed. After destruction, the results of access to all memory
+      used by the space become undefined.
+    */
+    DLMALLOC_EXPORT size_t destroy_mspace(mspace msp);
+
+    /*
+      create_mspace_with_base uses the memory supplied as the initial base
+      of a new mspace. Part (less than 128*sizeof(size_t) bytes) of this
+      space is used for bookkeeping, so the capacity must be at least this
+      large. (Otherwise 0 is returned.) When this initial space is
+      exhausted, additional memory will be obtained from the system.
+      Destroying this space will deallocate all additionally allocated
+      space (if possible) but not the initial base.
+    */
+    DLMALLOC_EXPORT mspace create_mspace_with_base(void* base, size_t capacity, int locked);
+
+    /*
+      mspace_track_large_chunks controls whether requests for large chunks
+      are allocated in their own untracked mmapped regions, separate from
+      others in this mspace. By default large chunks are not tracked,
+      which reduces fragmentation. However, such chunks are not
+      necessarily released to the system upon destroy_mspace.  Enabling
+      tracking by setting to true may increase fragmentation, but avoids
+      leakage when relying on destroy_mspace to release all memory
+      allocated using this space.  The function returns the previous
+      setting.
+    */
+    DLMALLOC_EXPORT int mspace_track_large_chunks(mspace msp, int enable);
 
 
-/*
-  mspace_malloc behaves as malloc, but operates within
-  the given space.
-*/
-DLMALLOC_EXPORT void* mspace_malloc(mspace msp, size_t bytes);
+    /*
+      mspace_malloc behaves as malloc, but operates within
+      the given space.
+    */
+    DLMALLOC_EXPORT void* mspace_malloc(mspace msp, size_t bytes);
 
-/*
-  mspace_free behaves as free, but operates within
-  the given space.
+    /*
+      mspace_free behaves as free, but operates within
+      the given space.
 
-  If compiled with FOOTERS==1, mspace_free is not actually needed.
-  free may be called instead of mspace_free because freed chunks from
-  any space are handled by their originating spaces.
-*/
-DLMALLOC_EXPORT void mspace_free(mspace msp, void* mem);
+      If compiled with FOOTERS==1, mspace_free is not actually needed.
+      free may be called instead of mspace_free because freed chunks from
+      any space are handled by their originating spaces.
+    */
+    DLMALLOC_EXPORT void mspace_free(mspace msp, void* mem);
 
-/*
-  mspace_realloc behaves as realloc, but operates within
-  the given space.
+    /*
+      mspace_realloc behaves as realloc, but operates within
+      the given space.
 
-  If compiled with FOOTERS==1, mspace_realloc is not actually
-  needed.  realloc may be called instead of mspace_realloc because
-  realloced chunks from any space are handled by their originating
-  spaces.
-*/
-DLMALLOC_EXPORT void* mspace_realloc(mspace msp, void* mem, size_t newsize);
+      If compiled with FOOTERS==1, mspace_realloc is not actually
+      needed.  realloc may be called instead of mspace_realloc because
+      realloced chunks from any space are handled by their originating
+      spaces.
+    */
+    DLMALLOC_EXPORT void* mspace_realloc(mspace msp, void* mem, size_t newsize);
 
-/*
-  mspace_calloc behaves as calloc, but operates within
-  the given space.
-*/
-DLMALLOC_EXPORT void* mspace_calloc(mspace msp, size_t n_elements, size_t elem_size);
+    /*
+      mspace_calloc behaves as calloc, but operates within
+      the given space.
+    */
+    DLMALLOC_EXPORT void* mspace_calloc(mspace msp, size_t n_elements, size_t elem_size);
 
-/*
-  mspace_memalign behaves as memalign, but operates within
-  the given space.
-*/
-DLMALLOC_EXPORT void* mspace_memalign(mspace msp, size_t alignment, size_t bytes);
+    /*
+      mspace_memalign behaves as memalign, but operates within
+      the given space.
+    */
+    DLMALLOC_EXPORT void* mspace_memalign(mspace msp, size_t alignment, size_t bytes);
 
-/*
-  mspace_independent_calloc behaves as independent_calloc, but
-  operates within the given space.
-*/
-DLMALLOC_EXPORT void** mspace_independent_calloc(mspace msp, size_t n_elements,
-                                 size_t elem_size, void* chunks[]);
+    /*
+      mspace_independent_calloc behaves as independent_calloc, but
+      operates within the given space.
+    */
+    DLMALLOC_EXPORT void** mspace_independent_calloc(mspace msp, size_t n_elements,
+        size_t elem_size, void* chunks[]);
 
-/*
-  mspace_independent_comalloc behaves as independent_comalloc, but
-  operates within the given space.
-*/
-DLMALLOC_EXPORT void** mspace_independent_comalloc(mspace msp, size_t n_elements,
-                                   size_t sizes[], void* chunks[]);
+    /*
+      mspace_independent_comalloc behaves as independent_comalloc, but
+      operates within the given space.
+    */
+    DLMALLOC_EXPORT void** mspace_independent_comalloc(mspace msp, size_t n_elements,
+        size_t sizes[], void* chunks[]);
 
-/*
-  mspace_footprint() returns the number of bytes obtained from the
-  system for this space.
-*/
-DLMALLOC_EXPORT size_t mspace_footprint(mspace msp);
+    /*
+      mspace_footprint() returns the number of bytes obtained from the
+      system for this space.
+    */
+    DLMALLOC_EXPORT size_t mspace_footprint(mspace msp);
 
-/*
-  mspace_max_footprint() returns the peak number of bytes obtained from the
-  system for this space.
-*/
-DLMALLOC_EXPORT size_t mspace_max_footprint(mspace msp);
+    /*
+      mspace_max_footprint() returns the peak number of bytes obtained from the
+      system for this space.
+    */
+    DLMALLOC_EXPORT size_t mspace_max_footprint(mspace msp);
 
 
 #if !NO_MALLINFO
-/*
-  mspace_mallinfo behaves as mallinfo, but reports properties of
-  the given space.
-*/
-DLMALLOC_EXPORT struct mallinfo mspace_mallinfo(mspace msp);
+    /*
+      mspace_mallinfo behaves as mallinfo, but reports properties of
+      the given space.
+    */
+    DLMALLOC_EXPORT struct mallinfo mspace_mallinfo(mspace msp);
 #endif /* NO_MALLINFO */
 
-/*
-  malloc_usable_size(void* p) behaves the same as malloc_usable_size;
-*/
-DLMALLOC_EXPORT size_t mspace_usable_size(const void* mem);
+    /*
+      malloc_usable_size(void* p) behaves the same as malloc_usable_size;
+    */
+    DLMALLOC_EXPORT size_t mspace_usable_size(const void* mem);
 
-/*
-  mspace_malloc_stats behaves as malloc_stats, but reports
-  properties of the given space.
-*/
-DLMALLOC_EXPORT void mspace_malloc_stats(mspace msp);
+    /*
+      mspace_malloc_stats behaves as malloc_stats, but reports
+      properties of the given space.
+    */
+    DLMALLOC_EXPORT void mspace_malloc_stats(mspace msp);
 
-/*
-  mspace_trim behaves as malloc_trim, but
-  operates within the given space.
-*/
-DLMALLOC_EXPORT int mspace_trim(mspace msp, size_t pad);
+    /*
+      mspace_trim behaves as malloc_trim, but
+      operates within the given space.
+    */
+    DLMALLOC_EXPORT int mspace_trim(mspace msp, size_t pad);
 
-/*
-  An alias for mallopt.
-*/
-DLMALLOC_EXPORT int mspace_mallopt(int, int);
+    /*
+      An alias for mallopt.
+    */
+    DLMALLOC_EXPORT int mspace_mallopt(int, int);
 
 #endif /* MSPACES */
 
@@ -1484,7 +1519,7 @@ DLMALLOC_EXPORT int mspace_mallopt(int, int);
 #include <unistd.h>     /* for sbrk, sysconf */
 #else /* LACKS_UNISTD_H */
 #if !defined(__FreeBSD__) && !defined(__OpenBSD__) && !defined(__NetBSD__)
-extern void*     sbrk(ptrdiff_t);
+extern void* sbrk(ptrdiff_t);
 #endif /* FreeBSD etc */
 #endif /* LACKS_UNISTD_H */
 
@@ -1505,8 +1540,8 @@ extern void*     sbrk(ptrdiff_t);
 #ifdef __cplusplus
 extern "C" {
 #endif /* __cplusplus */
-LONG __cdecl _InterlockedCompareExchange(LONG volatile *Dest, LONG Exchange, LONG Comp);
-LONG __cdecl _InterlockedExchange(LONG volatile *Target, LONG Value);
+    LONG __cdecl _InterlockedCompareExchange(LONG volatile* Dest, LONG Exchange, LONG Comp);
+    LONG __cdecl _InterlockedExchange(LONG volatile* Target, LONG Value);
 #ifdef __cplusplus
 }
 #endif /* __cplusplus */
@@ -1532,8 +1567,8 @@ LONG __cdecl _InterlockedExchange(LONG volatile *Target, LONG Value);
 #ifdef __cplusplus
 extern "C" {
 #endif /* __cplusplus */
-unsigned char _BitScanForward(unsigned long *index, unsigned long mask);
-unsigned char _BitScanReverse(unsigned long *index, unsigned long mask);
+    unsigned char _BitScanForward(unsigned long* index, unsigned long mask);
+    unsigned char _BitScanReverse(unsigned long* index, unsigned long mask);
 #ifdef __cplusplus
 }
 #endif /* __cplusplus */
@@ -1556,7 +1591,7 @@ unsigned char _BitScanReverse(unsigned long *index, unsigned long mask);
 #    define malloc_getpagesize sysconf(_SC_PAGE_SIZE)
 #  else
 #    if defined(BSD) || defined(DGUX) || defined(HAVE_GETPAGESIZE)
-       extern size_t getpagesize();
+extern size_t getpagesize();
 #      define malloc_getpagesize getpagesize()
 #    else
 #      ifdef WIN32 /* use supplied emulation of getpagesize */
@@ -1663,33 +1698,33 @@ static int dev_zero_fd = -1; /* Cached file descriptor for /dev/zero. */
 
 /* Win32 MMAP via VirtualAlloc */
 static FORCEINLINE void* win32mmap(size_t size) {
-  void* ptr = VirtualAlloc(0, size, MEM_RESERVE|MEM_COMMIT, PAGE_READWRITE);
-  return (ptr != 0)? ptr: MFAIL;
+    void* ptr = VirtualAlloc(0, size, MEM_RESERVE | MEM_COMMIT, PAGE_READWRITE);
+    return (ptr != 0) ? ptr : MFAIL;
 }
 
 /* For direct MMAP, use MEM_TOP_DOWN to minimize interference */
 static FORCEINLINE void* win32direct_mmap(size_t size) {
-  void* ptr = VirtualAlloc(0, size, MEM_RESERVE|MEM_COMMIT|MEM_TOP_DOWN,
-                           PAGE_READWRITE);
-  return (ptr != 0)? ptr: MFAIL;
+    void* ptr = VirtualAlloc(0, size, MEM_RESERVE | MEM_COMMIT | MEM_TOP_DOWN,
+        PAGE_READWRITE);
+    return (ptr != 0) ? ptr : MFAIL;
 }
 
 /* This function supports releasing coalesed segments */
 static FORCEINLINE int win32munmap(void* ptr, size_t size) {
-  MEMORY_BASIC_INFORMATION minfo;
-  char* cptr = (char*)ptr;
-  while (size) {
-    if (VirtualQuery(cptr, &minfo, sizeof(minfo)) == 0)
-      return -1;
-    if (minfo.BaseAddress != cptr || minfo.AllocationBase != cptr ||
-        minfo.State != MEM_COMMIT || minfo.RegionSize > size)
-      return -1;
-    if (VirtualFree(cptr, 0, MEM_RELEASE) == 0)
-      return -1;
-    cptr += minfo.RegionSize;
-    size -= minfo.RegionSize;
-  }
-  return 0;
+    MEMORY_BASIC_INFORMATION minfo;
+    char* cptr = (char*)ptr;
+    while (size) {
+        if (VirtualQuery(cptr, &minfo, sizeof(minfo)) == 0)
+            return -1;
+        if (minfo.BaseAddress != cptr || minfo.AllocationBase != cptr ||
+            minfo.State != MEM_COMMIT || minfo.RegionSize > size)
+            return -1;
+        if (VirtualFree(cptr, 0, MEM_RELEASE) == 0)
+            return -1;
+        cptr += minfo.RegionSize;
+        size -= minfo.RegionSize;
+    }
+    return 0;
 }
 
 #define MMAP_DEFAULT(s)             win32mmap(s)
@@ -1708,61 +1743,61 @@ static FORCEINLINE int win32munmap(void* ptr, size_t size) {
  * Define CALL_MORECORE
  */
 #if HAVE_MORECORE
-    #ifdef MORECORE
-        #define CALL_MORECORE(S)    MORECORE(S)
-    #else  /* MORECORE */
-        #define CALL_MORECORE(S)    MORECORE_DEFAULT(S)
-    #endif /* MORECORE */
+#ifdef MORECORE
+#define CALL_MORECORE(S)    MORECORE(S)
+#else  /* MORECORE */
+#define CALL_MORECORE(S)    MORECORE_DEFAULT(S)
+#endif /* MORECORE */
 #else  /* HAVE_MORECORE */
-    #define CALL_MORECORE(S)        MFAIL
+#define CALL_MORECORE(S)        MFAIL
 #endif /* HAVE_MORECORE */
 
-/**
- * Define CALL_MMAP/CALL_MUNMAP/CALL_DIRECT_MMAP
- */
+ /**
+  * Define CALL_MMAP/CALL_MUNMAP/CALL_DIRECT_MMAP
+  */
 #if HAVE_MMAP
-    #define USE_MMAP_BIT            (SIZE_T_ONE)
-
-    #ifdef MMAP
-        #define CALL_MMAP(s)        MMAP(s)
-    #else /* MMAP */
-        #define CALL_MMAP(s)        MMAP_DEFAULT(s)
-    #endif /* MMAP */
-    #ifdef MUNMAP
-        #define CALL_MUNMAP(a, s)   MUNMAP((a), (s))
-    #else /* MUNMAP */
-        #define CALL_MUNMAP(a, s)   MUNMAP_DEFAULT((a), (s))
-    #endif /* MUNMAP */
-    #ifdef DIRECT_MMAP
-        #define CALL_DIRECT_MMAP(s) DIRECT_MMAP(s)
-    #else /* DIRECT_MMAP */
-        #define CALL_DIRECT_MMAP(s) DIRECT_MMAP_DEFAULT(s)
-    #endif /* DIRECT_MMAP */
+#define USE_MMAP_BIT            (SIZE_T_ONE)
+
+#ifdef MMAP
+#define CALL_MMAP(s)        MMAP(s)
+#else /* MMAP */
+#define CALL_MMAP(s)        MMAP_DEFAULT(s)
+#endif /* MMAP */
+#ifdef MUNMAP
+#define CALL_MUNMAP(a, s)   MUNMAP((a), (s))
+#else /* MUNMAP */
+#define CALL_MUNMAP(a, s)   MUNMAP_DEFAULT((a), (s))
+#endif /* MUNMAP */
+#ifdef DIRECT_MMAP
+#define CALL_DIRECT_MMAP(s) DIRECT_MMAP(s)
+#else /* DIRECT_MMAP */
+#define CALL_DIRECT_MMAP(s) DIRECT_MMAP_DEFAULT(s)
+#endif /* DIRECT_MMAP */
 #else  /* HAVE_MMAP */
-    #define USE_MMAP_BIT            (SIZE_T_ZERO)
-
-    #define MMAP(s)                 MFAIL
-    #define MUNMAP(a, s)            (-1)
-    #define DIRECT_MMAP(s)          MFAIL
-    #define CALL_DIRECT_MMAP(s)     DIRECT_MMAP(s)
-    #define CALL_MMAP(s)            MMAP(s)
-    #define CALL_MUNMAP(a, s)       MUNMAP((a), (s))
+#define USE_MMAP_BIT            (SIZE_T_ZERO)
+
+#define MMAP(s)                 MFAIL
+#define MUNMAP(a, s)            (-1)
+#define DIRECT_MMAP(s)          MFAIL
+#define CALL_DIRECT_MMAP(s)     DIRECT_MMAP(s)
+#define CALL_MMAP(s)            MMAP(s)
+#define CALL_MUNMAP(a, s)       MUNMAP((a), (s))
 #endif /* HAVE_MMAP */
 
-/**
- * Define CALL_MREMAP
- */
+  /**
+   * Define CALL_MREMAP
+   */
 #if HAVE_MMAP && HAVE_MREMAP
-    #ifdef MREMAP
-        #define CALL_MREMAP(addr, osz, nsz, mv) MREMAP((addr), (osz), (nsz), (mv))
-    #else /* MREMAP */
-        #define CALL_MREMAP(addr, osz, nsz, mv) MREMAP_DEFAULT((addr), (osz), (nsz), (mv))
-    #endif /* MREMAP */
+#ifdef MREMAP
+#define CALL_MREMAP(addr, osz, nsz, mv) MREMAP((addr), (osz), (nsz), (mv))
+#else /* MREMAP */
+#define CALL_MREMAP(addr, osz, nsz, mv) MREMAP_DEFAULT((addr), (osz), (nsz), (mv))
+#endif /* MREMAP */
 #else  /* HAVE_MMAP && HAVE_MREMAP */
-    #define CALL_MREMAP(addr, osz, nsz, mv)     MFAIL
+#define CALL_MREMAP(addr, osz, nsz, mv)     MFAIL
 #endif /* HAVE_MMAP && HAVE_MREMAP */
 
-/* mstate bit set if continguous morecore disabled or failed */
+   /* mstate bit set if continguous morecore disabled or failed */
 #define USE_NONCONTIGUOUS_BIT (4U)
 
 /* segment bit set in create_mspace_with_base */
@@ -1828,25 +1863,25 @@ static FORCEINLINE int win32munmap(void* ptr, size_t size) {
 
 #elif (defined(__GNUC__) && (defined(__i386__) || defined(__x86_64__)))
 /* Custom spin locks for older gcc on x86 */
-static FORCEINLINE int x86_cas_lock(int *sl) {
-  int ret;
-  int val = 1;
-  int cmp = 0;
-  __asm__ __volatile__  ("lock; cmpxchgl %1, %2"
-                         : "=a" (ret)
-                         : "r" (val), "m" (*(sl)), "0"(cmp)
-                         : "memory", "cc");
-  return ret;
+static FORCEINLINE int x86_cas_lock(int* sl) {
+    int ret;
+    int val = 1;
+    int cmp = 0;
+    __asm__ __volatile__("lock; cmpxchgl %1, %2"
+        : "=a" (ret)
+        : "r" (val), "m" (*(sl)), "0"(cmp)
+        : "memory", "cc");
+    return ret;
 }
 
 static FORCEINLINE void x86_clear_lock(int* sl) {
-  assert(*sl != 0);
-  int prev = 0;
-  int ret;
-  __asm__ __volatile__ ("lock; xchgl %0, %1"
-                        : "=r" (ret)
-                        : "m" (*(sl)), "0"(prev)
-                        : "memory");
+    assert(*sl != 0);
+    int prev = 0;
+    int ret;
+    __asm__ __volatile__("lock; xchgl %0, %1"
+        : "=r" (ret)
+        : "m" (*(sl)), "0"(prev)
+        : "memory");
 }
 
 #define CAS_LOCK(sl)     x86_cas_lock(sl)
@@ -1873,14 +1908,14 @@ static FORCEINLINE void x86_clear_lock(int* sl) {
 
 #if !defined(USE_RECURSIVE_LOCKS) || USE_RECURSIVE_LOCKS == 0
 /* Plain spin locks use single word (embedded in malloc_states) */
-static int spin_acquire_lock(int *sl) {
-  int spins = 0;
-  while (*(volatile int *)sl != 0 || CAS_LOCK(sl)) {
-    if ((++spins & SPINS_PER_YIELD) == 0) {
-      SPIN_LOCK_YIELD;
+static int spin_acquire_lock(int* sl) {
+    int spins = 0;
+    while (*(volatile int*)sl != 0 || CAS_LOCK(sl)) {
+        if ((++spins & SPINS_PER_YIELD) == 0) {
+            SPIN_LOCK_YIELD;
+        }
     }
-  }
-  return 0;
+    return 0;
 }
 
 #define MLOCK_T               int
@@ -1909,56 +1944,56 @@ static MLOCK_T malloc_global_mutex = 0;
 #endif
 
 struct malloc_recursive_lock {
-  int sl;
-  unsigned int c;
-  THREAD_ID_T threadid;
+    int sl;
+    unsigned int c;
+    THREAD_ID_T threadid;
 };
 
 #define MLOCK_T  struct malloc_recursive_lock
-static MLOCK_T malloc_global_mutex = { 0, 0, (THREAD_ID_T)0};
+static MLOCK_T malloc_global_mutex = { 0, 0, (THREAD_ID_T)0 };
 
-static FORCEINLINE void recursive_release_lock(MLOCK_T *lk) {
-  assert(lk->sl != 0);
-  if (--lk->c == 0) {
-    CLEAR_LOCK(&lk->sl);
-  }
+static FORCEINLINE void recursive_release_lock(MLOCK_T* lk) {
+    assert(lk->sl != 0);
+    if (--lk->c == 0) {
+        CLEAR_LOCK(&lk->sl);
+    }
 }
 
-static FORCEINLINE int recursive_acquire_lock(MLOCK_T *lk) {
-  THREAD_ID_T mythreadid = CURRENT_THREAD;
-  int spins = 0;
-  for (;;) {
-    if (*((volatile int *)(&lk->sl)) == 0) {
-      if (!CAS_LOCK(&lk->sl)) {
-        lk->threadid = mythreadid;
-        lk->c = 1;
-        return 0;
-      }
-    }
-    else if (EQ_OWNER(lk->threadid, mythreadid)) {
-      ++lk->c;
-      return 0;
-    }
-    if ((++spins & SPINS_PER_YIELD) == 0) {
-      SPIN_LOCK_YIELD;
+static FORCEINLINE int recursive_acquire_lock(MLOCK_T* lk) {
+    THREAD_ID_T mythreadid = CURRENT_THREAD;
+    int spins = 0;
+    for (;;) {
+        if (*((volatile int*)(&lk->sl)) == 0) {
+            if (!CAS_LOCK(&lk->sl)) {
+                lk->threadid = mythreadid;
+                lk->c = 1;
+                return 0;
+            }
+        }
+        else if (EQ_OWNER(lk->threadid, mythreadid)) {
+            ++lk->c;
+            return 0;
+        }
+        if ((++spins & SPINS_PER_YIELD) == 0) {
+            SPIN_LOCK_YIELD;
+        }
     }
-  }
 }
 
-static FORCEINLINE int recursive_try_lock(MLOCK_T *lk) {
-  THREAD_ID_T mythreadid = CURRENT_THREAD;
-  if (*((volatile int *)(&lk->sl)) == 0) {
-    if (!CAS_LOCK(&lk->sl)) {
-      lk->threadid = mythreadid;
-      lk->c = 1;
-      return 1;
+static FORCEINLINE int recursive_try_lock(MLOCK_T* lk) {
+    THREAD_ID_T mythreadid = CURRENT_THREAD;
+    if (*((volatile int*)(&lk->sl)) == 0) {
+        if (!CAS_LOCK(&lk->sl)) {
+            lk->threadid = mythreadid;
+            lk->c = 1;
+            return 1;
+        }
     }
-  }
-  else if (EQ_OWNER(lk->threadid, mythreadid)) {
-    ++lk->c;
-    return 1;
-  }
-  return 0;
+    else if (EQ_OWNER(lk->threadid, mythreadid)) {
+        ++lk->c;
+        return 1;
+    }
+    return 0;
 }
 
 #define RELEASE_LOCK(lk)      recursive_release_lock(lk)
@@ -1982,19 +2017,19 @@ static volatile LONG malloc_global_mutex_status;
 
 /* Use spin loop to initialize global lock */
 static void init_malloc_global_mutex() {
-  for (;;) {
-    long stat = malloc_global_mutex_status;
-    if (stat > 0)
-      return;
-    /* transition to < 0 while initializing, then to > 0) */
-    if (stat == 0 &&
-        interlockedcompareexchange(&malloc_global_mutex_status, (LONG)-1, (LONG)0) == 0) {
-      InitializeCriticalSection(&malloc_global_mutex);
-      interlockedexchange(&malloc_global_mutex_status, (LONG)1);
-      return;
+    for (;;) {
+        long stat = malloc_global_mutex_status;
+        if (stat > 0)
+            return;
+        /* transition to < 0 while initializing, then to > 0) */
+        if (stat == 0 &&
+            interlockedcompareexchange(&malloc_global_mutex_status, (LONG)-1, (LONG)0) == 0) {
+            InitializeCriticalSection(&malloc_global_mutex);
+            interlockedexchange(&malloc_global_mutex_status, (LONG)1);
+            return;
+        }
+        SleepEx(0, FALSE);
     }
-    SleepEx(0, FALSE);
-  }
 }
 
 #else /* pthreads-based locks */
@@ -2008,23 +2043,23 @@ static void init_malloc_global_mutex() {
 #if defined(USE_RECURSIVE_LOCKS) && USE_RECURSIVE_LOCKS != 0 && defined(linux) && !defined(PTHREAD_MUTEX_RECURSIVE)
 /* Cope with old-style linux recursive lock initialization by adding */
 /* skipped internal declaration from pthread.h */
-extern int pthread_mutexattr_setkind_np __P ((pthread_mutexattr_t *__attr,
-                                              int __kind));
+extern int pthread_mutexattr_setkind_np __P((pthread_mutexattr_t* __attr,
+    int __kind));
 #define PTHREAD_MUTEX_RECURSIVE PTHREAD_MUTEX_RECURSIVE_NP
 #define pthread_mutexattr_settype(x,y) pthread_mutexattr_setkind_np(x,y)
 #endif /* USE_RECURSIVE_LOCKS ... */
 
 static MLOCK_T malloc_global_mutex = PTHREAD_MUTEX_INITIALIZER;
 
-static int pthread_init_lock (MLOCK_T *lk) {
-  pthread_mutexattr_t attr;
-  if (pthread_mutexattr_init(&attr)) return 1;
+static int pthread_init_lock(MLOCK_T* lk) {
+    pthread_mutexattr_t attr;
+    if (pthread_mutexattr_init(&attr)) return 1;
 #if defined(USE_RECURSIVE_LOCKS) && USE_RECURSIVE_LOCKS != 0
-  if (pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE)) return 1;
+    if (pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE)) return 1;
 #endif
-  if (pthread_mutex_init(lk, &attr)) return 1;
-  if (pthread_mutexattr_destroy(&attr)) return 1;
-  return 0;
+    if (pthread_mutex_init(lk, &attr)) return 1;
+    if (pthread_mutexattr_destroy(&attr)) return 1;
+    return 0;
 }
 
 #endif /* ... lock types ... */
@@ -2180,10 +2215,10 @@ static int pthread_init_lock (MLOCK_T *lk) {
 */
 
 struct malloc_chunk {
-  size_t               prev_foot;  /* Size of previous chunk (if free).  */
-  size_t               head;       /* Size and inuse bits. */
-  struct malloc_chunk* fd;         /* double links -- used only if free. */
-  struct malloc_chunk* bk;
+    size_t               prev_foot;  /* Size of previous chunk (if free).  */
+    size_t               head;       /* Size and inuse bits. */
+    struct malloc_chunk* fd;         /* double links -- used only if free. */
+    struct malloc_chunk* bk;
 };
 
 typedef struct malloc_chunk  mchunk;
@@ -2389,15 +2424,15 @@ nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
 */
 
 struct malloc_tree_chunk {
-  /* The first four fields must be compatible with malloc_chunk */
-  size_t                    prev_foot;
-  size_t                    head;
-  struct malloc_tree_chunk* fd;
-  struct malloc_tree_chunk* bk;
-
-  struct malloc_tree_chunk* child[2];
-  struct malloc_tree_chunk* parent;
-  bindex_t                  index;
+    /* The first four fields must be compatible with malloc_chunk */
+    size_t                    prev_foot;
+    size_t                    head;
+    struct malloc_tree_chunk* fd;
+    struct malloc_tree_chunk* bk;
+
+    struct malloc_tree_chunk* child[2];
+    struct malloc_tree_chunk* parent;
+    bindex_t                  index;
 };
 
 typedef struct malloc_tree_chunk  tchunk;
@@ -2465,10 +2500,10 @@ typedef struct malloc_tree_chunk* tbinptr; /* The type of bins of trees */
 */
 
 struct malloc_segment {
-  char*        base;             /* base address */
-  size_t       size;             /* allocated size */
-  struct malloc_segment* next;   /* ptr to next segment */
-  flag_t       sflags;           /* mmap and extern flag */
+    char* base;             /* base address */
+    size_t       size;             /* allocated size */
+    struct malloc_segment* next;   /* ptr to next segment */
+    flag_t       sflags;           /* mmap and extern flag */
 };
 
 #define is_mmapped_segment(S)  ((S)->sflags & USE_MMAP_BIT)
@@ -2575,31 +2610,31 @@ typedef struct malloc_segment* msegmentptr;
 #define MAX_SMALL_REQUEST (MAX_SMALL_SIZE - CHUNK_ALIGN_MASK - CHUNK_OVERHEAD)
 
 struct malloc_state {
-  binmap_t   smallmap;
-  binmap_t   treemap;
-  size_t     dvsize;
-  size_t     topsize;
-  char*      least_addr;
-  mchunkptr  dv;
-  mchunkptr  top;
-  size_t     trim_check;
-  size_t     release_checks;
-  size_t     magic;
-  mchunkptr  smallbins[(NSMALLBINS+1)*2];
-  tbinptr    treebins[NTREEBINS];
-  size_t     footprint;
-  size_t     max_footprint;
-  size_t     footprint_limit; /* zero means no limit */
-  flag_t     mflags;
+    binmap_t   smallmap;
+    binmap_t   treemap;
+    size_t     dvsize;
+    size_t     topsize;
+    char* least_addr;
+    mchunkptr  dv;
+    mchunkptr  top;
+    size_t     trim_check;
+    size_t     release_checks;
+    size_t     magic;
+    mchunkptr  smallbins[(NSMALLBINS + 1) * 2];
+    tbinptr    treebins[NTREEBINS];
+    size_t     footprint;
+    size_t     max_footprint;
+    size_t     footprint_limit; /* zero means no limit */
+    flag_t     mflags;
 #if USE_LOCKS
-  MLOCK_T    mutex;     /* locate lock among fields that rarely change */
+    MLOCK_T    mutex;     /* locate lock among fields that rarely change */
 #endif /* USE_LOCKS */
-  msegment   seg;
-  void*      extp;      /* Unused but available for extensions */
-  size_t     exts;
+    msegment   seg;
+    void* extp;      /* Unused but available for extensions */
+    size_t     exts;
 };
 
-typedef struct malloc_state*    mstate;
+typedef struct malloc_state* mstate;
 
 /* ------------- Global malloc_state and malloc_params ------------------- */
 
@@ -2611,12 +2646,12 @@ typedef struct malloc_state*    mstate;
 */
 
 struct malloc_params {
-  size_t magic;
-  size_t page_size;
-  size_t granularity;
-  size_t mmap_threshold;
-  size_t trim_threshold;
-  flag_t default_mflags;
+    size_t magic;
+    size_t page_size;
+    size_t granularity;
+    size_t mmap_threshold;
+    size_t trim_threshold;
+    flag_t default_mflags;
 };
 
 static struct malloc_params mparams;
@@ -2694,24 +2729,24 @@ static struct malloc_state _gm_;
 
 /* Return segment holding given address */
 static msegmentptr segment_holding(mstate m, char* addr) {
-  msegmentptr sp = &m->seg;
-  for (;;) {
-    if (addr >= sp->base && addr < sp->base + sp->size)
-      return sp;
-    if ((sp = sp->next) == 0)
-      return 0;
-  }
+    msegmentptr sp = &m->seg;
+    for (;;) {
+        if (addr >= sp->base && addr < sp->base + sp->size)
+            return sp;
+        if ((sp = sp->next) == 0)
+            return 0;
+    }
 }
 
 /* Return true if segment contains a segment link */
 static int has_segment_link(mstate m, msegmentptr ss) {
-  msegmentptr sp = &m->seg;
-  for (;;) {
-    if ((char*)sp >= ss->base && (char*)sp < ss->base + ss->size)
-      return 1;
-    if ((sp = sp->next) == 0)
-      return 0;
-  }
+    msegmentptr sp = &m->seg;
+    for (;;) {
+        if ((char*)sp >= ss->base && (char*)sp < ss->base + ss->size)
+            return 1;
+        if ((sp = sp->next) == 0)
+            return 0;
+    }
 }
 
 #ifndef MORECORE_CANNOT_TRIM
@@ -3088,123 +3123,139 @@ static size_t traverse_and_check(mstate m);
 /* ---------------------------- setting mparams -------------------------- */
 
 #if LOCK_AT_FORK
-static void pre_fork(void)         { ACQUIRE_LOCK(&(gm)->mutex); }
+static void pre_fork(void) { ACQUIRE_LOCK(&(gm)->mutex); }
 static void post_fork_parent(void) { RELEASE_LOCK(&(gm)->mutex); }
-static void post_fork_child(void)  { INITIAL_LOCK(&(gm)->mutex); }
+static void post_fork_child(void) { INITIAL_LOCK(&(gm)->mutex); }
 #endif /* LOCK_AT_FORK */
 
 /* Initialize mparams */
 static int init_mparams(void) {
 #ifdef NEED_GLOBAL_LOCK_INIT
-  if (malloc_global_mutex_status <= 0)
-    init_malloc_global_mutex();
+    if (malloc_global_mutex_status <= 0)
+        init_malloc_global_mutex();
 #endif
 
-  ACQUIRE_MALLOC_GLOBAL_LOCK();
-  if (mparams.magic == 0) {
-    size_t magic;
-    size_t psize;
-    size_t gsize;
+    ACQUIRE_MALLOC_GLOBAL_LOCK();
+    if (mparams.magic == 0) {
+        size_t magic;
+        size_t psize;
+        size_t gsize;
 
 #ifndef WIN32
-    psize = malloc_getpagesize;
-    gsize = ((DEFAULT_GRANULARITY != 0)? DEFAULT_GRANULARITY : psize);
+        psize = malloc_getpagesize;
+        gsize = ((DEFAULT_GRANULARITY != 0) ? DEFAULT_GRANULARITY : psize);
 #else /* WIN32 */
-    {
-      SYSTEM_INFO system_info;
-      GetSystemInfo(&system_info);
-      psize = system_info.dwPageSize;
-      gsize = ((DEFAULT_GRANULARITY != 0)?
-               DEFAULT_GRANULARITY : system_info.dwAllocationGranularity);
-    }
+        {
+            // Modified for Hyperlight - call to GetSystemInfo to get psize is replaced by call to host function GetOSPageSize()
+            // gsize will be set to psize as DEFAULT_GRANULARITY is set to 0 as MORECORE_CONTIGUOUS is set.
+#ifndef HYPERLIGHT     
+             SYSTEM_INFO system_info;
+            GetSystemInfo(&system_info);
+            psize = system_info.dwPageSize;
+            gsize = ((DEFAULT_GRANULARITY != 0) ?
+                DEFAULT_GRANULARITY : system_info.dwAllocationGranularity);
+#else
+            psize = GetOSPageSize();
+            gsize = gsize = ((DEFAULT_GRANULARITY != 0) ?
+                DEFAULT_GRANULARITY : psize);
+#endif /* HYPERLIGHT */
+        }
 #endif /* WIN32 */
 
-    /* Sanity-check configuration:
-       size_t must be unsigned and as wide as pointer type.
-       ints must be at least 4 bytes.
-       alignment must be at least 8.
-       Alignment, min chunk size, and page size must all be powers of 2.
-    */
-    if ((sizeof(size_t) != sizeof(char*)) ||
-        (MAX_SIZE_T < MIN_CHUNK_SIZE)  ||
-        (sizeof(int) < 4)  ||
-        (MALLOC_ALIGNMENT < (size_t)8U) ||
-        ((MALLOC_ALIGNMENT & (MALLOC_ALIGNMENT-SIZE_T_ONE)) != 0) ||
-        ((MCHUNK_SIZE      & (MCHUNK_SIZE-SIZE_T_ONE))      != 0) ||
-        ((gsize            & (gsize-SIZE_T_ONE))            != 0) ||
-        ((psize            & (psize-SIZE_T_ONE))            != 0))
-      ABORT;
-    mparams.granularity = gsize;
-    mparams.page_size = psize;
-    mparams.mmap_threshold = DEFAULT_MMAP_THRESHOLD;
-    mparams.trim_threshold = DEFAULT_TRIM_THRESHOLD;
+        /* Sanity-check configuration:
+           size_t must be unsigned and as wide as pointer type.
+           ints must be at least 4 bytes.
+           alignment must be at least 8.
+           Alignment, min chunk size, and page size must all be powers of 2.
+        */
+        if ((sizeof(size_t) != sizeof(char*)) ||
+            (MAX_SIZE_T < MIN_CHUNK_SIZE) ||
+            (sizeof(int) < 4) ||
+            (MALLOC_ALIGNMENT < (size_t)8U) ||
+            ((MALLOC_ALIGNMENT & (MALLOC_ALIGNMENT - SIZE_T_ONE)) != 0) ||
+            ((MCHUNK_SIZE & (MCHUNK_SIZE - SIZE_T_ONE)) != 0) ||
+            ((gsize & (gsize - SIZE_T_ONE)) != 0) ||
+            ((psize & (psize - SIZE_T_ONE)) != 0))
+            ABORT;
+        mparams.granularity = gsize;
+        mparams.page_size = psize;
+        mparams.mmap_threshold = DEFAULT_MMAP_THRESHOLD;
+        mparams.trim_threshold = DEFAULT_TRIM_THRESHOLD;
 #if MORECORE_CONTIGUOUS
-    mparams.default_mflags = USE_LOCK_BIT|USE_MMAP_BIT;
+        mparams.default_mflags = USE_LOCK_BIT | USE_MMAP_BIT;
 #else  /* MORECORE_CONTIGUOUS */
-    mparams.default_mflags = USE_LOCK_BIT|USE_MMAP_BIT|USE_NONCONTIGUOUS_BIT;
+        mparams.default_mflags = USE_LOCK_BIT | USE_MMAP_BIT | USE_NONCONTIGUOUS_BIT;
 #endif /* MORECORE_CONTIGUOUS */
 
 #if !ONLY_MSPACES
-    /* Set up lock for main malloc area */
-    gm->mflags = mparams.default_mflags;
-    (void)INITIAL_LOCK(&gm->mutex);
+        /* Set up lock for main malloc area */
+        gm->mflags = mparams.default_mflags;
+        (void)INITIAL_LOCK(&gm->mutex);
 #endif
 #if LOCK_AT_FORK
-    pthread_atfork(&pre_fork, &post_fork_parent, &post_fork_child);
+        pthread_atfork(&pre_fork, &post_fork_parent, &post_fork_child);
 #endif
 
-    {
+        {
 #if USE_DEV_RANDOM
-      int fd;
-      unsigned char buf[sizeof(size_t)];
-      /* Try to use /dev/urandom, else fall back on using time */
-      if ((fd = open("/dev/urandom", O_RDONLY)) >= 0 &&
-          read(fd, buf, sizeof(buf)) == sizeof(buf)) {
-        magic = *((size_t *) buf);
-        close(fd);
-      }
-      else
+            int fd;
+            unsigned char buf[sizeof(size_t)];
+            /* Try to use /dev/urandom, else fall back on using time */
+            if ((fd = open("/dev/urandom", O_RDONLY)) >= 0 &&
+                read(fd, buf, sizeof(buf)) == sizeof(buf)) {
+                magic = *((size_t*)buf);
+                close(fd);
+            }
+            else
 #endif /* USE_DEV_RANDOM */
 #ifdef WIN32
-      magic = (size_t)(GetTickCount() ^ (size_t)0x55555555U);
+                // Modified for Hyperlight - call to GetHyperLightTickCount provided by Hyperlight
+#ifndef HYPERLIGHT     
+            magic = (size_t)(GetTickCount() ^ (size_t)0x55555555U);
+#else
+            magic = (size_t)(GetHyperLightTickCount() ^ (size_t)0x55555555U);
+#endif /* HYPERLIGHT */
+
+                
+               
 #elif defined(LACKS_TIME_H)
-      magic = (size_t)&magic ^ (size_t)0x55555555U;
+                magic = (size_t)&magic ^ (size_t)0x55555555U;
 #else
-      magic = (size_t)(time(0) ^ (size_t)0x55555555U);
+                magic = (size_t)(time(0) ^ (size_t)0x55555555U);
 #endif
-      magic |= (size_t)8U;    /* ensure nonzero */
-      magic &= ~(size_t)7U;   /* improve chances of fault for bad values */
-      /* Until memory modes commonly available, use volatile-write */
-      (*(volatile size_t *)(&(mparams.magic))) = magic;
+            magic |= (size_t)8U;    /* ensure nonzero */
+            magic &= ~(size_t)7U;   /* improve chances of fault for bad values */
+            /* Until memory modes commonly available, use volatile-write */
+            (*(volatile size_t*)(&(mparams.magic))) = magic;
+        }
     }
-  }
 
-  RELEASE_MALLOC_GLOBAL_LOCK();
-  return 1;
+    RELEASE_MALLOC_GLOBAL_LOCK();
+    return 1;
 }
 
 /* support for mallopt */
 static int change_mparam(int param_number, int value) {
-  size_t val;
-  ensure_initialization();
-  val = (value == -1)? MAX_SIZE_T : (size_t)value;
-  switch(param_number) {
-  case M_TRIM_THRESHOLD:
-    mparams.trim_threshold = val;
-    return 1;
-  case M_GRANULARITY:
-    if (val >= mparams.page_size && ((val & (val-1)) == 0)) {
-      mparams.granularity = val;
-      return 1;
+    size_t val;
+    ensure_initialization();
+    val = (value == -1) ? MAX_SIZE_T : (size_t)value;
+    switch (param_number) {
+    case M_TRIM_THRESHOLD:
+        mparams.trim_threshold = val;
+        return 1;
+    case M_GRANULARITY:
+        if (val >= mparams.page_size && ((val & (val - 1)) == 0)) {
+            mparams.granularity = val;
+            return 1;
+        }
+        else
+            return 0;
+    case M_MMAP_THRESHOLD:
+        mparams.mmap_threshold = val;
+        return 1;
+    default:
+        return 0;
     }
-    else
-      return 0;
-  case M_MMAP_THRESHOLD:
-    mparams.mmap_threshold = val;
-    return 1;
-  default:
-    return 0;
-  }
 }
 
 #if DEBUG
@@ -3212,267 +3263,267 @@ static int change_mparam(int param_number, int value) {
 
 /* Check properties of any chunk, whether free, inuse, mmapped etc  */
 static void do_check_any_chunk(mstate m, mchunkptr p) {
-  assert((is_aligned(chunk2mem(p))) || (p->head == FENCEPOST_HEAD));
-  assert(ok_address(m, p));
+    assert((is_aligned(chunk2mem(p))) || (p->head == FENCEPOST_HEAD));
+    assert(ok_address(m, p));
 }
 
 /* Check properties of top chunk */
 static void do_check_top_chunk(mstate m, mchunkptr p) {
-  msegmentptr sp = segment_holding(m, (char*)p);
-  size_t  sz = p->head & ~INUSE_BITS; /* third-lowest bit can be set! */
-  assert(sp != 0);
-  assert((is_aligned(chunk2mem(p))) || (p->head == FENCEPOST_HEAD));
-  assert(ok_address(m, p));
-  assert(sz == m->topsize);
-  assert(sz > 0);
-  assert(sz == ((sp->base + sp->size) - (char*)p) - TOP_FOOT_SIZE);
-  assert(pinuse(p));
-  assert(!pinuse(chunk_plus_offset(p, sz)));
+    msegmentptr sp = segment_holding(m, (char*)p);
+    size_t  sz = p->head & ~INUSE_BITS; /* third-lowest bit can be set! */
+    assert(sp != 0);
+    assert((is_aligned(chunk2mem(p))) || (p->head == FENCEPOST_HEAD));
+    assert(ok_address(m, p));
+    assert(sz == m->topsize);
+    assert(sz > 0);
+    assert(sz == ((sp->base + sp->size) - (char*)p) - TOP_FOOT_SIZE);
+    assert(pinuse(p));
+    assert(!pinuse(chunk_plus_offset(p, sz)));
 }
 
 /* Check properties of (inuse) mmapped chunks */
 static void do_check_mmapped_chunk(mstate m, mchunkptr p) {
-  size_t  sz = chunksize(p);
-  size_t len = (sz + (p->prev_foot) + MMAP_FOOT_PAD);
-  assert(is_mmapped(p));
-  assert(use_mmap(m));
-  assert((is_aligned(chunk2mem(p))) || (p->head == FENCEPOST_HEAD));
-  assert(ok_address(m, p));
-  assert(!is_small(sz));
-  assert((len & (mparams.page_size-SIZE_T_ONE)) == 0);
-  assert(chunk_plus_offset(p, sz)->head == FENCEPOST_HEAD);
-  assert(chunk_plus_offset(p, sz+SIZE_T_SIZE)->head == 0);
+    size_t  sz = chunksize(p);
+    size_t len = (sz + (p->prev_foot) + MMAP_FOOT_PAD);
+    assert(is_mmapped(p));
+    assert(use_mmap(m));
+    assert((is_aligned(chunk2mem(p))) || (p->head == FENCEPOST_HEAD));
+    assert(ok_address(m, p));
+    assert(!is_small(sz));
+    assert((len & (mparams.page_size - SIZE_T_ONE)) == 0);
+    assert(chunk_plus_offset(p, sz)->head == FENCEPOST_HEAD);
+    assert(chunk_plus_offset(p, sz + SIZE_T_SIZE)->head == 0);
 }
 
 /* Check properties of inuse chunks */
 static void do_check_inuse_chunk(mstate m, mchunkptr p) {
-  do_check_any_chunk(m, p);
-  assert(is_inuse(p));
-  assert(next_pinuse(p));
-  /* If not pinuse and not mmapped, previous chunk has OK offset */
-  assert(is_mmapped(p) || pinuse(p) || next_chunk(prev_chunk(p)) == p);
-  if (is_mmapped(p))
-    do_check_mmapped_chunk(m, p);
+    do_check_any_chunk(m, p);
+    assert(is_inuse(p));
+    assert(next_pinuse(p));
+    /* If not pinuse and not mmapped, previous chunk has OK offset */
+    assert(is_mmapped(p) || pinuse(p) || next_chunk(prev_chunk(p)) == p);
+    if (is_mmapped(p))
+        do_check_mmapped_chunk(m, p);
 }
 
 /* Check properties of free chunks */
 static void do_check_free_chunk(mstate m, mchunkptr p) {
-  size_t sz = chunksize(p);
-  mchunkptr next = chunk_plus_offset(p, sz);
-  do_check_any_chunk(m, p);
-  assert(!is_inuse(p));
-  assert(!next_pinuse(p));
-  assert (!is_mmapped(p));
-  if (p != m->dv && p != m->top) {
-    if (sz >= MIN_CHUNK_SIZE) {
-      assert((sz & CHUNK_ALIGN_MASK) == 0);
-      assert(is_aligned(chunk2mem(p)));
-      assert(next->prev_foot == sz);
-      assert(pinuse(p));
-      assert (next == m->top || is_inuse(next));
-      assert(p->fd->bk == p);
-      assert(p->bk->fd == p);
+    size_t sz = chunksize(p);
+    mchunkptr next = chunk_plus_offset(p, sz);
+    do_check_any_chunk(m, p);
+    assert(!is_inuse(p));
+    assert(!next_pinuse(p));
+    assert(!is_mmapped(p));
+    if (p != m->dv && p != m->top) {
+        if (sz >= MIN_CHUNK_SIZE) {
+            assert((sz & CHUNK_ALIGN_MASK) == 0);
+            assert(is_aligned(chunk2mem(p)));
+            assert(next->prev_foot == sz);
+            assert(pinuse(p));
+            assert(next == m->top || is_inuse(next));
+            assert(p->fd->bk == p);
+            assert(p->bk->fd == p);
+        }
+        else  /* markers are always of size SIZE_T_SIZE */
+            assert(sz == SIZE_T_SIZE);
     }
-    else  /* markers are always of size SIZE_T_SIZE */
-      assert(sz == SIZE_T_SIZE);
-  }
 }
 
 /* Check properties of malloced chunks at the point they are malloced */
 static void do_check_malloced_chunk(mstate m, void* mem, size_t s) {
-  if (mem != 0) {
-    mchunkptr p = mem2chunk(mem);
-    size_t sz = p->head & ~INUSE_BITS;
-    do_check_inuse_chunk(m, p);
-    assert((sz & CHUNK_ALIGN_MASK) == 0);
-    assert(sz >= MIN_CHUNK_SIZE);
-    assert(sz >= s);
-    /* unless mmapped, size is less than MIN_CHUNK_SIZE more than request */
-    assert(is_mmapped(p) || sz < (s + MIN_CHUNK_SIZE));
-  }
+    if (mem != 0) {
+        mchunkptr p = mem2chunk(mem);
+        size_t sz = p->head & ~INUSE_BITS;
+        do_check_inuse_chunk(m, p);
+        assert((sz & CHUNK_ALIGN_MASK) == 0);
+        assert(sz >= MIN_CHUNK_SIZE);
+        assert(sz >= s);
+        /* unless mmapped, size is less than MIN_CHUNK_SIZE more than request */
+        assert(is_mmapped(p) || sz < (s + MIN_CHUNK_SIZE));
+    }
 }
 
 /* Check a tree and its subtrees.  */
 static void do_check_tree(mstate m, tchunkptr t) {
-  tchunkptr head = 0;
-  tchunkptr u = t;
-  bindex_t tindex = t->index;
-  size_t tsize = chunksize(t);
-  bindex_t idx;
-  compute_tree_index(tsize, idx);
-  assert(tindex == idx);
-  assert(tsize >= MIN_LARGE_SIZE);
-  assert(tsize >= minsize_for_tree_index(idx));
-  assert((idx == NTREEBINS-1) || (tsize < minsize_for_tree_index((idx+1))));
-
-  do { /* traverse through chain of same-sized nodes */
-    do_check_any_chunk(m, ((mchunkptr)u));
-    assert(u->index == tindex);
-    assert(chunksize(u) == tsize);
-    assert(!is_inuse(u));
-    assert(!next_pinuse(u));
-    assert(u->fd->bk == u);
-    assert(u->bk->fd == u);
-    if (u->parent == 0) {
-      assert(u->child[0] == 0);
-      assert(u->child[1] == 0);
-    }
-    else {
-      assert(head == 0); /* only one node on chain has parent */
-      head = u;
-      assert(u->parent != u);
-      assert (u->parent->child[0] == u ||
-              u->parent->child[1] == u ||
-              *((tbinptr*)(u->parent)) == u);
-      if (u->child[0] != 0) {
-        assert(u->child[0]->parent == u);
-        assert(u->child[0] != u);
-        do_check_tree(m, u->child[0]);
-      }
-      if (u->child[1] != 0) {
-        assert(u->child[1]->parent == u);
-        assert(u->child[1] != u);
-        do_check_tree(m, u->child[1]);
-      }
-      if (u->child[0] != 0 && u->child[1] != 0) {
-        assert(chunksize(u->child[0]) < chunksize(u->child[1]));
-      }
-    }
-    u = u->fd;
-  } while (u != t);
-  assert(head != 0);
+    tchunkptr head = 0;
+    tchunkptr u = t;
+    bindex_t tindex = t->index;
+    size_t tsize = chunksize(t);
+    bindex_t idx;
+    compute_tree_index(tsize, idx);
+    assert(tindex == idx);
+    assert(tsize >= MIN_LARGE_SIZE);
+    assert(tsize >= minsize_for_tree_index(idx));
+    assert((idx == NTREEBINS - 1) || (tsize < minsize_for_tree_index((idx + 1))));
+
+    do { /* traverse through chain of same-sized nodes */
+        do_check_any_chunk(m, ((mchunkptr)u));
+        assert(u->index == tindex);
+        assert(chunksize(u) == tsize);
+        assert(!is_inuse(u));
+        assert(!next_pinuse(u));
+        assert(u->fd->bk == u);
+        assert(u->bk->fd == u);
+        if (u->parent == 0) {
+            assert(u->child[0] == 0);
+            assert(u->child[1] == 0);
+        }
+        else {
+            assert(head == 0); /* only one node on chain has parent */
+            head = u;
+            assert(u->parent != u);
+            assert(u->parent->child[0] == u ||
+                u->parent->child[1] == u ||
+                *((tbinptr*)(u->parent)) == u);
+            if (u->child[0] != 0) {
+                assert(u->child[0]->parent == u);
+                assert(u->child[0] != u);
+                do_check_tree(m, u->child[0]);
+            }
+            if (u->child[1] != 0) {
+                assert(u->child[1]->parent == u);
+                assert(u->child[1] != u);
+                do_check_tree(m, u->child[1]);
+            }
+            if (u->child[0] != 0 && u->child[1] != 0) {
+                assert(chunksize(u->child[0]) < chunksize(u->child[1]));
+            }
+        }
+        u = u->fd;
+    } while (u != t);
+    assert(head != 0);
 }
 
 /*  Check all the chunks in a treebin.  */
 static void do_check_treebin(mstate m, bindex_t i) {
-  tbinptr* tb = treebin_at(m, i);
-  tchunkptr t = *tb;
-  int empty = (m->treemap & (1U << i)) == 0;
-  if (t == 0)
-    assert(empty);
-  if (!empty)
-    do_check_tree(m, t);
+    tbinptr* tb = treebin_at(m, i);
+    tchunkptr t = *tb;
+    int empty = (m->treemap & (1U << i)) == 0;
+    if (t == 0)
+        assert(empty);
+    if (!empty)
+        do_check_tree(m, t);
 }
 
 /*  Check all the chunks in a smallbin.  */
 static void do_check_smallbin(mstate m, bindex_t i) {
-  sbinptr b = smallbin_at(m, i);
-  mchunkptr p = b->bk;
-  unsigned int empty = (m->smallmap & (1U << i)) == 0;
-  if (p == b)
-    assert(empty);
-  if (!empty) {
-    for (; p != b; p = p->bk) {
-      size_t size = chunksize(p);
-      mchunkptr q;
-      /* each chunk claims to be free */
-      do_check_free_chunk(m, p);
-      /* chunk belongs in bin */
-      assert(small_index(size) == i);
-      assert(p->bk == b || chunksize(p->bk) == chunksize(p));
-      /* chunk is followed by an inuse chunk */
-      q = next_chunk(p);
-      if (q->head != FENCEPOST_HEAD)
-        do_check_inuse_chunk(m, q);
+    sbinptr b = smallbin_at(m, i);
+    mchunkptr p = b->bk;
+    unsigned int empty = (m->smallmap & (1U << i)) == 0;
+    if (p == b)
+        assert(empty);
+    if (!empty) {
+        for (; p != b; p = p->bk) {
+            size_t size = chunksize(p);
+            mchunkptr q;
+            /* each chunk claims to be free */
+            do_check_free_chunk(m, p);
+            /* chunk belongs in bin */
+            assert(small_index(size) == i);
+            assert(p->bk == b || chunksize(p->bk) == chunksize(p));
+            /* chunk is followed by an inuse chunk */
+            q = next_chunk(p);
+            if (q->head != FENCEPOST_HEAD)
+                do_check_inuse_chunk(m, q);
+        }
     }
-  }
 }
 
 /* Find x in a bin. Used in other check functions. */
 static int bin_find(mstate m, mchunkptr x) {
-  size_t size = chunksize(x);
-  if (is_small(size)) {
-    bindex_t sidx = small_index(size);
-    sbinptr b = smallbin_at(m, sidx);
-    if (smallmap_is_marked(m, sidx)) {
-      mchunkptr p = b;
-      do {
-        if (p == x)
-          return 1;
-      } while ((p = p->fd) != b);
-    }
-  }
-  else {
-    bindex_t tidx;
-    compute_tree_index(size, tidx);
-    if (treemap_is_marked(m, tidx)) {
-      tchunkptr t = *treebin_at(m, tidx);
-      size_t sizebits = size << leftshift_for_tree_index(tidx);
-      while (t != 0 && chunksize(t) != size) {
-        t = t->child[(sizebits >> (SIZE_T_BITSIZE-SIZE_T_ONE)) & 1];
-        sizebits <<= 1;
-      }
-      if (t != 0) {
-        tchunkptr u = t;
-        do {
-          if (u == (tchunkptr)x)
-            return 1;
-        } while ((u = u->fd) != t);
-      }
+    size_t size = chunksize(x);
+    if (is_small(size)) {
+        bindex_t sidx = small_index(size);
+        sbinptr b = smallbin_at(m, sidx);
+        if (smallmap_is_marked(m, sidx)) {
+            mchunkptr p = b;
+            do {
+                if (p == x)
+                    return 1;
+            } while ((p = p->fd) != b);
+        }
     }
-  }
-  return 0;
-}
+    else {
+        bindex_t tidx;
+        compute_tree_index(size, tidx);
+        if (treemap_is_marked(m, tidx)) {
+            tchunkptr t = *treebin_at(m, tidx);
+            size_t sizebits = size << leftshift_for_tree_index(tidx);
+            while (t != 0 && chunksize(t) != size) {
+                t = t->child[(sizebits >> (SIZE_T_BITSIZE - SIZE_T_ONE)) & 1];
+                sizebits <<= 1;
+            }
+            if (t != 0) {
+                tchunkptr u = t;
+                do {
+                    if (u == (tchunkptr)x)
+                        return 1;
+                } while ((u = u->fd) != t);
+            }
+        }
+    }
+    return 0;
+}
 
 /* Traverse each chunk and check it; return total */
 static size_t traverse_and_check(mstate m) {
-  size_t sum = 0;
-  if (is_initialized(m)) {
-    msegmentptr s = &m->seg;
-    sum += m->topsize + TOP_FOOT_SIZE;
-    while (s != 0) {
-      mchunkptr q = align_as_chunk(s->base);
-      mchunkptr lastq = 0;
-      assert(pinuse(q));
-      while (segment_holds(s, q) &&
-             q != m->top && q->head != FENCEPOST_HEAD) {
-        sum += chunksize(q);
-        if (is_inuse(q)) {
-          assert(!bin_find(m, q));
-          do_check_inuse_chunk(m, q);
-        }
-        else {
-          assert(q == m->dv || bin_find(m, q));
-          assert(lastq == 0 || is_inuse(lastq)); /* Not 2 consecutive free */
-          do_check_free_chunk(m, q);
+    size_t sum = 0;
+    if (is_initialized(m)) {
+        msegmentptr s = &m->seg;
+        sum += m->topsize + TOP_FOOT_SIZE;
+        while (s != 0) {
+            mchunkptr q = align_as_chunk(s->base);
+            mchunkptr lastq = 0;
+            assert(pinuse(q));
+            while (segment_holds(s, q) &&
+                q != m->top && q->head != FENCEPOST_HEAD) {
+                sum += chunksize(q);
+                if (is_inuse(q)) {
+                    assert(!bin_find(m, q));
+                    do_check_inuse_chunk(m, q);
+                }
+                else {
+                    assert(q == m->dv || bin_find(m, q));
+                    assert(lastq == 0 || is_inuse(lastq)); /* Not 2 consecutive free */
+                    do_check_free_chunk(m, q);
+                }
+                lastq = q;
+                q = next_chunk(q);
+            }
+            s = s->next;
         }
-        lastq = q;
-        q = next_chunk(q);
-      }
-      s = s->next;
     }
-  }
-  return sum;
+    return sum;
 }
 
 
 /* Check all properties of malloc_state. */
 static void do_check_malloc_state(mstate m) {
-  bindex_t i;
-  size_t total;
-  /* check bins */
-  for (i = 0; i < NSMALLBINS; ++i)
-    do_check_smallbin(m, i);
-  for (i = 0; i < NTREEBINS; ++i)
-    do_check_treebin(m, i);
-
-  if (m->dvsize != 0) { /* check dv chunk */
-    do_check_any_chunk(m, m->dv);
-    assert(m->dvsize == chunksize(m->dv));
-    assert(m->dvsize >= MIN_CHUNK_SIZE);
-    assert(bin_find(m, m->dv) == 0);
-  }
+    bindex_t i;
+    size_t total;
+    /* check bins */
+    for (i = 0; i < NSMALLBINS; ++i)
+        do_check_smallbin(m, i);
+    for (i = 0; i < NTREEBINS; ++i)
+        do_check_treebin(m, i);
+
+    if (m->dvsize != 0) { /* check dv chunk */
+        do_check_any_chunk(m, m->dv);
+        assert(m->dvsize == chunksize(m->dv));
+        assert(m->dvsize >= MIN_CHUNK_SIZE);
+        assert(bin_find(m, m->dv) == 0);
+    }
 
-  if (m->top != 0) {   /* check top chunk */
-    do_check_top_chunk(m, m->top);
-    /*assert(m->topsize == chunksize(m->top)); redundant */
-    assert(m->topsize > 0);
-    assert(bin_find(m, m->top) == 0);
-  }
+    if (m->top != 0) {   /* check top chunk */
+        do_check_top_chunk(m, m->top);
+        /*assert(m->topsize == chunksize(m->top)); redundant */
+        assert(m->topsize > 0);
+        assert(bin_find(m, m->top) == 0);
+    }
 
-  total = traverse_and_check(m);
-  assert(total <= m->footprint);
-  assert(m->footprint <= m->max_footprint);
+    total = traverse_and_check(m);
+    assert(total <= m->footprint);
+    assert(m->footprint <= m->max_footprint);
 }
 #endif /* DEBUG */
 
@@ -3480,75 +3531,75 @@ static void do_check_malloc_state(mstate m) {
 
 #if !NO_MALLINFO
 static struct mallinfo internal_mallinfo(mstate m) {
-  struct mallinfo nm = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
-  ensure_initialization();
-  if (!PREACTION(m)) {
-    check_malloc_state(m);
-    if (is_initialized(m)) {
-      size_t nfree = SIZE_T_ONE; /* top always free */
-      size_t mfree = m->topsize + TOP_FOOT_SIZE;
-      size_t sum = mfree;
-      msegmentptr s = &m->seg;
-      while (s != 0) {
-        mchunkptr q = align_as_chunk(s->base);
-        while (segment_holds(s, q) &&
-               q != m->top && q->head != FENCEPOST_HEAD) {
-          size_t sz = chunksize(q);
-          sum += sz;
-          if (!is_inuse(q)) {
-            mfree += sz;
-            ++nfree;
-          }
-          q = next_chunk(q);
+    struct mallinfo nm = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
+    ensure_initialization();
+    if (!PREACTION(m)) {
+        check_malloc_state(m);
+        if (is_initialized(m)) {
+            size_t nfree = SIZE_T_ONE; /* top always free */
+            size_t mfree = m->topsize + TOP_FOOT_SIZE;
+            size_t sum = mfree;
+            msegmentptr s = &m->seg;
+            while (s != 0) {
+                mchunkptr q = align_as_chunk(s->base);
+                while (segment_holds(s, q) &&
+                    q != m->top && q->head != FENCEPOST_HEAD) {
+                    size_t sz = chunksize(q);
+                    sum += sz;
+                    if (!is_inuse(q)) {
+                        mfree += sz;
+                        ++nfree;
+                    }
+                    q = next_chunk(q);
+                }
+                s = s->next;
+            }
+
+            nm.arena = sum;
+            nm.ordblks = nfree;
+            nm.hblkhd = m->footprint - sum;
+            nm.usmblks = m->max_footprint;
+            nm.uordblks = m->footprint - mfree;
+            nm.fordblks = mfree;
+            nm.keepcost = m->topsize;
         }
-        s = s->next;
-      }
 
-      nm.arena    = sum;
-      nm.ordblks  = nfree;
-      nm.hblkhd   = m->footprint - sum;
-      nm.usmblks  = m->max_footprint;
-      nm.uordblks = m->footprint - mfree;
-      nm.fordblks = mfree;
-      nm.keepcost = m->topsize;
+        POSTACTION(m);
     }
-
-    POSTACTION(m);
-  }
-  return nm;
+    return nm;
 }
 #endif /* !NO_MALLINFO */
 
 #if !NO_MALLOC_STATS
 static void internal_malloc_stats(mstate m) {
-  ensure_initialization();
-  if (!PREACTION(m)) {
-    size_t maxfp = 0;
-    size_t fp = 0;
-    size_t used = 0;
-    check_malloc_state(m);
-    if (is_initialized(m)) {
-      msegmentptr s = &m->seg;
-      maxfp = m->max_footprint;
-      fp = m->footprint;
-      used = fp - (m->topsize + TOP_FOOT_SIZE);
-
-      while (s != 0) {
-        mchunkptr q = align_as_chunk(s->base);
-        while (segment_holds(s, q) &&
-               q != m->top && q->head != FENCEPOST_HEAD) {
-          if (!is_inuse(q))
-            used -= chunksize(q);
-          q = next_chunk(q);
+    ensure_initialization();
+    if (!PREACTION(m)) {
+        size_t maxfp = 0;
+        size_t fp = 0;
+        size_t used = 0;
+        check_malloc_state(m);
+        if (is_initialized(m)) {
+            msegmentptr s = &m->seg;
+            maxfp = m->max_footprint;
+            fp = m->footprint;
+            used = fp - (m->topsize + TOP_FOOT_SIZE);
+
+            while (s != 0) {
+                mchunkptr q = align_as_chunk(s->base);
+                while (segment_holds(s, q) &&
+                    q != m->top && q->head != FENCEPOST_HEAD) {
+                    if (!is_inuse(q))
+                        used -= chunksize(q);
+                    q = next_chunk(q);
+                }
+                s = s->next;
+            }
         }
-        s = s->next;
-      }
+        POSTACTION(m); /* drop lock */
+        fprintf(stderr, "max system bytes = %10lu\n", (unsigned long)(maxfp));
+        fprintf(stderr, "system bytes     = %10lu\n", (unsigned long)(fp));
+        fprintf(stderr, "in use bytes     = %10lu\n", (unsigned long)(used));
     }
-    POSTACTION(m); /* drop lock */
-    fprintf(stderr, "max system bytes = %10lu\n", (unsigned long)(maxfp));
-    fprintf(stderr, "system bytes     = %10lu\n", (unsigned long)(fp));
-    fprintf(stderr, "in use bytes     = %10lu\n", (unsigned long)(used));
-  }
 }
 #endif /* NO_MALLOC_STATS */
 
@@ -3819,69 +3870,69 @@ static void internal_malloc_stats(mstate m) {
 
 /* Malloc using mmap */
 static void* mmap_alloc(mstate m, size_t nb) {
-  size_t mmsize = mmap_align(nb + SIX_SIZE_T_SIZES + CHUNK_ALIGN_MASK);
-  if (m->footprint_limit != 0) {
-    size_t fp = m->footprint + mmsize;
-    if (fp <= m->footprint || fp > m->footprint_limit)
-      return 0;
-  }
-  if (mmsize > nb) {     /* Check for wrap around 0 */
-    char* mm = (char*)(CALL_DIRECT_MMAP(mmsize));
-    if (mm != CMFAIL) {
-      size_t offset = align_offset(chunk2mem(mm));
-      size_t psize = mmsize - offset - MMAP_FOOT_PAD;
-      mchunkptr p = (mchunkptr)(mm + offset);
-      p->prev_foot = offset;
-      p->head = psize;
-      mark_inuse_foot(m, p, psize);
-      chunk_plus_offset(p, psize)->head = FENCEPOST_HEAD;
-      chunk_plus_offset(p, psize+SIZE_T_SIZE)->head = 0;
-
-      if (m->least_addr == 0 || mm < m->least_addr)
-        m->least_addr = mm;
-      if ((m->footprint += mmsize) > m->max_footprint)
-        m->max_footprint = m->footprint;
-      assert(is_aligned(chunk2mem(p)));
-      check_mmapped_chunk(m, p);
-      return chunk2mem(p);
+    size_t mmsize = mmap_align(nb + SIX_SIZE_T_SIZES + CHUNK_ALIGN_MASK);
+    if (m->footprint_limit != 0) {
+        size_t fp = m->footprint + mmsize;
+        if (fp <= m->footprint || fp > m->footprint_limit)
+            return 0;
     }
-  }
-  return 0;
+    if (mmsize > nb) {     /* Check for wrap around 0 */
+        char* mm = (char*)(CALL_DIRECT_MMAP(mmsize));
+        if (mm != CMFAIL) {
+            size_t offset = align_offset(chunk2mem(mm));
+            size_t psize = mmsize - offset - MMAP_FOOT_PAD;
+            mchunkptr p = (mchunkptr)(mm + offset);
+            p->prev_foot = offset;
+            p->head = psize;
+            mark_inuse_foot(m, p, psize);
+            chunk_plus_offset(p, psize)->head = FENCEPOST_HEAD;
+            chunk_plus_offset(p, psize + SIZE_T_SIZE)->head = 0;
+
+            if (m->least_addr == 0 || mm < m->least_addr)
+                m->least_addr = mm;
+            if ((m->footprint += mmsize) > m->max_footprint)
+                m->max_footprint = m->footprint;
+            assert(is_aligned(chunk2mem(p)));
+            check_mmapped_chunk(m, p);
+            return chunk2mem(p);
+        }
+    }
+    return 0;
 }
 
 /* Realloc using mmap */
 static mchunkptr mmap_resize(mstate m, mchunkptr oldp, size_t nb, int flags) {
-  size_t oldsize = chunksize(oldp);
-  (void)flags; /* placate people compiling -Wunused */
-  if (is_small(nb)) /* Can't shrink mmap regions below small size */
-    return 0;
-  /* Keep old chunk if big enough but not too big */
-  if (oldsize >= nb + SIZE_T_SIZE &&
-      (oldsize - nb) <= (mparams.granularity << 1))
-    return oldp;
-  else {
-    size_t offset = oldp->prev_foot;
-    size_t oldmmsize = oldsize + offset + MMAP_FOOT_PAD;
-    size_t newmmsize = mmap_align(nb + SIX_SIZE_T_SIZES + CHUNK_ALIGN_MASK);
-    char* cp = (char*)CALL_MREMAP((char*)oldp - offset,
-                                  oldmmsize, newmmsize, flags);
-    if (cp != CMFAIL) {
-      mchunkptr newp = (mchunkptr)(cp + offset);
-      size_t psize = newmmsize - offset - MMAP_FOOT_PAD;
-      newp->head = psize;
-      mark_inuse_foot(m, newp, psize);
-      chunk_plus_offset(newp, psize)->head = FENCEPOST_HEAD;
-      chunk_plus_offset(newp, psize+SIZE_T_SIZE)->head = 0;
-
-      if (cp < m->least_addr)
-        m->least_addr = cp;
-      if ((m->footprint += newmmsize - oldmmsize) > m->max_footprint)
-        m->max_footprint = m->footprint;
-      check_mmapped_chunk(m, newp);
-      return newp;
+    size_t oldsize = chunksize(oldp);
+    (void)flags; /* placate people compiling -Wunused */
+    if (is_small(nb)) /* Can't shrink mmap regions below small size */
+        return 0;
+    /* Keep old chunk if big enough but not too big */
+    if (oldsize >= nb + SIZE_T_SIZE &&
+        (oldsize - nb) <= (mparams.granularity << 1))
+        return oldp;
+    else {
+        size_t offset = oldp->prev_foot;
+        size_t oldmmsize = oldsize + offset + MMAP_FOOT_PAD;
+        size_t newmmsize = mmap_align(nb + SIX_SIZE_T_SIZES + CHUNK_ALIGN_MASK);
+        char* cp = (char*)CALL_MREMAP((char*)oldp - offset,
+            oldmmsize, newmmsize, flags);
+        if (cp != CMFAIL) {
+            mchunkptr newp = (mchunkptr)(cp + offset);
+            size_t psize = newmmsize - offset - MMAP_FOOT_PAD;
+            newp->head = psize;
+            mark_inuse_foot(m, newp, psize);
+            chunk_plus_offset(newp, psize)->head = FENCEPOST_HEAD;
+            chunk_plus_offset(newp, psize + SIZE_T_SIZE)->head = 0;
+
+            if (cp < m->least_addr)
+                m->least_addr = cp;
+            if ((m->footprint += newmmsize - oldmmsize) > m->max_footprint)
+                m->max_footprint = m->footprint;
+            check_mmapped_chunk(m, newp);
+            return newp;
+        }
     }
-  }
-  return 0;
+    return 0;
 }
 
 
@@ -3889,913 +3940,913 @@ static mchunkptr mmap_resize(mstate m, mchunkptr oldp, size_t nb, int flags) {
 
 /* Initialize top chunk and its size */
 static void init_top(mstate m, mchunkptr p, size_t psize) {
-  /* Ensure alignment */
-  size_t offset = align_offset(chunk2mem(p));
-  p = (mchunkptr)((char*)p + offset);
-  psize -= offset;
-
-  m->top = p;
-  m->topsize = psize;
-  p->head = psize | PINUSE_BIT;
-  /* set size of fake trailing chunk holding overhead space only once */
-  chunk_plus_offset(p, psize)->head = TOP_FOOT_SIZE;
-  m->trim_check = mparams.trim_threshold; /* reset on each update */
+    /* Ensure alignment */
+    size_t offset = align_offset(chunk2mem(p));
+    p = (mchunkptr)((char*)p + offset);
+    psize -= offset;
+
+    m->top = p;
+    m->topsize = psize;
+    p->head = psize | PINUSE_BIT;
+    /* set size of fake trailing chunk holding overhead space only once */
+    chunk_plus_offset(p, psize)->head = TOP_FOOT_SIZE;
+    m->trim_check = mparams.trim_threshold; /* reset on each update */
 }
 
 /* Initialize bins for a new mstate that is otherwise zeroed out */
 static void init_bins(mstate m) {
-  /* Establish circular links for smallbins */
-  bindex_t i;
-  for (i = 0; i < NSMALLBINS; ++i) {
-    sbinptr bin = smallbin_at(m,i);
-    bin->fd = bin->bk = bin;
-  }
+    /* Establish circular links for smallbins */
+    bindex_t i;
+    for (i = 0; i < NSMALLBINS; ++i) {
+        sbinptr bin = smallbin_at(m, i);
+        bin->fd = bin->bk = bin;
+    }
 }
 
 #if PROCEED_ON_ERROR
 
 /* default corruption action */
 static void reset_on_error(mstate m) {
-  int i;
-  ++malloc_corruption_error_count;
-  /* Reinitialize fields to forget about all memory */
-  m->smallmap = m->treemap = 0;
-  m->dvsize = m->topsize = 0;
-  m->seg.base = 0;
-  m->seg.size = 0;
-  m->seg.next = 0;
-  m->top = m->dv = 0;
-  for (i = 0; i < NTREEBINS; ++i)
-    *treebin_at(m, i) = 0;
-  init_bins(m);
+    int i;
+    ++malloc_corruption_error_count;
+    /* Reinitialize fields to forget about all memory */
+    m->smallmap = m->treemap = 0;
+    m->dvsize = m->topsize = 0;
+    m->seg.base = 0;
+    m->seg.size = 0;
+    m->seg.next = 0;
+    m->top = m->dv = 0;
+    for (i = 0; i < NTREEBINS; ++i)
+        *treebin_at(m, i) = 0;
+    init_bins(m);
 }
 #endif /* PROCEED_ON_ERROR */
 
 /* Allocate chunk and prepend remainder with chunk in successor base. */
 static void* prepend_alloc(mstate m, char* newbase, char* oldbase,
-                           size_t nb) {
-  mchunkptr p = align_as_chunk(newbase);
-  mchunkptr oldfirst = align_as_chunk(oldbase);
-  size_t psize = (char*)oldfirst - (char*)p;
-  mchunkptr q = chunk_plus_offset(p, nb);
-  size_t qsize = psize - nb;
-  set_size_and_pinuse_of_inuse_chunk(m, p, nb);
-
-  assert((char*)oldfirst > (char*)q);
-  assert(pinuse(oldfirst));
-  assert(qsize >= MIN_CHUNK_SIZE);
-
-  /* consolidate remainder with first chunk of old base */
-  if (oldfirst == m->top) {
-    size_t tsize = m->topsize += qsize;
-    m->top = q;
-    q->head = tsize | PINUSE_BIT;
-    check_top_chunk(m, q);
-  }
-  else if (oldfirst == m->dv) {
-    size_t dsize = m->dvsize += qsize;
-    m->dv = q;
-    set_size_and_pinuse_of_free_chunk(q, dsize);
-  }
-  else {
-    if (!is_inuse(oldfirst)) {
-      size_t nsize = chunksize(oldfirst);
-      unlink_chunk(m, oldfirst, nsize);
-      oldfirst = chunk_plus_offset(oldfirst, nsize);
-      qsize += nsize;
+    size_t nb) {
+    mchunkptr p = align_as_chunk(newbase);
+    mchunkptr oldfirst = align_as_chunk(oldbase);
+    size_t psize = (char*)oldfirst - (char*)p;
+    mchunkptr q = chunk_plus_offset(p, nb);
+    size_t qsize = psize - nb;
+    set_size_and_pinuse_of_inuse_chunk(m, p, nb);
+
+    assert((char*)oldfirst > (char*)q);
+    assert(pinuse(oldfirst));
+    assert(qsize >= MIN_CHUNK_SIZE);
+
+    /* consolidate remainder with first chunk of old base */
+    if (oldfirst == m->top) {
+        size_t tsize = m->topsize += qsize;
+        m->top = q;
+        q->head = tsize | PINUSE_BIT;
+        check_top_chunk(m, q);
+    }
+    else if (oldfirst == m->dv) {
+        size_t dsize = m->dvsize += qsize;
+        m->dv = q;
+        set_size_and_pinuse_of_free_chunk(q, dsize);
+    }
+    else {
+        if (!is_inuse(oldfirst)) {
+            size_t nsize = chunksize(oldfirst);
+            unlink_chunk(m, oldfirst, nsize);
+            oldfirst = chunk_plus_offset(oldfirst, nsize);
+            qsize += nsize;
+        }
+        set_free_with_pinuse(q, qsize, oldfirst);
+        insert_chunk(m, q, qsize);
+        check_free_chunk(m, q);
     }
-    set_free_with_pinuse(q, qsize, oldfirst);
-    insert_chunk(m, q, qsize);
-    check_free_chunk(m, q);
-  }
 
-  check_malloced_chunk(m, chunk2mem(p), nb);
-  return chunk2mem(p);
+    check_malloced_chunk(m, chunk2mem(p), nb);
+    return chunk2mem(p);
 }
 
 /* Add a segment to hold a new noncontiguous region */
 static void add_segment(mstate m, char* tbase, size_t tsize, flag_t mmapped) {
-  /* Determine locations and sizes of segment, fenceposts, old top */
-  char* old_top = (char*)m->top;
-  msegmentptr oldsp = segment_holding(m, old_top);
-  char* old_end = oldsp->base + oldsp->size;
-  size_t ssize = pad_request(sizeof(struct malloc_segment));
-  char* rawsp = old_end - (ssize + FOUR_SIZE_T_SIZES + CHUNK_ALIGN_MASK);
-  size_t offset = align_offset(chunk2mem(rawsp));
-  char* asp = rawsp + offset;
-  char* csp = (asp < (old_top + MIN_CHUNK_SIZE))? old_top : asp;
-  mchunkptr sp = (mchunkptr)csp;
-  msegmentptr ss = (msegmentptr)(chunk2mem(sp));
-  mchunkptr tnext = chunk_plus_offset(sp, ssize);
-  mchunkptr p = tnext;
-  int nfences = 0;
-
-  /* reset top to new space */
-  init_top(m, (mchunkptr)tbase, tsize - TOP_FOOT_SIZE);
-
-  /* Set up segment record */
-  assert(is_aligned(ss));
-  set_size_and_pinuse_of_inuse_chunk(m, sp, ssize);
-  *ss = m->seg; /* Push current record */
-  m->seg.base = tbase;
-  m->seg.size = tsize;
-  m->seg.sflags = mmapped;
-  m->seg.next = ss;
-
-  /* Insert trailing fenceposts */
-  for (;;) {
-    mchunkptr nextp = chunk_plus_offset(p, SIZE_T_SIZE);
-    p->head = FENCEPOST_HEAD;
-    ++nfences;
-    if ((char*)(&(nextp->head)) < old_end)
-      p = nextp;
-    else
-      break;
-  }
-  assert(nfences >= 2);
-
-  /* Insert the rest of old top into a bin as an ordinary free chunk */
-  if (csp != old_top) {
-    mchunkptr q = (mchunkptr)old_top;
-    size_t psize = csp - old_top;
-    mchunkptr tn = chunk_plus_offset(q, psize);
-    set_free_with_pinuse(q, psize, tn);
-    insert_chunk(m, q, psize);
-  }
+    /* Determine locations and sizes of segment, fenceposts, old top */
+    char* old_top = (char*)m->top;
+    msegmentptr oldsp = segment_holding(m, old_top);
+    char* old_end = oldsp->base + oldsp->size;
+    size_t ssize = pad_request(sizeof(struct malloc_segment));
+    char* rawsp = old_end - (ssize + FOUR_SIZE_T_SIZES + CHUNK_ALIGN_MASK);
+    size_t offset = align_offset(chunk2mem(rawsp));
+    char* asp = rawsp + offset;
+    char* csp = (asp < (old_top + MIN_CHUNK_SIZE)) ? old_top : asp;
+    mchunkptr sp = (mchunkptr)csp;
+    msegmentptr ss = (msegmentptr)(chunk2mem(sp));
+    mchunkptr tnext = chunk_plus_offset(sp, ssize);
+    mchunkptr p = tnext;
+    int nfences = 0;
+
+    /* reset top to new space */
+    init_top(m, (mchunkptr)tbase, tsize - TOP_FOOT_SIZE);
+
+    /* Set up segment record */
+    assert(is_aligned(ss));
+    set_size_and_pinuse_of_inuse_chunk(m, sp, ssize);
+    *ss = m->seg; /* Push current record */
+    m->seg.base = tbase;
+    m->seg.size = tsize;
+    m->seg.sflags = mmapped;
+    m->seg.next = ss;
+
+    /* Insert trailing fenceposts */
+    for (;;) {
+        mchunkptr nextp = chunk_plus_offset(p, SIZE_T_SIZE);
+        p->head = FENCEPOST_HEAD;
+        ++nfences;
+        if ((char*)(&(nextp->head)) < old_end)
+            p = nextp;
+        else
+            break;
+    }
+    assert(nfences >= 2);
+
+    /* Insert the rest of old top into a bin as an ordinary free chunk */
+    if (csp != old_top) {
+        mchunkptr q = (mchunkptr)old_top;
+        size_t psize = csp - old_top;
+        mchunkptr tn = chunk_plus_offset(q, psize);
+        set_free_with_pinuse(q, psize, tn);
+        insert_chunk(m, q, psize);
+    }
 
-  check_top_chunk(m, m->top);
+    check_top_chunk(m, m->top);
 }
 
 /* -------------------------- System allocation -------------------------- */
 
 /* Get memory from system using MORECORE or MMAP */
 static void* sys_alloc(mstate m, size_t nb) {
-  char* tbase = CMFAIL;
-  size_t tsize = 0;
-  flag_t mmap_flag = 0;
-  size_t asize; /* allocation size */
-
-  ensure_initialization();
-
-  /* Directly map large chunks, but only if already initialized */
-  if (use_mmap(m) && nb >= mparams.mmap_threshold && m->topsize != 0) {
-    void* mem = mmap_alloc(m, nb);
-    if (mem != 0)
-      return mem;
-  }
-
-  asize = granularity_align(nb + SYS_ALLOC_PADDING);
-  if (asize <= nb)
-    return 0; /* wraparound */
-  if (m->footprint_limit != 0) {
-    size_t fp = m->footprint + asize;
-    if (fp <= m->footprint || fp > m->footprint_limit)
-      return 0;
-  }
+    char* tbase = CMFAIL;
+    size_t tsize = 0;
+    flag_t mmap_flag = 0;
+    size_t asize; /* allocation size */
+
+    ensure_initialization();
+
+    /* Directly map large chunks, but only if already initialized */
+    if (use_mmap(m) && nb >= mparams.mmap_threshold && m->topsize != 0) {
+        void* mem = mmap_alloc(m, nb);
+        if (mem != 0)
+            return mem;
+    }
 
-  /*
-    Try getting memory in any of three ways (in most-preferred to
-    least-preferred order):
-    1. A call to MORECORE that can normally contiguously extend memory.
-       (disabled if not MORECORE_CONTIGUOUS or not HAVE_MORECORE or
-       or main space is mmapped or a previous contiguous call failed)
-    2. A call to MMAP new space (disabled if not HAVE_MMAP).
-       Note that under the default settings, if MORECORE is unable to
-       fulfill a request, and HAVE_MMAP is true, then mmap is
-       used as a noncontiguous system allocator. This is a useful backup
-       strategy for systems with holes in address spaces -- in this case
-       sbrk cannot contiguously expand the heap, but mmap may be able to
-       find space.
-    3. A call to MORECORE that cannot usually contiguously extend memory.
-       (disabled if not HAVE_MORECORE)
-
-   In all cases, we need to request enough bytes from system to ensure
-   we can malloc nb bytes upon success, so pad with enough space for
-   top_foot, plus alignment-pad to make sure we don't lose bytes if
-   not on boundary, and round this up to a granularity unit.
-  */
+    asize = granularity_align(nb + SYS_ALLOC_PADDING);
+    if (asize <= nb)
+        return 0; /* wraparound */
+    if (m->footprint_limit != 0) {
+        size_t fp = m->footprint + asize;
+        if (fp <= m->footprint || fp > m->footprint_limit)
+            return 0;
+    }
 
-  if (MORECORE_CONTIGUOUS && !use_noncontiguous(m)) {
-    char* br = CMFAIL;
-    size_t ssize = asize; /* sbrk call size */
-    msegmentptr ss = (m->top == 0)? 0 : segment_holding(m, (char*)m->top);
-    ACQUIRE_MALLOC_GLOBAL_LOCK();
+    /*
+      Try getting memory in any of three ways (in most-preferred to
+      least-preferred order):
+      1. A call to MORECORE that can normally contiguously extend memory.
+         (disabled if not MORECORE_CONTIGUOUS or not HAVE_MORECORE or
+         or main space is mmapped or a previous contiguous call failed)
+      2. A call to MMAP new space (disabled if not HAVE_MMAP).
+         Note that under the default settings, if MORECORE is unable to
+         fulfill a request, and HAVE_MMAP is true, then mmap is
+         used as a noncontiguous system allocator. This is a useful backup
+         strategy for systems with holes in address spaces -- in this case
+         sbrk cannot contiguously expand the heap, but mmap may be able to
+         find space.
+      3. A call to MORECORE that cannot usually contiguously extend memory.
+         (disabled if not HAVE_MORECORE)
+
+     In all cases, we need to request enough bytes from system to ensure
+     we can malloc nb bytes upon success, so pad with enough space for
+     top_foot, plus alignment-pad to make sure we don't lose bytes if
+     not on boundary, and round this up to a granularity unit.
+    */
 
-    if (ss == 0) {  /* First time through or recovery */
-      char* base = (char*)CALL_MORECORE(0);
-      if (base != CMFAIL) {
-        size_t fp;
-        /* Adjust to end on a page boundary */
-        if (!is_page_aligned(base))
-          ssize += (page_align((size_t)base) - (size_t)base);
-        fp = m->footprint + ssize; /* recheck limits */
-        if (ssize > nb && ssize < HALF_MAX_SIZE_T &&
-            (m->footprint_limit == 0 ||
-             (fp > m->footprint && fp <= m->footprint_limit)) &&
-            (br = (char*)(CALL_MORECORE(ssize))) == base) {
-          tbase = base;
-          tsize = ssize;
+    if (MORECORE_CONTIGUOUS && !use_noncontiguous(m)) {
+        char* br = CMFAIL;
+        size_t ssize = asize; /* sbrk call size */
+        msegmentptr ss = (m->top == 0) ? 0 : segment_holding(m, (char*)m->top);
+        ACQUIRE_MALLOC_GLOBAL_LOCK();
+
+        if (ss == 0) {  /* First time through or recovery */
+            char* base = (char *)CALL_MORECORE(0);
+            if (base != CMFAIL) {
+                size_t fp;
+                /* Adjust to end on a page boundary */
+                if (!is_page_aligned(base))
+                    ssize += (page_align((size_t)base) - (size_t)base);
+                fp = m->footprint + ssize; /* recheck limits */
+                if (ssize > nb && ssize < HALF_MAX_SIZE_T &&
+                    (m->footprint_limit == 0 ||
+                        (fp > m->footprint && fp <= m->footprint_limit)) &&
+                    (br = (char*)(CALL_MORECORE(ssize))) == base) {
+                    tbase = base;
+                    tsize = ssize;
+                }
+            }
         }
-      }
-    }
-    else {
-      /* Subtract out existing available top space from MORECORE request. */
-      ssize = granularity_align(nb - m->topsize + SYS_ALLOC_PADDING);
-      /* Use mem here only if it did continuously extend old space */
-      if (ssize < HALF_MAX_SIZE_T &&
-          (br = (char*)(CALL_MORECORE(ssize))) == ss->base+ss->size) {
-        tbase = br;
-        tsize = ssize;
-      }
-    }
-
-    if (tbase == CMFAIL) {    /* Cope with partial failure */
-      if (br != CMFAIL) {    /* Try to use/extend the space we did get */
-        if (ssize < HALF_MAX_SIZE_T &&
-            ssize < nb + SYS_ALLOC_PADDING) {
-          size_t esize = granularity_align(nb + SYS_ALLOC_PADDING - ssize);
-          if (esize < HALF_MAX_SIZE_T) {
-            char* end = (char*)CALL_MORECORE(esize);
-            if (end != CMFAIL)
-              ssize += esize;
-            else {            /* Can't use; try to release */
-              (void) CALL_MORECORE(-ssize);
-              br = CMFAIL;
+        else {
+            /* Subtract out existing available top space from MORECORE request. */
+            ssize = granularity_align(nb - m->topsize + SYS_ALLOC_PADDING);
+            /* Use mem here only if it did continuously extend old space */
+            if (ssize < HALF_MAX_SIZE_T &&
+                (br = (char*)(CALL_MORECORE(ssize))) == ss->base + ss->size) {
+                tbase = br;
+                tsize = ssize;
             }
-          }
         }
-      }
-      if (br != CMFAIL) {    /* Use the space we did get */
-        tbase = br;
-        tsize = ssize;
-      }
-      else
-        disable_contiguous(m); /* Don't try contiguous path in the future */
-    }
 
-    RELEASE_MALLOC_GLOBAL_LOCK();
-  }
+        if (tbase == CMFAIL) {    /* Cope with partial failure */
+            if (br != CMFAIL) {    /* Try to use/extend the space we did get */
+                if (ssize < HALF_MAX_SIZE_T &&
+                    ssize < nb + SYS_ALLOC_PADDING) {
+                    size_t esize = granularity_align(nb + SYS_ALLOC_PADDING - ssize);
+                    if (esize < HALF_MAX_SIZE_T) {
+                        char* end = (char*)CALL_MORECORE(esize);
+                        if (end != CMFAIL)
+                            ssize += esize;
+                        else {            /* Can't use; try to release */
+                            (void)CALL_MORECORE(-ssize);
+                            br = CMFAIL;
+                        }
+                    }
+                }
+            }
+            if (br != CMFAIL) {    /* Use the space we did get */
+                tbase = br;
+                tsize = ssize;
+            }
+            else
+                disable_contiguous(m); /* Don't try contiguous path in the future */
+        }
 
-  if (HAVE_MMAP && tbase == CMFAIL) {  /* Try MMAP */
-    char* mp = (char*)(CALL_MMAP(asize));
-    if (mp != CMFAIL) {
-      tbase = mp;
-      tsize = asize;
-      mmap_flag = USE_MMAP_BIT;
+        RELEASE_MALLOC_GLOBAL_LOCK();
     }
-  }
 
-  if (HAVE_MORECORE && tbase == CMFAIL) { /* Try noncontiguous MORECORE */
-    if (asize < HALF_MAX_SIZE_T) {
-      char* br = CMFAIL;
-      char* end = CMFAIL;
-      ACQUIRE_MALLOC_GLOBAL_LOCK();
-      br = (char*)(CALL_MORECORE(asize));
-      end = (char*)(CALL_MORECORE(0));
-      RELEASE_MALLOC_GLOBAL_LOCK();
-      if (br != CMFAIL && end != CMFAIL && br < end) {
-        size_t ssize = end - br;
-        if (ssize > nb + TOP_FOOT_SIZE) {
-          tbase = br;
-          tsize = ssize;
+    if (HAVE_MMAP && tbase == CMFAIL) {  /* Try MMAP */
+        char* mp = (char*)(CALL_MMAP(asize));
+        if (mp != CMFAIL) {
+            tbase = mp;
+            tsize = asize;
+            mmap_flag = USE_MMAP_BIT;
         }
-      }
     }
-  }
 
-  if (tbase != CMFAIL) {
+    if (HAVE_MORECORE && tbase == CMFAIL) { /* Try noncontiguous MORECORE */
+        if (asize < HALF_MAX_SIZE_T) {
+            char* br = CMFAIL;
+            char* end = CMFAIL;
+            ACQUIRE_MALLOC_GLOBAL_LOCK();
+            br = (char*)(CALL_MORECORE(asize));
+            end = (char*)(CALL_MORECORE(0));
+            RELEASE_MALLOC_GLOBAL_LOCK();
+            if (br != CMFAIL && end != CMFAIL && br < end) {
+                size_t ssize = ((size_t*)end) - br;
+                if (ssize > nb + TOP_FOOT_SIZE) {
+                    tbase = br;
+                    tsize = ssize;
+                }
+            }
+        }
+    }
 
-    if ((m->footprint += tsize) > m->max_footprint)
-      m->max_footprint = m->footprint;
+    if (tbase != CMFAIL) {
 
-    if (!is_initialized(m)) { /* first-time initialization */
-      if (m->least_addr == 0 || tbase < m->least_addr)
-        m->least_addr = tbase;
-      m->seg.base = tbase;
-      m->seg.size = tsize;
-      m->seg.sflags = mmap_flag;
-      m->magic = mparams.magic;
-      m->release_checks = MAX_RELEASE_CHECK_RATE;
-      init_bins(m);
+        if ((m->footprint += tsize) > m->max_footprint)
+            m->max_footprint = m->footprint;
+
+        if (!is_initialized(m)) { /* first-time initialization */
+            if (m->least_addr == 0 || tbase < m->least_addr)
+                m->least_addr = tbase;
+            m->seg.base = tbase;
+            m->seg.size = tsize;
+            m->seg.sflags = mmap_flag;
+            m->magic = mparams.magic;
+            m->release_checks = MAX_RELEASE_CHECK_RATE;
+            init_bins(m);
 #if !ONLY_MSPACES
-      if (is_global(m))
-        init_top(m, (mchunkptr)tbase, tsize - TOP_FOOT_SIZE);
-      else
+            if (is_global(m))
+                init_top(m, (mchunkptr)tbase, tsize - TOP_FOOT_SIZE);
+            else
 #endif
-      {
-        /* Offset top by embedded malloc_state */
-        mchunkptr mn = next_chunk(mem2chunk(m));
-        init_top(m, mn, (size_t)((tbase + tsize) - (char*)mn) -TOP_FOOT_SIZE);
-      }
-    }
+            {
+                /* Offset top by embedded malloc_state */
+                mchunkptr mn = next_chunk(mem2chunk(m));
+                init_top(m, mn, (size_t)((tbase + tsize) - (char*)mn) - TOP_FOOT_SIZE);
+            }
+        }
 
-    else {
-      /* Try to merge with an existing segment */
-      msegmentptr sp = &m->seg;
-      /* Only consider most recent segment if traversal suppressed */
-      while (sp != 0 && tbase != sp->base + sp->size)
-        sp = (NO_SEGMENT_TRAVERSAL) ? 0 : sp->next;
-      if (sp != 0 &&
-          !is_extern_segment(sp) &&
-          (sp->sflags & USE_MMAP_BIT) == mmap_flag &&
-          segment_holds(sp, m->top)) { /* append */
-        sp->size += tsize;
-        init_top(m, m->top, m->topsize + tsize);
-      }
-      else {
-        if (tbase < m->least_addr)
-          m->least_addr = tbase;
-        sp = &m->seg;
-        while (sp != 0 && sp->base != tbase + tsize)
-          sp = (NO_SEGMENT_TRAVERSAL) ? 0 : sp->next;
-        if (sp != 0 &&
-            !is_extern_segment(sp) &&
-            (sp->sflags & USE_MMAP_BIT) == mmap_flag) {
-          char* oldbase = sp->base;
-          sp->base = tbase;
-          sp->size += tsize;
-          return prepend_alloc(m, tbase, oldbase, nb);
+        else {
+            /* Try to merge with an existing segment */
+            msegmentptr sp = &m->seg;
+            /* Only consider most recent segment if traversal suppressed */
+            while (sp != 0 && tbase != sp->base + sp->size)
+                sp = (NO_SEGMENT_TRAVERSAL) ? 0 : sp->next;
+            if (sp != 0 &&
+                !is_extern_segment(sp) &&
+                (sp->sflags & USE_MMAP_BIT) == mmap_flag &&
+                segment_holds(sp, m->top)) { /* append */
+                sp->size += tsize;
+                init_top(m, m->top, m->topsize + tsize);
+            }
+            else {
+                if (tbase < m->least_addr)
+                    m->least_addr = tbase;
+                sp = &m->seg;
+                while (sp != 0 && sp->base != (size_t)tbase + tsize)
+                    sp = (NO_SEGMENT_TRAVERSAL) ? 0 : sp->next;
+                if (sp != 0 &&
+                    !is_extern_segment(sp) &&
+                    (sp->sflags & USE_MMAP_BIT) == mmap_flag) {
+                    char* oldbase = sp->base;
+                    sp->base = tbase;
+                    sp->size += tsize;
+                    return prepend_alloc(m, tbase, oldbase, nb);
+                }
+                else
+                    add_segment(m, tbase, tsize, mmap_flag);
+            }
         }
-        else
-          add_segment(m, tbase, tsize, mmap_flag);
-      }
-    }
 
-    if (nb < m->topsize) { /* Allocate from new or extended top space */
-      size_t rsize = m->topsize -= nb;
-      mchunkptr p = m->top;
-      mchunkptr r = m->top = chunk_plus_offset(p, nb);
-      r->head = rsize | PINUSE_BIT;
-      set_size_and_pinuse_of_inuse_chunk(m, p, nb);
-      check_top_chunk(m, m->top);
-      check_malloced_chunk(m, chunk2mem(p), nb);
-      return chunk2mem(p);
+        if (nb < m->topsize) { /* Allocate from new or extended top space */
+            size_t rsize = m->topsize -= nb;
+            mchunkptr p = m->top;
+            mchunkptr r = m->top = chunk_plus_offset(p, nb);
+            r->head = rsize | PINUSE_BIT;
+            set_size_and_pinuse_of_inuse_chunk(m, p, nb);
+            check_top_chunk(m, m->top);
+            check_malloced_chunk(m, chunk2mem(p), nb);
+            return chunk2mem(p);
+        }
     }
-  }
 
-  MALLOC_FAILURE_ACTION;
-  return 0;
+    MALLOC_FAILURE_ACTION;
+    return 0;
 }
 
 /* -----------------------  system deallocation -------------------------- */
 
 /* Unmap and unlink any mmapped segments that don't contain used chunks */
 static size_t release_unused_segments(mstate m) {
-  size_t released = 0;
-  int nsegs = 0;
-  msegmentptr pred = &m->seg;
-  msegmentptr sp = pred->next;
-  while (sp != 0) {
-    char* base = sp->base;
-    size_t size = sp->size;
-    msegmentptr next = sp->next;
-    ++nsegs;
-    if (is_mmapped_segment(sp) && !is_extern_segment(sp)) {
-      mchunkptr p = align_as_chunk(base);
-      size_t psize = chunksize(p);
-      /* Can unmap if first chunk holds entire segment and not pinned */
-      if (!is_inuse(p) && (char*)p + psize >= base + size - TOP_FOOT_SIZE) {
-        tchunkptr tp = (tchunkptr)p;
-        assert(segment_holds(sp, (char*)sp));
-        if (p == m->dv) {
-          m->dv = 0;
-          m->dvsize = 0;
-        }
-        else {
-          unlink_large_chunk(m, tp);
-        }
-        if (CALL_MUNMAP(base, size) == 0) {
-          released += size;
-          m->footprint -= size;
-          /* unlink obsoleted record */
-          sp = pred;
-          sp->next = next;
-        }
-        else { /* back out if cannot unmap */
-          insert_large_chunk(m, tp, psize);
+    size_t released = 0;
+    int nsegs = 0;
+    msegmentptr pred = &m->seg;
+    msegmentptr sp = pred->next;
+    while (sp != 0) {
+        char* base = sp->base;
+        size_t size = sp->size;
+        msegmentptr next = sp->next;
+        ++nsegs;
+        if (is_mmapped_segment(sp) && !is_extern_segment(sp)) {
+            mchunkptr p = align_as_chunk(base);
+            size_t psize = chunksize(p);
+            /* Can unmap if first chunk holds entire segment and not pinned */
+            if (!is_inuse(p) && (char*)p + psize >= base + size - TOP_FOOT_SIZE) {
+                tchunkptr tp = (tchunkptr)p;
+                assert(segment_holds(sp, (char*)sp));
+                if (p == m->dv) {
+                    m->dv = 0;
+                    m->dvsize = 0;
+                }
+                else {
+                    unlink_large_chunk(m, tp);
+                }
+                if (CALL_MUNMAP(base, size) == 0) {
+                    released += size;
+                    m->footprint -= size;
+                    /* unlink obsoleted record */
+                    sp = pred;
+                    sp->next = next;
+                }
+                else { /* back out if cannot unmap */
+                    insert_large_chunk(m, tp, psize);
+                }
+            }
         }
-      }
+        if (NO_SEGMENT_TRAVERSAL) /* scan only first segment */
+            break;
+        pred = sp;
+        sp = next;
     }
-    if (NO_SEGMENT_TRAVERSAL) /* scan only first segment */
-      break;
-    pred = sp;
-    sp = next;
-  }
-  /* Reset check counter */
-  m->release_checks = (((size_t) nsegs > (size_t) MAX_RELEASE_CHECK_RATE)?
-                       (size_t) nsegs : (size_t) MAX_RELEASE_CHECK_RATE);
-  return released;
+    /* Reset check counter */
+    m->release_checks = (((size_t)nsegs > (size_t)MAX_RELEASE_CHECK_RATE) ?
+        (size_t)nsegs : (size_t)MAX_RELEASE_CHECK_RATE);
+    return released;
 }
 
 static int sys_trim(mstate m, size_t pad) {
-  size_t released = 0;
-  ensure_initialization();
-  if (pad < MAX_REQUEST && is_initialized(m)) {
-    pad += TOP_FOOT_SIZE; /* ensure enough room for segment overhead */
-
-    if (m->topsize > pad) {
-      /* Shrink top space in granularity-size units, keeping at least one */
-      size_t unit = mparams.granularity;
-      size_t extra = ((m->topsize - pad + (unit - SIZE_T_ONE)) / unit -
-                      SIZE_T_ONE) * unit;
-      msegmentptr sp = segment_holding(m, (char*)m->top);
-
-      if (!is_extern_segment(sp)) {
-        if (is_mmapped_segment(sp)) {
-          if (HAVE_MMAP &&
-              sp->size >= extra &&
-              !has_segment_link(m, sp)) { /* can't shrink if pinned */
-            size_t newsize = sp->size - extra;
-            (void)newsize; /* placate people compiling -Wunused-variable */
-            /* Prefer mremap, fall back to munmap */
-            if ((CALL_MREMAP(sp->base, sp->size, newsize, 0) != MFAIL) ||
-                (CALL_MUNMAP(sp->base + newsize, extra) == 0)) {
-              released = extra;
+    size_t released = 0;
+    ensure_initialization();
+    if (pad < MAX_REQUEST && is_initialized(m)) {
+        pad += TOP_FOOT_SIZE; /* ensure enough room for segment overhead */
+
+        if (m->topsize > pad) {
+            /* Shrink top space in granularity-size units, keeping at least one */
+            size_t unit = mparams.granularity;
+            size_t extra = ((m->topsize - pad + (unit - SIZE_T_ONE)) / unit -
+                SIZE_T_ONE) * unit;
+            msegmentptr sp = segment_holding(m, (char*)m->top);
+
+            if (!is_extern_segment(sp)) {
+                if (is_mmapped_segment(sp)) {
+                    if (HAVE_MMAP &&
+                        sp->size >= extra &&
+                        !has_segment_link(m, sp)) { /* can't shrink if pinned */
+                        size_t newsize = sp->size - extra;
+                        (void)newsize; /* placate people compiling -Wunused-variable */
+                        /* Prefer mremap, fall back to munmap */
+                        if ((CALL_MREMAP(sp->base, sp->size, newsize, 0) != MFAIL) ||
+                            (CALL_MUNMAP(sp->base + newsize, extra) == 0)) {
+                            released = extra;
+                        }
+                    }
+                }
+                else if (HAVE_MORECORE) {
+                    if (extra >= HALF_MAX_SIZE_T) /* Avoid wrapping negative */
+                        extra = (HALF_MAX_SIZE_T)+SIZE_T_ONE - unit;
+                    ACQUIRE_MALLOC_GLOBAL_LOCK();
+                    {
+                        /* Make sure end of memory is where we last set it. */
+                        char* old_br = (char*)(CALL_MORECORE(0));
+                        if (old_br == sp->base + sp->size) {
+                            char* rel_br = (char*)(CALL_MORECORE(-extra));
+                            char* new_br = (char*)(CALL_MORECORE(0));
+                            if (rel_br != CMFAIL && new_br < old_br)
+                                released = old_br - new_br;
+                        }
+                    }
+                    RELEASE_MALLOC_GLOBAL_LOCK();
+                }
             }
-          }
-        }
-        else if (HAVE_MORECORE) {
-          if (extra >= HALF_MAX_SIZE_T) /* Avoid wrapping negative */
-            extra = (HALF_MAX_SIZE_T) + SIZE_T_ONE - unit;
-          ACQUIRE_MALLOC_GLOBAL_LOCK();
-          {
-            /* Make sure end of memory is where we last set it. */
-            char* old_br = (char*)(CALL_MORECORE(0));
-            if (old_br == sp->base + sp->size) {
-              char* rel_br = (char*)(CALL_MORECORE(-extra));
-              char* new_br = (char*)(CALL_MORECORE(0));
-              if (rel_br != CMFAIL && new_br < old_br)
-                released = old_br - new_br;
+
+            if (released != 0) {
+                sp->size -= released;
+                m->footprint -= released;
+                init_top(m, m->top, m->topsize - released);
+                check_top_chunk(m, m->top);
             }
-          }
-          RELEASE_MALLOC_GLOBAL_LOCK();
         }
-      }
 
-      if (released != 0) {
-        sp->size -= released;
-        m->footprint -= released;
-        init_top(m, m->top, m->topsize - released);
-        check_top_chunk(m, m->top);
-      }
-    }
-
-    /* Unmap any unused mmapped segments */
-    if (HAVE_MMAP)
-      released += release_unused_segments(m);
+        /* Unmap any unused mmapped segments */
+        if (HAVE_MMAP)
+            released += release_unused_segments(m);
 
-    /* On failure, disable autotrim to avoid repeated failed future calls */
-    if (released == 0 && m->topsize > m->trim_check)
-      m->trim_check = MAX_SIZE_T;
-  }
+        /* On failure, disable autotrim to avoid repeated failed future calls */
+        if (released == 0 && m->topsize > m->trim_check)
+            m->trim_check = MAX_SIZE_T;
+    }
 
-  return (released != 0)? 1 : 0;
+    return (released != 0) ? 1 : 0;
 }
 
 /* Consolidate and bin a chunk. Differs from exported versions
    of free mainly in that the chunk need not be marked as inuse.
 */
 static void dispose_chunk(mstate m, mchunkptr p, size_t psize) {
-  mchunkptr next = chunk_plus_offset(p, psize);
-  if (!pinuse(p)) {
-    mchunkptr prev;
-    size_t prevsize = p->prev_foot;
-    if (is_mmapped(p)) {
-      psize += prevsize + MMAP_FOOT_PAD;
-      if (CALL_MUNMAP((char*)p - prevsize, psize) == 0)
-        m->footprint -= psize;
-      return;
-    }
-    prev = chunk_minus_offset(p, prevsize);
-    psize += prevsize;
-    p = prev;
-    if (RTCHECK(ok_address(m, prev))) { /* consolidate backward */
-      if (p != m->dv) {
-        unlink_chunk(m, p, prevsize);
-      }
-      else if ((next->head & INUSE_BITS) == INUSE_BITS) {
-        m->dvsize = psize;
-        set_free_with_pinuse(p, psize, next);
-        return;
-      }
-    }
-    else {
-      CORRUPTION_ERROR_ACTION(m);
-      return;
+    mchunkptr next = chunk_plus_offset(p, psize);
+    if (!pinuse(p)) {
+        mchunkptr prev;
+        size_t prevsize = p->prev_foot;
+        if (is_mmapped(p)) {
+            psize += prevsize + MMAP_FOOT_PAD;
+            if (CALL_MUNMAP((char*)p - prevsize, psize) == 0)
+                m->footprint -= psize;
+            return;
+        }
+        prev = chunk_minus_offset(p, prevsize);
+        psize += prevsize;
+        p = prev;
+        if (RTCHECK(ok_address(m, prev))) { /* consolidate backward */
+            if (p != m->dv) {
+                unlink_chunk(m, p, prevsize);
+            }
+            else if ((next->head & INUSE_BITS) == INUSE_BITS) {
+                m->dvsize = psize;
+                set_free_with_pinuse(p, psize, next);
+                return;
+            }
+        }
+        else {
+            CORRUPTION_ERROR_ACTION(m);
+            return;
+        }
     }
-  }
-  if (RTCHECK(ok_address(m, next))) {
-    if (!cinuse(next)) {  /* consolidate forward */
-      if (next == m->top) {
-        size_t tsize = m->topsize += psize;
-        m->top = p;
-        p->head = tsize | PINUSE_BIT;
-        if (p == m->dv) {
-          m->dv = 0;
-          m->dvsize = 0;
+    if (RTCHECK(ok_address(m, next))) {
+        if (!cinuse(next)) {  /* consolidate forward */
+            if (next == m->top) {
+                size_t tsize = m->topsize += psize;
+                m->top = p;
+                p->head = tsize | PINUSE_BIT;
+                if (p == m->dv) {
+                    m->dv = 0;
+                    m->dvsize = 0;
+                }
+                return;
+            }
+            else if (next == m->dv) {
+                size_t dsize = m->dvsize += psize;
+                m->dv = p;
+                set_size_and_pinuse_of_free_chunk(p, dsize);
+                return;
+            }
+            else {
+                size_t nsize = chunksize(next);
+                psize += nsize;
+                unlink_chunk(m, next, nsize);
+                set_size_and_pinuse_of_free_chunk(p, psize);
+                if (p == m->dv) {
+                    m->dvsize = psize;
+                    return;
+                }
+            }
         }
-        return;
-      }
-      else if (next == m->dv) {
-        size_t dsize = m->dvsize += psize;
-        m->dv = p;
-        set_size_and_pinuse_of_free_chunk(p, dsize);
-        return;
-      }
-      else {
-        size_t nsize = chunksize(next);
-        psize += nsize;
-        unlink_chunk(m, next, nsize);
-        set_size_and_pinuse_of_free_chunk(p, psize);
-        if (p == m->dv) {
-          m->dvsize = psize;
-          return;
+        else {
+            set_free_with_pinuse(p, psize, next);
         }
-      }
+        insert_chunk(m, p, psize);
     }
     else {
-      set_free_with_pinuse(p, psize, next);
+        CORRUPTION_ERROR_ACTION(m);
     }
-    insert_chunk(m, p, psize);
-  }
-  else {
-    CORRUPTION_ERROR_ACTION(m);
-  }
 }
 
 /* ---------------------------- malloc --------------------------- */
 
 /* allocate a large request from the best fitting chunk in a treebin */
 static void* tmalloc_large(mstate m, size_t nb) {
-  tchunkptr v = 0;
-  size_t rsize = -nb; /* Unsigned negation */
-  tchunkptr t;
-  bindex_t idx;
-  compute_tree_index(nb, idx);
-  if ((t = *treebin_at(m, idx)) != 0) {
-    /* Traverse tree for this bin looking for node with size == nb */
-    size_t sizebits = nb << leftshift_for_tree_index(idx);
-    tchunkptr rst = 0;  /* The deepest untaken right subtree */
-    for (;;) {
-      tchunkptr rt;
-      size_t trem = chunksize(t) - nb;
-      if (trem < rsize) {
-        v = t;
-        if ((rsize = trem) == 0)
-          break;
-      }
-      rt = t->child[1];
-      t = t->child[(sizebits >> (SIZE_T_BITSIZE-SIZE_T_ONE)) & 1];
-      if (rt != 0 && rt != t)
-        rst = rt;
-      if (t == 0) {
-        t = rst; /* set t to least subtree holding sizes > nb */
-        break;
-      }
-      sizebits <<= 1;
+    tchunkptr v = 0;
+    size_t rsize = -nb; /* Unsigned negation */
+    tchunkptr t;
+    bindex_t idx;
+    compute_tree_index(nb, idx);
+    if ((t = *treebin_at(m, idx)) != 0) {
+        /* Traverse tree for this bin looking for node with size == nb */
+        size_t sizebits = nb << leftshift_for_tree_index(idx);
+        tchunkptr rst = 0;  /* The deepest untaken right subtree */
+        for (;;) {
+            tchunkptr rt;
+            size_t trem = chunksize(t) - nb;
+            if (trem < rsize) {
+                v = t;
+                if ((rsize = trem) == 0)
+                    break;
+            }
+            rt = t->child[1];
+            t = t->child[(sizebits >> (SIZE_T_BITSIZE - SIZE_T_ONE)) & 1];
+            if (rt != 0 && rt != t)
+                rst = rt;
+            if (t == 0) {
+                t = rst; /* set t to least subtree holding sizes > nb */
+                break;
+            }
+            sizebits <<= 1;
+        }
     }
-  }
-  if (t == 0 && v == 0) { /* set t to root of next non-empty treebin */
-    binmap_t leftbits = left_bits(idx2bit(idx)) & m->treemap;
-    if (leftbits != 0) {
-      bindex_t i;
-      binmap_t leastbit = least_bit(leftbits);
-      compute_bit2idx(leastbit, i);
-      t = *treebin_at(m, i);
+    if (t == 0 && v == 0) { /* set t to root of next non-empty treebin */
+        binmap_t leftbits = left_bits(idx2bit(idx)) & m->treemap;
+        if (leftbits != 0) {
+            bindex_t i;
+            binmap_t leastbit = least_bit(leftbits);
+            compute_bit2idx(leastbit, i);
+            t = *treebin_at(m, i);
+        }
     }
-  }
 
-  while (t != 0) { /* find smallest of tree or subtree */
-    size_t trem = chunksize(t) - nb;
-    if (trem < rsize) {
-      rsize = trem;
-      v = t;
+    while (t != 0) { /* find smallest of tree or subtree */
+        size_t trem = chunksize(t) - nb;
+        if (trem < rsize) {
+            rsize = trem;
+            v = t;
+        }
+        t = leftmost_child(t);
     }
-    t = leftmost_child(t);
-  }
 
-  /*  If dv is a better fit, return 0 so malloc will use it */
-  if (v != 0 && rsize < (size_t)(m->dvsize - nb)) {
-    if (RTCHECK(ok_address(m, v))) { /* split */
-      mchunkptr r = chunk_plus_offset(v, nb);
-      assert(chunksize(v) == rsize + nb);
-      if (RTCHECK(ok_next(v, r))) {
-        unlink_large_chunk(m, v);
-        if (rsize < MIN_CHUNK_SIZE)
-          set_inuse_and_pinuse(m, v, (rsize + nb));
-        else {
-          set_size_and_pinuse_of_inuse_chunk(m, v, nb);
-          set_size_and_pinuse_of_free_chunk(r, rsize);
-          insert_chunk(m, r, rsize);
+    /*  If dv is a better fit, return 0 so malloc will use it */
+    if (v != 0 && rsize < (size_t)(m->dvsize - nb)) {
+        if (RTCHECK(ok_address(m, v))) { /* split */
+            mchunkptr r = chunk_plus_offset(v, nb);
+            assert(chunksize(v) == rsize + nb);
+            if (RTCHECK(ok_next(v, r))) {
+                unlink_large_chunk(m, v);
+                if (rsize < MIN_CHUNK_SIZE)
+                    set_inuse_and_pinuse(m, v, (rsize + nb));
+                else {
+                    set_size_and_pinuse_of_inuse_chunk(m, v, nb);
+                    set_size_and_pinuse_of_free_chunk(r, rsize);
+                    insert_chunk(m, r, rsize);
+                }
+                return chunk2mem(v);
+            }
         }
-        return chunk2mem(v);
-      }
+        CORRUPTION_ERROR_ACTION(m);
     }
-    CORRUPTION_ERROR_ACTION(m);
-  }
-  return 0;
+    return 0;
 }
 
 /* allocate a small request from the best fitting chunk in a treebin */
 static void* tmalloc_small(mstate m, size_t nb) {
-  tchunkptr t, v;
-  size_t rsize;
-  bindex_t i;
-  binmap_t leastbit = least_bit(m->treemap);
-  compute_bit2idx(leastbit, i);
-  v = t = *treebin_at(m, i);
-  rsize = chunksize(t) - nb;
-
-  while ((t = leftmost_child(t)) != 0) {
-    size_t trem = chunksize(t) - nb;
-    if (trem < rsize) {
-      rsize = trem;
-      v = t;
+    tchunkptr t, v;
+    size_t rsize;
+    bindex_t i;
+    binmap_t leastbit = least_bit(m->treemap);
+    compute_bit2idx(leastbit, i);
+    v = t = *treebin_at(m, i);
+    rsize = chunksize(t) - nb;
+
+    while ((t = leftmost_child(t)) != 0) {
+        size_t trem = chunksize(t) - nb;
+        if (trem < rsize) {
+            rsize = trem;
+            v = t;
+        }
     }
-  }
 
-  if (RTCHECK(ok_address(m, v))) {
-    mchunkptr r = chunk_plus_offset(v, nb);
-    assert(chunksize(v) == rsize + nb);
-    if (RTCHECK(ok_next(v, r))) {
-      unlink_large_chunk(m, v);
-      if (rsize < MIN_CHUNK_SIZE)
-        set_inuse_and_pinuse(m, v, (rsize + nb));
-      else {
-        set_size_and_pinuse_of_inuse_chunk(m, v, nb);
-        set_size_and_pinuse_of_free_chunk(r, rsize);
-        replace_dv(m, r, rsize);
-      }
-      return chunk2mem(v);
+    if (RTCHECK(ok_address(m, v))) {
+        mchunkptr r = chunk_plus_offset(v, nb);
+        assert(chunksize(v) == rsize + nb);
+        if (RTCHECK(ok_next(v, r))) {
+            unlink_large_chunk(m, v);
+            if (rsize < MIN_CHUNK_SIZE)
+                set_inuse_and_pinuse(m, v, (rsize + nb));
+            else {
+                set_size_and_pinuse_of_inuse_chunk(m, v, nb);
+                set_size_and_pinuse_of_free_chunk(r, rsize);
+                replace_dv(m, r, rsize);
+            }
+            return chunk2mem(v);
+        }
     }
-  }
 
-  CORRUPTION_ERROR_ACTION(m);
-  return 0;
+    CORRUPTION_ERROR_ACTION(m);
+    return 0;
 }
 
 #if !ONLY_MSPACES
 
 void* dlmalloc(size_t bytes) {
-  /*
-     Basic algorithm:
-     If a small request (< 256 bytes minus per-chunk overhead):
-       1. If one exists, use a remainderless chunk in associated smallbin.
-          (Remainderless means that there are too few excess bytes to
-          represent as a chunk.)
-       2. If it is big enough, use the dv chunk, which is normally the
-          chunk adjacent to the one used for the most recent small request.
-       3. If one exists, split the smallest available chunk in a bin,
-          saving remainder in dv.
-       4. If it is big enough, use the top chunk.
-       5. If available, get memory from system and use it
-     Otherwise, for a large request:
-       1. Find the smallest available binned chunk that fits, and use it
-          if it is better fitting than dv chunk, splitting if necessary.
-       2. If better fitting than any binned chunk, use the dv chunk.
-       3. If it is big enough, use the top chunk.
-       4. If request size >= mmap threshold, try to directly mmap this chunk.
-       5. If available, get memory from system and use it
-
-     The ugly goto's here ensure that postaction occurs along all paths.
-  */
+    /*
+       Basic algorithm:
+       If a small request (< 256 bytes minus per-chunk overhead):
+         1. If one exists, use a remainderless chunk in associated smallbin.
+            (Remainderless means that there are too few excess bytes to
+            represent as a chunk.)
+         2. If it is big enough, use the dv chunk, which is normally the
+            chunk adjacent to the one used for the most recent small request.
+         3. If one exists, split the smallest available chunk in a bin,
+            saving remainder in dv.
+         4. If it is big enough, use the top chunk.
+         5. If available, get memory from system and use it
+       Otherwise, for a large request:
+         1. Find the smallest available binned chunk that fits, and use it
+            if it is better fitting than dv chunk, splitting if necessary.
+         2. If better fitting than any binned chunk, use the dv chunk.
+         3. If it is big enough, use the top chunk.
+         4. If request size >= mmap threshold, try to directly mmap this chunk.
+         5. If available, get memory from system and use it
+
+       The ugly goto's here ensure that postaction occurs along all paths.
+    */
 
 #if USE_LOCKS
-  ensure_initialization(); /* initialize in sys_alloc if not using locks */
+    ensure_initialization(); /* initialize in sys_alloc if not using locks */
 #endif
 
-  if (!PREACTION(gm)) {
-    void* mem;
-    size_t nb;
-    if (bytes <= MAX_SMALL_REQUEST) {
-      bindex_t idx;
-      binmap_t smallbits;
-      nb = (bytes < MIN_REQUEST)? MIN_CHUNK_SIZE : pad_request(bytes);
-      idx = small_index(nb);
-      smallbits = gm->smallmap >> idx;
-
-      if ((smallbits & 0x3U) != 0) { /* Remainderless fit to a smallbin. */
-        mchunkptr b, p;
-        idx += ~smallbits & 1;       /* Uses next bin if idx empty */
-        b = smallbin_at(gm, idx);
-        p = b->fd;
-        assert(chunksize(p) == small_index2size(idx));
-        unlink_first_small_chunk(gm, b, p, idx);
-        set_inuse_and_pinuse(gm, p, small_index2size(idx));
-        mem = chunk2mem(p);
-        check_malloced_chunk(gm, mem, nb);
-        goto postaction;
-      }
+    if (!PREACTION(gm)) {
+        void* mem;
+        size_t nb;
+        if (bytes <= MAX_SMALL_REQUEST) {
+            bindex_t idx;
+            binmap_t smallbits;
+            nb = (bytes < MIN_REQUEST) ? MIN_CHUNK_SIZE : pad_request(bytes);
+            idx = small_index(nb);
+            smallbits = gm->smallmap >> idx;
+
+            if ((smallbits & 0x3U) != 0) { /* Remainderless fit to a smallbin. */
+                mchunkptr b, p;
+                idx += ~smallbits & 1;       /* Uses next bin if idx empty */
+                b = smallbin_at(gm, idx);
+                p = b->fd;
+                assert(chunksize(p) == small_index2size(idx));
+                unlink_first_small_chunk(gm, b, p, idx);
+                set_inuse_and_pinuse(gm, p, small_index2size(idx));
+                mem = chunk2mem(p);
+                check_malloced_chunk(gm, mem, nb);
+                goto postaction;
+            }
 
-      else if (nb > gm->dvsize) {
-        if (smallbits != 0) { /* Use chunk in next nonempty smallbin */
-          mchunkptr b, p, r;
-          size_t rsize;
-          bindex_t i;
-          binmap_t leftbits = (smallbits << idx) & left_bits(idx2bit(idx));
-          binmap_t leastbit = least_bit(leftbits);
-          compute_bit2idx(leastbit, i);
-          b = smallbin_at(gm, i);
-          p = b->fd;
-          assert(chunksize(p) == small_index2size(i));
-          unlink_first_small_chunk(gm, b, p, i);
-          rsize = small_index2size(i) - nb;
-          /* Fit here cannot be remainderless if 4byte sizes */
-          if (SIZE_T_SIZE != 4 && rsize < MIN_CHUNK_SIZE)
-            set_inuse_and_pinuse(gm, p, small_index2size(i));
-          else {
-            set_size_and_pinuse_of_inuse_chunk(gm, p, nb);
-            r = chunk_plus_offset(p, nb);
-            set_size_and_pinuse_of_free_chunk(r, rsize);
-            replace_dv(gm, r, rsize);
-          }
-          mem = chunk2mem(p);
-          check_malloced_chunk(gm, mem, nb);
-          goto postaction;
+            else if (nb > gm->dvsize) {
+                if (smallbits != 0) { /* Use chunk in next nonempty smallbin */
+                    mchunkptr b, p, r;
+                    size_t rsize;
+                    bindex_t i;
+                    binmap_t leftbits = (smallbits << idx) & left_bits(idx2bit(idx));
+                    binmap_t leastbit = least_bit(leftbits);
+                    compute_bit2idx(leastbit, i);
+                    b = smallbin_at(gm, i);
+                    p = b->fd;
+                    assert(chunksize(p) == small_index2size(i));
+                    unlink_first_small_chunk(gm, b, p, i);
+                    rsize = small_index2size(i) - nb;
+                    /* Fit here cannot be remainderless if 4byte sizes */
+                    if (SIZE_T_SIZE != 4 && rsize < MIN_CHUNK_SIZE)
+                        set_inuse_and_pinuse(gm, p, small_index2size(i));
+                    else {
+                        set_size_and_pinuse_of_inuse_chunk(gm, p, nb);
+                        r = chunk_plus_offset(p, nb);
+                        set_size_and_pinuse_of_free_chunk(r, rsize);
+                        replace_dv(gm, r, rsize);
+                    }
+                    mem = chunk2mem(p);
+                    check_malloced_chunk(gm, mem, nb);
+                    goto postaction;
+                }
+
+                else if (gm->treemap != 0 && (mem = tmalloc_small(gm, nb)) != 0) {
+                    check_malloced_chunk(gm, mem, nb);
+                    goto postaction;
+                }
+            }
         }
-
-        else if (gm->treemap != 0 && (mem = tmalloc_small(gm, nb)) != 0) {
-          check_malloced_chunk(gm, mem, nb);
-          goto postaction;
+        else if (bytes >= MAX_REQUEST)
+            nb = MAX_SIZE_T; /* Too big to allocate. Force failure (in sys alloc) */
+        else {
+            nb = pad_request(bytes);
+            if (gm->treemap != 0 && (mem = tmalloc_large(gm, nb)) != 0) {
+                check_malloced_chunk(gm, mem, nb);
+                goto postaction;
+            }
         }
-      }
-    }
-    else if (bytes >= MAX_REQUEST)
-      nb = MAX_SIZE_T; /* Too big to allocate. Force failure (in sys alloc) */
-    else {
-      nb = pad_request(bytes);
-      if (gm->treemap != 0 && (mem = tmalloc_large(gm, nb)) != 0) {
-        check_malloced_chunk(gm, mem, nb);
-        goto postaction;
-      }
-    }
 
-    if (nb <= gm->dvsize) {
-      size_t rsize = gm->dvsize - nb;
-      mchunkptr p = gm->dv;
-      if (rsize >= MIN_CHUNK_SIZE) { /* split dv */
-        mchunkptr r = gm->dv = chunk_plus_offset(p, nb);
-        gm->dvsize = rsize;
-        set_size_and_pinuse_of_free_chunk(r, rsize);
-        set_size_and_pinuse_of_inuse_chunk(gm, p, nb);
-      }
-      else { /* exhaust dv */
-        size_t dvs = gm->dvsize;
-        gm->dvsize = 0;
-        gm->dv = 0;
-        set_inuse_and_pinuse(gm, p, dvs);
-      }
-      mem = chunk2mem(p);
-      check_malloced_chunk(gm, mem, nb);
-      goto postaction;
-    }
+        if (nb <= gm->dvsize) {
+            size_t rsize = gm->dvsize - nb;
+            mchunkptr p = gm->dv;
+            if (rsize >= MIN_CHUNK_SIZE) { /* split dv */
+                mchunkptr r = gm->dv = chunk_plus_offset(p, nb);
+                gm->dvsize = rsize;
+                set_size_and_pinuse_of_free_chunk(r, rsize);
+                set_size_and_pinuse_of_inuse_chunk(gm, p, nb);
+            }
+            else { /* exhaust dv */
+                size_t dvs = gm->dvsize;
+                gm->dvsize = 0;
+                gm->dv = 0;
+                set_inuse_and_pinuse(gm, p, dvs);
+            }
+            mem = chunk2mem(p);
+            check_malloced_chunk(gm, mem, nb);
+            goto postaction;
+        }
 
-    else if (nb < gm->topsize) { /* Split top */
-      size_t rsize = gm->topsize -= nb;
-      mchunkptr p = gm->top;
-      mchunkptr r = gm->top = chunk_plus_offset(p, nb);
-      r->head = rsize | PINUSE_BIT;
-      set_size_and_pinuse_of_inuse_chunk(gm, p, nb);
-      mem = chunk2mem(p);
-      check_top_chunk(gm, gm->top);
-      check_malloced_chunk(gm, mem, nb);
-      goto postaction;
-    }
+        else if (nb < gm->topsize) { /* Split top */
+            size_t rsize = gm->topsize -= nb;
+            mchunkptr p = gm->top;
+            mchunkptr r = gm->top = chunk_plus_offset(p, nb);
+            r->head = rsize | PINUSE_BIT;
+            set_size_and_pinuse_of_inuse_chunk(gm, p, nb);
+            mem = chunk2mem(p);
+            check_top_chunk(gm, gm->top);
+            check_malloced_chunk(gm, mem, nb);
+            goto postaction;
+        }
 
-    mem = sys_alloc(gm, nb);
+        mem = sys_alloc(gm, nb);
 
-  postaction:
-    POSTACTION(gm);
-    return mem;
-  }
+    postaction:
+        POSTACTION(gm);
+        return mem;
+    }
 
-  return 0;
+    return 0;
 }
 
 /* ---------------------------- free --------------------------- */
 
 void dlfree(void* mem) {
-  /*
-     Consolidate freed chunks with preceeding or succeeding bordering
-     free chunks, if they exist, and then place in a bin.  Intermixed
-     with special cases for top, dv, mmapped chunks, and usage errors.
-  */
+    /*
+       Consolidate freed chunks with preceeding or succeeding bordering
+       free chunks, if they exist, and then place in a bin.  Intermixed
+       with special cases for top, dv, mmapped chunks, and usage errors.
+    */
 
-  if (mem != 0) {
-    mchunkptr p  = mem2chunk(mem);
+    if (mem != 0) {
+        mchunkptr p = mem2chunk(mem);
 #if FOOTERS
-    mstate fm = get_mstate_for(p);
-    if (!ok_magic(fm)) {
-      USAGE_ERROR_ACTION(fm, p);
-      return;
-    }
+        mstate fm = get_mstate_for(p);
+        if (!ok_magic(fm)) {
+            USAGE_ERROR_ACTION(fm, p);
+            return;
+        }
 #else /* FOOTERS */
 #define fm gm
 #endif /* FOOTERS */
-    if (!PREACTION(fm)) {
-      check_inuse_chunk(fm, p);
-      if (RTCHECK(ok_address(fm, p) && ok_inuse(p))) {
-        size_t psize = chunksize(p);
-        mchunkptr next = chunk_plus_offset(p, psize);
-        if (!pinuse(p)) {
-          size_t prevsize = p->prev_foot;
-          if (is_mmapped(p)) {
-            psize += prevsize + MMAP_FOOT_PAD;
-            if (CALL_MUNMAP((char*)p - prevsize, psize) == 0)
-              fm->footprint -= psize;
-            goto postaction;
-          }
-          else {
-            mchunkptr prev = chunk_minus_offset(p, prevsize);
-            psize += prevsize;
-            p = prev;
-            if (RTCHECK(ok_address(fm, prev))) { /* consolidate backward */
-              if (p != fm->dv) {
-                unlink_chunk(fm, p, prevsize);
-              }
-              else if ((next->head & INUSE_BITS) == INUSE_BITS) {
-                fm->dvsize = psize;
-                set_free_with_pinuse(p, psize, next);
-                goto postaction;
-              }
+        if (!PREACTION(fm)) {
+            check_inuse_chunk(fm, p);
+            if (RTCHECK(ok_address(fm, p) && ok_inuse(p))) {
+                size_t psize = chunksize(p);
+                mchunkptr next = chunk_plus_offset(p, psize);
+                if (!pinuse(p)) {
+                    size_t prevsize = p->prev_foot;
+                    if (is_mmapped(p)) {
+                        psize += prevsize + MMAP_FOOT_PAD;
+                        if (CALL_MUNMAP((char*)p - prevsize, psize) == 0)
+                            fm->footprint -= psize;
+                        goto postaction;
+                    }
+                    else {
+                        mchunkptr prev = chunk_minus_offset(p, prevsize);
+                        psize += prevsize;
+                        p = prev;
+                        if (RTCHECK(ok_address(fm, prev))) { /* consolidate backward */
+                            if (p != fm->dv) {
+                                unlink_chunk(fm, p, prevsize);
+                            }
+                            else if ((next->head & INUSE_BITS) == INUSE_BITS) {
+                                fm->dvsize = psize;
+                                set_free_with_pinuse(p, psize, next);
+                                goto postaction;
+                            }
+                        }
+                        else
+                            goto erroraction;
+                    }
+                }
+
+                if (RTCHECK(ok_next(p, next) && ok_pinuse(next))) {
+                    if (!cinuse(next)) {  /* consolidate forward */
+                        if (next == fm->top) {
+                            size_t tsize = fm->topsize += psize;
+                            fm->top = p;
+                            p->head = tsize | PINUSE_BIT;
+                            if (p == fm->dv) {
+                                fm->dv = 0;
+                                fm->dvsize = 0;
+                            }
+                            if (should_trim(fm, tsize))
+                                sys_trim(fm, 0);
+                            goto postaction;
+                        }
+                        else if (next == fm->dv) {
+                            size_t dsize = fm->dvsize += psize;
+                            fm->dv = p;
+                            set_size_and_pinuse_of_free_chunk(p, dsize);
+                            goto postaction;
+                        }
+                        else {
+                            size_t nsize = chunksize(next);
+                            psize += nsize;
+                            unlink_chunk(fm, next, nsize);
+                            set_size_and_pinuse_of_free_chunk(p, psize);
+                            if (p == fm->dv) {
+                                fm->dvsize = psize;
+                                goto postaction;
+                            }
+                        }
+                    }
+                    else
+                        set_free_with_pinuse(p, psize, next);
+
+                    if (is_small(psize)) {
+                        insert_small_chunk(fm, p, psize);
+                        check_free_chunk(fm, p);
+                    }
+                    else {
+                        tchunkptr tp = (tchunkptr)p;
+                        insert_large_chunk(fm, tp, psize);
+                        check_free_chunk(fm, p);
+                        if (--fm->release_checks == 0)
+                            release_unused_segments(fm);
+                    }
+                    goto postaction;
+                }
             }
-            else
-              goto erroraction;
-          }
+        erroraction:
+            USAGE_ERROR_ACTION(fm, p);
+        postaction:
+            POSTACTION(fm);
         }
-
-        if (RTCHECK(ok_next(p, next) && ok_pinuse(next))) {
-          if (!cinuse(next)) {  /* consolidate forward */
-            if (next == fm->top) {
-              size_t tsize = fm->topsize += psize;
-              fm->top = p;
-              p->head = tsize | PINUSE_BIT;
-              if (p == fm->dv) {
-                fm->dv = 0;
-                fm->dvsize = 0;
-              }
-              if (should_trim(fm, tsize))
-                sys_trim(fm, 0);
-              goto postaction;
-            }
-            else if (next == fm->dv) {
-              size_t dsize = fm->dvsize += psize;
-              fm->dv = p;
-              set_size_and_pinuse_of_free_chunk(p, dsize);
-              goto postaction;
-            }
-            else {
-              size_t nsize = chunksize(next);
-              psize += nsize;
-              unlink_chunk(fm, next, nsize);
-              set_size_and_pinuse_of_free_chunk(p, psize);
-              if (p == fm->dv) {
-                fm->dvsize = psize;
-                goto postaction;
-              }
-            }
-          }
-          else
-            set_free_with_pinuse(p, psize, next);
-
-          if (is_small(psize)) {
-            insert_small_chunk(fm, p, psize);
-            check_free_chunk(fm, p);
-          }
-          else {
-            tchunkptr tp = (tchunkptr)p;
-            insert_large_chunk(fm, tp, psize);
-            check_free_chunk(fm, p);
-            if (--fm->release_checks == 0)
-              release_unused_segments(fm);
-          }
-          goto postaction;
-        }
-      }
-    erroraction:
-      USAGE_ERROR_ACTION(fm, p);
-    postaction:
-      POSTACTION(fm);
     }
-  }
 #if !FOOTERS
 #undef fm
 #endif /* FOOTERS */
 }
 
 void* dlcalloc(size_t n_elements, size_t elem_size) {
-  void* mem;
-  size_t req = 0;
-  if (n_elements != 0) {
-    req = n_elements * elem_size;
-    if (((n_elements | elem_size) & ~(size_t)0xffff) &&
-        (req / n_elements != elem_size))
-      req = MAX_SIZE_T; /* force downstream failure on overflow */
-  }
-  mem = dlmalloc(req);
-  if (mem != 0 && calloc_must_clear(mem2chunk(mem)))
-    memset(mem, 0, req);
-  return mem;
+    void* mem;
+    size_t req = 0;
+    if (n_elements != 0) {
+        req = n_elements * elem_size;
+        if (((n_elements | elem_size) & ~(size_t)0xffff) &&
+            (req / n_elements != elem_size))
+            req = MAX_SIZE_T; /* force downstream failure on overflow */
+    }
+    mem = dlmalloc(req);
+    if (mem != 0 && calloc_must_clear(mem2chunk(mem)))
+        memset(mem, 0, req);
+    return mem;
 }
 
 #endif /* !ONLY_MSPACES */
@@ -4804,156 +4855,156 @@ void* dlcalloc(size_t n_elements, size_t elem_size) {
 
 /* Try to realloc; only in-place unless can_move true */
 static mchunkptr try_realloc_chunk(mstate m, mchunkptr p, size_t nb,
-                                   int can_move) {
-  mchunkptr newp = 0;
-  size_t oldsize = chunksize(p);
-  mchunkptr next = chunk_plus_offset(p, oldsize);
-  if (RTCHECK(ok_address(m, p) && ok_inuse(p) &&
-              ok_next(p, next) && ok_pinuse(next))) {
-    if (is_mmapped(p)) {
-      newp = mmap_resize(m, p, nb, can_move);
-    }
-    else if (oldsize >= nb) {             /* already big enough */
-      size_t rsize = oldsize - nb;
-      if (rsize >= MIN_CHUNK_SIZE) {      /* split off remainder */
-        mchunkptr r = chunk_plus_offset(p, nb);
-        set_inuse(m, p, nb);
-        set_inuse(m, r, rsize);
-        dispose_chunk(m, r, rsize);
-      }
-      newp = p;
-    }
-    else if (next == m->top) {  /* extend into top */
-      if (oldsize + m->topsize > nb) {
-        size_t newsize = oldsize + m->topsize;
-        size_t newtopsize = newsize - nb;
-        mchunkptr newtop = chunk_plus_offset(p, nb);
-        set_inuse(m, p, nb);
-        newtop->head = newtopsize |PINUSE_BIT;
-        m->top = newtop;
-        m->topsize = newtopsize;
-        newp = p;
-      }
-    }
-    else if (next == m->dv) { /* extend into dv */
-      size_t dvs = m->dvsize;
-      if (oldsize + dvs >= nb) {
-        size_t dsize = oldsize + dvs - nb;
-        if (dsize >= MIN_CHUNK_SIZE) {
-          mchunkptr r = chunk_plus_offset(p, nb);
-          mchunkptr n = chunk_plus_offset(r, dsize);
-          set_inuse(m, p, nb);
-          set_size_and_pinuse_of_free_chunk(r, dsize);
-          clear_pinuse(n);
-          m->dvsize = dsize;
-          m->dv = r;
+    int can_move) {
+    mchunkptr newp = 0;
+    size_t oldsize = chunksize(p);
+    mchunkptr next = chunk_plus_offset(p, oldsize);
+    if (RTCHECK(ok_address(m, p) && ok_inuse(p) &&
+        ok_next(p, next) && ok_pinuse(next))) {
+        if (is_mmapped(p)) {
+            newp = mmap_resize(m, p, nb, can_move);
         }
-        else { /* exhaust dv */
-          size_t newsize = oldsize + dvs;
-          set_inuse(m, p, newsize);
-          m->dvsize = 0;
-          m->dv = 0;
+        else if (oldsize >= nb) {             /* already big enough */
+            size_t rsize = oldsize - nb;
+            if (rsize >= MIN_CHUNK_SIZE) {      /* split off remainder */
+                mchunkptr r = chunk_plus_offset(p, nb);
+                set_inuse(m, p, nb);
+                set_inuse(m, r, rsize);
+                dispose_chunk(m, r, rsize);
+            }
+            newp = p;
         }
-        newp = p;
-      }
-    }
-    else if (!cinuse(next)) { /* extend into next free chunk */
-      size_t nextsize = chunksize(next);
-      if (oldsize + nextsize >= nb) {
-        size_t rsize = oldsize + nextsize - nb;
-        unlink_chunk(m, next, nextsize);
-        if (rsize < MIN_CHUNK_SIZE) {
-          size_t newsize = oldsize + nextsize;
-          set_inuse(m, p, newsize);
+        else if (next == m->top) {  /* extend into top */
+            if (oldsize + m->topsize > nb) {
+                size_t newsize = oldsize + m->topsize;
+                size_t newtopsize = newsize - nb;
+                mchunkptr newtop = chunk_plus_offset(p, nb);
+                set_inuse(m, p, nb);
+                newtop->head = newtopsize | PINUSE_BIT;
+                m->top = newtop;
+                m->topsize = newtopsize;
+                newp = p;
+            }
         }
-        else {
-          mchunkptr r = chunk_plus_offset(p, nb);
-          set_inuse(m, p, nb);
-          set_inuse(m, r, rsize);
-          dispose_chunk(m, r, rsize);
+        else if (next == m->dv) { /* extend into dv */
+            size_t dvs = m->dvsize;
+            if (oldsize + dvs >= nb) {
+                size_t dsize = oldsize + dvs - nb;
+                if (dsize >= MIN_CHUNK_SIZE) {
+                    mchunkptr r = chunk_plus_offset(p, nb);
+                    mchunkptr n = chunk_plus_offset(r, dsize);
+                    set_inuse(m, p, nb);
+                    set_size_and_pinuse_of_free_chunk(r, dsize);
+                    clear_pinuse(n);
+                    m->dvsize = dsize;
+                    m->dv = r;
+                }
+                else { /* exhaust dv */
+                    size_t newsize = oldsize + dvs;
+                    set_inuse(m, p, newsize);
+                    m->dvsize = 0;
+                    m->dv = 0;
+                }
+                newp = p;
+            }
+        }
+        else if (!cinuse(next)) { /* extend into next free chunk */
+            size_t nextsize = chunksize(next);
+            if (oldsize + nextsize >= nb) {
+                size_t rsize = oldsize + nextsize - nb;
+                unlink_chunk(m, next, nextsize);
+                if (rsize < MIN_CHUNK_SIZE) {
+                    size_t newsize = oldsize + nextsize;
+                    set_inuse(m, p, newsize);
+                }
+                else {
+                    mchunkptr r = chunk_plus_offset(p, nb);
+                    set_inuse(m, p, nb);
+                    set_inuse(m, r, rsize);
+                    dispose_chunk(m, r, rsize);
+                }
+                newp = p;
+            }
         }
-        newp = p;
-      }
     }
-  }
-  else {
-    USAGE_ERROR_ACTION(m, chunk2mem(p));
-  }
-  return newp;
+    else {
+        USAGE_ERROR_ACTION(m, chunk2mem(p));
+    }
+    return newp;
 }
 
 static void* internal_memalign(mstate m, size_t alignment, size_t bytes) {
-  void* mem = 0;
-  if (alignment <  MIN_CHUNK_SIZE) /* must be at least a minimum chunk size */
-    alignment = MIN_CHUNK_SIZE;
-  if ((alignment & (alignment-SIZE_T_ONE)) != 0) {/* Ensure a power of 2 */
-    size_t a = MALLOC_ALIGNMENT << 1;
-    while (a < alignment) a <<= 1;
-    alignment = a;
-  }
-  if (bytes >= MAX_REQUEST - alignment) {
-    if (m != 0)  { /* Test isn't needed but avoids compiler warning */
-      MALLOC_FAILURE_ACTION;
+    void* mem = 0;
+    if (alignment < MIN_CHUNK_SIZE) /* must be at least a minimum chunk size */
+        alignment = MIN_CHUNK_SIZE;
+    if ((alignment & (alignment - SIZE_T_ONE)) != 0) {/* Ensure a power of 2 */
+        size_t a = MALLOC_ALIGNMENT << 1;
+        while (a < alignment) a <<= 1;
+        alignment = a;
     }
-  }
-  else {
-    size_t nb = request2size(bytes);
-    size_t req = nb + alignment + MIN_CHUNK_SIZE - CHUNK_OVERHEAD;
-    mem = internal_malloc(m, req);
-    if (mem != 0) {
-      mchunkptr p = mem2chunk(mem);
-      if (PREACTION(m))
-        return 0;
-      if ((((size_t)(mem)) & (alignment - 1)) != 0) { /* misaligned */
-        /*
-          Find an aligned spot inside chunk.  Since we need to give
-          back leading space in a chunk of at least MIN_CHUNK_SIZE, if
-          the first calculation places us at a spot with less than
-          MIN_CHUNK_SIZE leader, we can move to the next aligned spot.
-          We've allocated enough total room so that this is always
-          possible.
-        */
-        char* br = (char*)mem2chunk((size_t)(((size_t)((char*)mem + alignment -
-                                                       SIZE_T_ONE)) &
-                                             -alignment));
-        char* pos = ((size_t)(br - (char*)(p)) >= MIN_CHUNK_SIZE)?
-          br : br+alignment;
-        mchunkptr newp = (mchunkptr)pos;
-        size_t leadsize = pos - (char*)(p);
-        size_t newsize = chunksize(p) - leadsize;
-
-        if (is_mmapped(p)) { /* For mmapped chunks, just adjust offset */
-          newp->prev_foot = p->prev_foot + leadsize;
-          newp->head = newsize;
+    if (bytes >= MAX_REQUEST - alignment) {
+        if (m != 0) { /* Test isn't needed but avoids compiler warning */
+            MALLOC_FAILURE_ACTION;
         }
-        else { /* Otherwise, give back leader, use the rest */
-          set_inuse(m, newp, newsize);
-          set_inuse(m, p, leadsize);
-          dispose_chunk(m, p, leadsize);
-        }
-        p = newp;
-      }
+    }
+    else {
+        size_t nb = request2size(bytes);
+        size_t req = nb + alignment + MIN_CHUNK_SIZE - CHUNK_OVERHEAD;
+        mem = internal_malloc(m, req);
+        if (mem != 0) {
+            mchunkptr p = mem2chunk(mem);
+            if (PREACTION(m))
+                return 0;
+            if ((((size_t)(mem)) & (alignment - 1)) != 0) { /* misaligned */
+              /*
+                Find an aligned spot inside chunk.  Since we need to give
+                back leading space in a chunk of at least MIN_CHUNK_SIZE, if
+                the first calculation places us at a spot with less than
+                MIN_CHUNK_SIZE leader, we can move to the next aligned spot.
+                We've allocated enough total room so that this is always
+                possible.
+              */
+                char* br = (char*)mem2chunk((size_t)(((size_t)((char*)mem + alignment -
+                    SIZE_T_ONE)) &
+                    -alignment));
+                char* pos = ((size_t)(br - (char*)(p)) >= MIN_CHUNK_SIZE) ?
+                    br : br + alignment;
+                mchunkptr newp = (mchunkptr)pos;
+                size_t leadsize = pos - (char*)(p);
+                size_t newsize = chunksize(p) - leadsize;
+
+                if (is_mmapped(p)) { /* For mmapped chunks, just adjust offset */
+                    newp->prev_foot = p->prev_foot + leadsize;
+                    newp->head = newsize;
+                }
+                else { /* Otherwise, give back leader, use the rest */
+                    set_inuse(m, newp, newsize);
+                    set_inuse(m, p, leadsize);
+                    dispose_chunk(m, p, leadsize);
+                }
+                p = newp;
+            }
 
-      /* Give back spare room at the end */
-      if (!is_mmapped(p)) {
-        size_t size = chunksize(p);
-        if (size > nb + MIN_CHUNK_SIZE) {
-          size_t remainder_size = size - nb;
-          mchunkptr remainder = chunk_plus_offset(p, nb);
-          set_inuse(m, p, nb);
-          set_inuse(m, remainder, remainder_size);
-          dispose_chunk(m, remainder, remainder_size);
-        }
-      }
+            /* Give back spare room at the end */
+            if (!is_mmapped(p)) {
+                size_t size = chunksize(p);
+                if (size > nb + MIN_CHUNK_SIZE) {
+                    size_t remainder_size = size - nb;
+                    mchunkptr remainder = chunk_plus_offset(p, nb);
+                    set_inuse(m, p, nb);
+                    set_inuse(m, remainder, remainder_size);
+                    dispose_chunk(m, remainder, remainder_size);
+                }
+            }
 
-      mem = chunk2mem(p);
-      assert (chunksize(p) >= nb);
-      assert(((size_t)mem & (alignment - 1)) == 0);
-      check_inuse_chunk(m, p);
-      POSTACTION(m);
+            mem = chunk2mem(p);
+            assert(chunksize(p) >= nb);
+            assert(((size_t)mem & (alignment - 1)) == 0);
+            check_inuse_chunk(m, p);
+            POSTACTION(m);
+        }
     }
-  }
-  return mem;
+    return mem;
 }
 
 /*
@@ -4964,122 +5015,122 @@ static void* internal_memalign(mstate m, size_t alignment, size_t bytes) {
     bit 1 set if elements should be zeroed
 */
 static void** ialloc(mstate m,
-                     size_t n_elements,
-                     size_t* sizes,
-                     int opts,
-                     void* chunks[]) {
-
-  size_t    element_size;   /* chunksize of each element, if all same */
-  size_t    contents_size;  /* total size of elements */
-  size_t    array_size;     /* request size of pointer array */
-  void*     mem;            /* malloced aggregate space */
-  mchunkptr p;              /* corresponding chunk */
-  size_t    remainder_size; /* remaining bytes while splitting */
-  void**    marray;         /* either "chunks" or malloced ptr array */
-  mchunkptr array_chunk;    /* chunk for malloced ptr array */
-  flag_t    was_enabled;    /* to disable mmap */
-  size_t    size;
-  size_t    i;
-
-  ensure_initialization();
-  /* compute array length, if needed */
-  if (chunks != 0) {
-    if (n_elements == 0)
-      return chunks; /* nothing to do */
-    marray = chunks;
-    array_size = 0;
-  }
-  else {
-    /* if empty req, must still return chunk representing empty array */
-    if (n_elements == 0)
-      return (void**)internal_malloc(m, 0);
-    marray = 0;
-    array_size = request2size(n_elements * (sizeof(void*)));
-  }
-
-  /* compute total element size */
-  if (opts & 0x1) { /* all-same-size */
-    element_size = request2size(*sizes);
-    contents_size = n_elements * element_size;
-  }
-  else { /* add up all the sizes */
-    element_size = 0;
-    contents_size = 0;
-    for (i = 0; i != n_elements; ++i)
-      contents_size += request2size(sizes[i]);
-  }
+    size_t n_elements,
+    size_t* sizes,
+    int opts,
+    void* chunks[]) {
+
+    size_t    element_size;   /* chunksize of each element, if all same */
+    size_t    contents_size;  /* total size of elements */
+    size_t    array_size;     /* request size of pointer array */
+    void* mem;            /* malloced aggregate space */
+    mchunkptr p;              /* corresponding chunk */
+    size_t    remainder_size; /* remaining bytes while splitting */
+    void** marray;         /* either "chunks" or malloced ptr array */
+    mchunkptr array_chunk;    /* chunk for malloced ptr array */
+    flag_t    was_enabled;    /* to disable mmap */
+    size_t    size;
+    size_t    i;
+
+    ensure_initialization();
+    /* compute array length, if needed */
+    if (chunks != 0) {
+        if (n_elements == 0)
+            return chunks; /* nothing to do */
+        marray = chunks;
+        array_size = 0;
+    }
+    else {
+        /* if empty req, must still return chunk representing empty array */
+        if (n_elements == 0)
+            return (void**)internal_malloc(m, 0);
+        marray = 0;
+        array_size = request2size(n_elements * (sizeof(void*)));
+    }
 
-  size = contents_size + array_size;
+    /* compute total element size */
+    if (opts & 0x1) { /* all-same-size */
+        element_size = request2size(*sizes);
+        contents_size = n_elements * element_size;
+    }
+    else { /* add up all the sizes */
+        element_size = 0;
+        contents_size = 0;
+        for (i = 0; i != n_elements; ++i)
+            contents_size += request2size(sizes[i]);
+    }
 
-  /*
-     Allocate the aggregate chunk.  First disable direct-mmapping so
-     malloc won't use it, since we would not be able to later
-     free/realloc space internal to a segregated mmap region.
-  */
-  was_enabled = use_mmap(m);
-  disable_mmap(m);
-  mem = internal_malloc(m, size - CHUNK_OVERHEAD);
-  if (was_enabled)
-    enable_mmap(m);
-  if (mem == 0)
-    return 0;
+    size = contents_size + array_size;
 
-  if (PREACTION(m)) return 0;
-  p = mem2chunk(mem);
-  remainder_size = chunksize(p);
+    /*
+       Allocate the aggregate chunk.  First disable direct-mmapping so
+       malloc won't use it, since we would not be able to later
+       free/realloc space internal to a segregated mmap region.
+    */
+    was_enabled = use_mmap(m);
+    disable_mmap(m);
+    mem = internal_malloc(m, size - CHUNK_OVERHEAD);
+    if (was_enabled)
+        enable_mmap(m);
+    if (mem == 0)
+        return 0;
 
-  assert(!is_mmapped(p));
+    if (PREACTION(m)) return 0;
+    p = mem2chunk(mem);
+    remainder_size = chunksize(p);
 
-  if (opts & 0x2) {       /* optionally clear the elements */
-    memset((size_t*)mem, 0, remainder_size - SIZE_T_SIZE - array_size);
-  }
+    assert(!is_mmapped(p));
 
-  /* If not provided, allocate the pointer array as final part of chunk */
-  if (marray == 0) {
-    size_t  array_chunk_size;
-    array_chunk = chunk_plus_offset(p, contents_size);
-    array_chunk_size = remainder_size - contents_size;
-    marray = (void**) (chunk2mem(array_chunk));
-    set_size_and_pinuse_of_inuse_chunk(m, array_chunk, array_chunk_size);
-    remainder_size = contents_size;
-  }
+    if (opts & 0x2) {       /* optionally clear the elements */
+        memset((size_t*)mem, 0, remainder_size - SIZE_T_SIZE - array_size);
+    }
 
-  /* split out elements */
-  for (i = 0; ; ++i) {
-    marray[i] = chunk2mem(p);
-    if (i != n_elements-1) {
-      if (element_size != 0)
-        size = element_size;
-      else
-        size = request2size(sizes[i]);
-      remainder_size -= size;
-      set_size_and_pinuse_of_inuse_chunk(m, p, size);
-      p = chunk_plus_offset(p, size);
+    /* If not provided, allocate the pointer array as final part of chunk */
+    if (marray == 0) {
+        size_t  array_chunk_size;
+        array_chunk = chunk_plus_offset(p, contents_size);
+        array_chunk_size = remainder_size - contents_size;
+        marray = (void**)(chunk2mem(array_chunk));
+        set_size_and_pinuse_of_inuse_chunk(m, array_chunk, array_chunk_size);
+        remainder_size = contents_size;
     }
-    else { /* the final element absorbs any overallocation slop */
-      set_size_and_pinuse_of_inuse_chunk(m, p, remainder_size);
-      break;
+
+    /* split out elements */
+    for (i = 0; ; ++i) {
+        marray[i] = chunk2mem(p);
+        if (i != n_elements - 1) {
+            if (element_size != 0)
+                size = element_size;
+            else
+                size = request2size(sizes[i]);
+            remainder_size -= size;
+            set_size_and_pinuse_of_inuse_chunk(m, p, size);
+            p = chunk_plus_offset(p, size);
+        }
+        else { /* the final element absorbs any overallocation slop */
+            set_size_and_pinuse_of_inuse_chunk(m, p, remainder_size);
+            break;
+        }
     }
-  }
 
 #if DEBUG
-  if (marray != chunks) {
-    /* final element must have exactly exhausted chunk */
-    if (element_size != 0) {
-      assert(remainder_size == element_size);
-    }
-    else {
-      assert(remainder_size == request2size(sizes[i]));
+    if (marray != chunks) {
+        /* final element must have exactly exhausted chunk */
+        if (element_size != 0) {
+            assert(remainder_size == element_size);
+        }
+        else {
+            assert(remainder_size == request2size(sizes[i]));
+        }
+        check_inuse_chunk(m, mem2chunk(marray));
     }
-    check_inuse_chunk(m, mem2chunk(marray));
-  }
-  for (i = 0; i != n_elements; ++i)
-    check_inuse_chunk(m, mem2chunk(marray[i]));
+    for (i = 0; i != n_elements; ++i)
+        check_inuse_chunk(m, mem2chunk(marray[i]));
 
 #endif /* DEBUG */
 
-  POSTACTION(m);
-  return marray;
+    POSTACTION(m);
+    return marray;
 }
 
 /* Try to free all pointers in the given array.
@@ -5090,86 +5141,86 @@ static void** ialloc(mstate m,
    with ialloc or the array is sorted.
 */
 static size_t internal_bulk_free(mstate m, void* array[], size_t nelem) {
-  size_t unfreed = 0;
-  if (!PREACTION(m)) {
-    void** a;
-    void** fence = &(array[nelem]);
-    for (a = array; a != fence; ++a) {
-      void* mem = *a;
-      if (mem != 0) {
-        mchunkptr p = mem2chunk(mem);
-        size_t psize = chunksize(p);
+    size_t unfreed = 0;
+    if (!PREACTION(m)) {
+        void** a;
+        void** fence = &(array[nelem]);
+        for (a = array; a != fence; ++a) {
+            void* mem = *a;
+            if (mem != 0) {
+                mchunkptr p = mem2chunk(mem);
+                size_t psize = chunksize(p);
 #if FOOTERS
-        if (get_mstate_for(p) != m) {
-          ++unfreed;
-          continue;
-        }
+                if (get_mstate_for(p) != m) {
+                    ++unfreed;
+                    continue;
+                }
 #endif
-        check_inuse_chunk(m, p);
-        *a = 0;
-        if (RTCHECK(ok_address(m, p) && ok_inuse(p))) {
-          void ** b = a + 1; /* try to merge with next chunk */
-          mchunkptr next = next_chunk(p);
-          if (b != fence && *b == chunk2mem(next)) {
-            size_t newsize = chunksize(next) + psize;
-            set_inuse(m, p, newsize);
-            *b = chunk2mem(p);
-          }
-          else
-            dispose_chunk(m, p, psize);
-        }
-        else {
-          CORRUPTION_ERROR_ACTION(m);
-          break;
+                check_inuse_chunk(m, p);
+                *a = 0;
+                if (RTCHECK(ok_address(m, p) && ok_inuse(p))) {
+                    void** b = a + 1; /* try to merge with next chunk */
+                    mchunkptr next = next_chunk(p);
+                    if (b != fence && *b == chunk2mem(next)) {
+                        size_t newsize = chunksize(next) + psize;
+                        set_inuse(m, p, newsize);
+                        *b = chunk2mem(p);
+                    }
+                    else
+                        dispose_chunk(m, p, psize);
+                }
+                else {
+                    CORRUPTION_ERROR_ACTION(m);
+                    break;
+                }
+            }
         }
-      }
+        if (should_trim(m, m->topsize))
+            sys_trim(m, 0);
+        POSTACTION(m);
     }
-    if (should_trim(m, m->topsize))
-      sys_trim(m, 0);
-    POSTACTION(m);
-  }
-  return unfreed;
+    return unfreed;
 }
 
 /* Traversal */
 #if MALLOC_INSPECT_ALL
 static void internal_inspect_all(mstate m,
-                                 void(*handler)(void *start,
-                                                void *end,
-                                                size_t used_bytes,
-                                                void* callback_arg),
-                                 void* arg) {
-  if (is_initialized(m)) {
-    mchunkptr top = m->top;
-    msegmentptr s;
-    for (s = &m->seg; s != 0; s = s->next) {
-      mchunkptr q = align_as_chunk(s->base);
-      while (segment_holds(s, q) && q->head != FENCEPOST_HEAD) {
-        mchunkptr next = next_chunk(q);
-        size_t sz = chunksize(q);
-        size_t used;
-        void* start;
-        if (is_inuse(q)) {
-          used = sz - CHUNK_OVERHEAD; /* must not be mmapped */
-          start = chunk2mem(q);
-        }
-        else {
-          used = 0;
-          if (is_small(sz)) {     /* offset by possible bookkeeping */
-            start = (void*)((char*)q + sizeof(struct malloc_chunk));
-          }
-          else {
-            start = (void*)((char*)q + sizeof(struct malloc_tree_chunk));
-          }
+    void(*handler)(void* start,
+        void* end,
+        size_t used_bytes,
+        void* callback_arg),
+    void* arg) {
+    if (is_initialized(m)) {
+        mchunkptr top = m->top;
+        msegmentptr s;
+        for (s = &m->seg; s != 0; s = s->next) {
+            mchunkptr q = align_as_chunk(s->base);
+            while (segment_holds(s, q) && q->head != FENCEPOST_HEAD) {
+                mchunkptr next = next_chunk(q);
+                size_t sz = chunksize(q);
+                size_t used;
+                void* start;
+                if (is_inuse(q)) {
+                    used = sz - CHUNK_OVERHEAD; /* must not be mmapped */
+                    start = chunk2mem(q);
+                }
+                else {
+                    used = 0;
+                    if (is_small(sz)) {     /* offset by possible bookkeeping */
+                        start = (void*)((char*)q + sizeof(struct malloc_chunk));
+                    }
+                    else {
+                        start = (void*)((char*)q + sizeof(struct malloc_tree_chunk));
+                    }
+                }
+                if (start < (void*)next)  /* skip if all space is bookkeeping */
+                    handler(start, next, used, arg);
+                if (q == top)
+                    break;
+                q = next;
+            }
         }
-        if (start < (void*)next)  /* skip if all space is bookkeeping */
-          handler(start, next, used, arg);
-        if (q == top)
-          break;
-        q = next;
-      }
     }
-  }
 }
 #endif /* MALLOC_INSPECT_ALL */
 
@@ -5178,211 +5229,211 @@ static void internal_inspect_all(mstate m,
 #if !ONLY_MSPACES
 
 void* dlrealloc(void* oldmem, size_t bytes) {
-  void* mem = 0;
-  if (oldmem == 0) {
-    mem = dlmalloc(bytes);
-  }
-  else if (bytes >= MAX_REQUEST) {
-    MALLOC_FAILURE_ACTION;
-  }
+    void* mem = 0;
+    if (oldmem == 0) {
+        mem = dlmalloc(bytes);
+    }
+    else if (bytes >= MAX_REQUEST) {
+        MALLOC_FAILURE_ACTION;
+    }
 #ifdef REALLOC_ZERO_BYTES_FREES
-  else if (bytes == 0) {
-    dlfree(oldmem);
-  }
+    else if (bytes == 0) {
+        dlfree(oldmem);
+    }
 #endif /* REALLOC_ZERO_BYTES_FREES */
-  else {
-    size_t nb = request2size(bytes);
-    mchunkptr oldp = mem2chunk(oldmem);
+    else {
+        size_t nb = request2size(bytes);
+        mchunkptr oldp = mem2chunk(oldmem);
 #if ! FOOTERS
-    mstate m = gm;
+        mstate m = gm;
 #else /* FOOTERS */
-    mstate m = get_mstate_for(oldp);
-    if (!ok_magic(m)) {
-      USAGE_ERROR_ACTION(m, oldmem);
-      return 0;
-    }
+        mstate m = get_mstate_for(oldp);
+        if (!ok_magic(m)) {
+            USAGE_ERROR_ACTION(m, oldmem);
+            return 0;
+        }
 #endif /* FOOTERS */
-    if (!PREACTION(m)) {
-      mchunkptr newp = try_realloc_chunk(m, oldp, nb, 1);
-      POSTACTION(m);
-      if (newp != 0) {
-        check_inuse_chunk(m, newp);
-        mem = chunk2mem(newp);
-      }
-      else {
-        mem = internal_malloc(m, bytes);
-        if (mem != 0) {
-          size_t oc = chunksize(oldp) - overhead_for(oldp);
-          memcpy(mem, oldmem, (oc < bytes)? oc : bytes);
-          internal_free(m, oldmem);
+        if (!PREACTION(m)) {
+            mchunkptr newp = try_realloc_chunk(m, oldp, nb, 1);
+            POSTACTION(m);
+            if (newp != 0) {
+                check_inuse_chunk(m, newp);
+                mem = chunk2mem(newp);
+            }
+            else {
+                mem = internal_malloc(m, bytes);
+                if (mem != 0) {
+                    size_t oc = chunksize(oldp) - overhead_for(oldp);
+                    memcpy(mem, oldmem, (oc < bytes) ? oc : bytes);
+                    internal_free(m, oldmem);
+                }
+            }
         }
-      }
     }
-  }
-  return mem;
+    return mem;
 }
 
 void* dlrealloc_in_place(void* oldmem, size_t bytes) {
-  void* mem = 0;
-  if (oldmem != 0) {
-    if (bytes >= MAX_REQUEST) {
-      MALLOC_FAILURE_ACTION;
-    }
-    else {
-      size_t nb = request2size(bytes);
-      mchunkptr oldp = mem2chunk(oldmem);
+    void* mem = 0;
+    if (oldmem != 0) {
+        if (bytes >= MAX_REQUEST) {
+            MALLOC_FAILURE_ACTION;
+        }
+        else {
+            size_t nb = request2size(bytes);
+            mchunkptr oldp = mem2chunk(oldmem);
 #if ! FOOTERS
-      mstate m = gm;
+            mstate m = gm;
 #else /* FOOTERS */
-      mstate m = get_mstate_for(oldp);
-      if (!ok_magic(m)) {
-        USAGE_ERROR_ACTION(m, oldmem);
-        return 0;
-      }
+            mstate m = get_mstate_for(oldp);
+            if (!ok_magic(m)) {
+                USAGE_ERROR_ACTION(m, oldmem);
+                return 0;
+            }
 #endif /* FOOTERS */
-      if (!PREACTION(m)) {
-        mchunkptr newp = try_realloc_chunk(m, oldp, nb, 0);
-        POSTACTION(m);
-        if (newp == oldp) {
-          check_inuse_chunk(m, newp);
-          mem = oldmem;
+            if (!PREACTION(m)) {
+                mchunkptr newp = try_realloc_chunk(m, oldp, nb, 0);
+                POSTACTION(m);
+                if (newp == oldp) {
+                    check_inuse_chunk(m, newp);
+                    mem = oldmem;
+                }
+            }
         }
-      }
     }
-  }
-  return mem;
+    return mem;
 }
 
 void* dlmemalign(size_t alignment, size_t bytes) {
-  if (alignment <= MALLOC_ALIGNMENT) {
-    return dlmalloc(bytes);
-  }
-  return internal_memalign(gm, alignment, bytes);
+    if (alignment <= MALLOC_ALIGNMENT) {
+        return dlmalloc(bytes);
+    }
+    return internal_memalign(gm, alignment, bytes);
 }
 
 int dlposix_memalign(void** pp, size_t alignment, size_t bytes) {
-  void* mem = 0;
-  if (alignment == MALLOC_ALIGNMENT)
-    mem = dlmalloc(bytes);
-  else {
-    size_t d = alignment / sizeof(void*);
-    size_t r = alignment % sizeof(void*);
-    if (r != 0 || d == 0 || (d & (d-SIZE_T_ONE)) != 0)
-      return EINVAL;
-    else if (bytes <= MAX_REQUEST - alignment) {
-      if (alignment <  MIN_CHUNK_SIZE)
-        alignment = MIN_CHUNK_SIZE;
-      mem = internal_memalign(gm, alignment, bytes);
+    void* mem = 0;
+    if (alignment == MALLOC_ALIGNMENT)
+        mem = dlmalloc(bytes);
+    else {
+        size_t d = alignment / sizeof(void*);
+        size_t r = alignment % sizeof(void*);
+        if (r != 0 || d == 0 || (d & (d - SIZE_T_ONE)) != 0)
+            return EINVAL;
+        else if (bytes <= MAX_REQUEST - alignment) {
+            if (alignment < MIN_CHUNK_SIZE)
+                alignment = MIN_CHUNK_SIZE;
+            mem = internal_memalign(gm, alignment, bytes);
+        }
+    }
+    if (mem == 0)
+        return ENOMEM;
+    else {
+        *pp = mem;
+        return 0;
     }
-  }
-  if (mem == 0)
-    return ENOMEM;
-  else {
-    *pp = mem;
-    return 0;
-  }
 }
 
 void* dlvalloc(size_t bytes) {
-  size_t pagesz;
-  ensure_initialization();
-  pagesz = mparams.page_size;
-  return dlmemalign(pagesz, bytes);
+    size_t pagesz;
+    ensure_initialization();
+    pagesz = mparams.page_size;
+    return dlmemalign(pagesz, bytes);
 }
 
 void* dlpvalloc(size_t bytes) {
-  size_t pagesz;
-  ensure_initialization();
-  pagesz = mparams.page_size;
-  return dlmemalign(pagesz, (bytes + pagesz - SIZE_T_ONE) & ~(pagesz - SIZE_T_ONE));
+    size_t pagesz;
+    ensure_initialization();
+    pagesz = mparams.page_size;
+    return dlmemalign(pagesz, (bytes + pagesz - SIZE_T_ONE) & ~(pagesz - SIZE_T_ONE));
 }
 
 void** dlindependent_calloc(size_t n_elements, size_t elem_size,
-                            void* chunks[]) {
-  size_t sz = elem_size; /* serves as 1-element array */
-  return ialloc(gm, n_elements, &sz, 3, chunks);
+    void* chunks[]) {
+    size_t sz = elem_size; /* serves as 1-element array */
+    return ialloc(gm, n_elements, &sz, 3, chunks);
 }
 
 void** dlindependent_comalloc(size_t n_elements, size_t sizes[],
-                              void* chunks[]) {
-  return ialloc(gm, n_elements, sizes, 0, chunks);
+    void* chunks[]) {
+    return ialloc(gm, n_elements, sizes, 0, chunks);
 }
 
 size_t dlbulk_free(void* array[], size_t nelem) {
-  return internal_bulk_free(gm, array, nelem);
+    return internal_bulk_free(gm, array, nelem);
 }
 
 #if MALLOC_INSPECT_ALL
-void dlmalloc_inspect_all(void(*handler)(void *start,
-                                         void *end,
-                                         size_t used_bytes,
-                                         void* callback_arg),
-                          void* arg) {
-  ensure_initialization();
-  if (!PREACTION(gm)) {
-    internal_inspect_all(gm, handler, arg);
-    POSTACTION(gm);
-  }
+void dlmalloc_inspect_all(void(*handler)(void* start,
+    void* end,
+    size_t used_bytes,
+    void* callback_arg),
+    void* arg) {
+    ensure_initialization();
+    if (!PREACTION(gm)) {
+        internal_inspect_all(gm, handler, arg);
+        POSTACTION(gm);
+    }
 }
 #endif /* MALLOC_INSPECT_ALL */
 
 int dlmalloc_trim(size_t pad) {
-  int result = 0;
-  ensure_initialization();
-  if (!PREACTION(gm)) {
-    result = sys_trim(gm, pad);
-    POSTACTION(gm);
-  }
-  return result;
+    int result = 0;
+    ensure_initialization();
+    if (!PREACTION(gm)) {
+        result = sys_trim(gm, pad);
+        POSTACTION(gm);
+    }
+    return result;
 }
 
 size_t dlmalloc_footprint(void) {
-  return gm->footprint;
+    return gm->footprint;
 }
 
 size_t dlmalloc_max_footprint(void) {
-  return gm->max_footprint;
+    return gm->max_footprint;
 }
 
 size_t dlmalloc_footprint_limit(void) {
-  size_t maf = gm->footprint_limit;
-  return maf == 0 ? MAX_SIZE_T : maf;
+    size_t maf = gm->footprint_limit;
+    return maf == 0 ? MAX_SIZE_T : maf;
 }
 
 size_t dlmalloc_set_footprint_limit(size_t bytes) {
-  size_t result;  /* invert sense of 0 */
-  if (bytes == 0)
-    result = granularity_align(1); /* Use minimal size */
-  if (bytes == MAX_SIZE_T)
-    result = 0;                    /* disable */
-  else
-    result = granularity_align(bytes);
-  return gm->footprint_limit = result;
+    size_t result;  /* invert sense of 0 */
+    if (bytes == 0)
+        result = granularity_align(1); /* Use minimal size */
+    if (bytes == MAX_SIZE_T)
+        result = 0;                    /* disable */
+    else
+        result = granularity_align(bytes);
+    return gm->footprint_limit = result;
 }
 
 #if !NO_MALLINFO
 struct mallinfo dlmallinfo(void) {
-  return internal_mallinfo(gm);
+    return internal_mallinfo(gm);
 }
 #endif /* NO_MALLINFO */
 
 #if !NO_MALLOC_STATS
 void dlmalloc_stats() {
-  internal_malloc_stats(gm);
+    internal_malloc_stats(gm);
 }
 #endif /* NO_MALLOC_STATS */
 
 int dlmallopt(int param_number, int value) {
-  return change_mparam(param_number, value);
+    return change_mparam(param_number, value);
 }
 
 size_t dlmalloc_usable_size(void* mem) {
-  if (mem != 0) {
-    mchunkptr p = mem2chunk(mem);
-    if (is_inuse(p))
-      return chunksize(p) - overhead_for(p);
-  }
-  return 0;
+    if (mem != 0) {
+        mchunkptr p = mem2chunk(mem);
+        if (is_inuse(p))
+            return chunksize(p) - overhead_for(p);
+    }
+    return 0;
 }
 
 #endif /* !ONLY_MSPACES */
@@ -5392,99 +5443,100 @@ size_t dlmalloc_usable_size(void* mem) {
 #if MSPACES
 
 static mstate init_user_mstate(char* tbase, size_t tsize) {
-  size_t msize = pad_request(sizeof(struct malloc_state));
-  mchunkptr mn;
-  mchunkptr msp = align_as_chunk(tbase);
-  mstate m = (mstate)(chunk2mem(msp));
-  memset(m, 0, msize);
-  (void)INITIAL_LOCK(&m->mutex);
-  msp->head = (msize|INUSE_BITS);
-  m->seg.base = m->least_addr = tbase;
-  m->seg.size = m->footprint = m->max_footprint = tsize;
-  m->magic = mparams.magic;
-  m->release_checks = MAX_RELEASE_CHECK_RATE;
-  m->mflags = mparams.default_mflags;
-  m->extp = 0;
-  m->exts = 0;
-  disable_contiguous(m);
-  init_bins(m);
-  mn = next_chunk(mem2chunk(m));
-  init_top(m, mn, (size_t)((tbase + tsize) - (char*)mn) - TOP_FOOT_SIZE);
-  check_top_chunk(m, m->top);
-  return m;
+    size_t msize = pad_request(sizeof(struct malloc_state));
+    mchunkptr mn;
+    mchunkptr msp = align_as_chunk(tbase);
+    mstate m = (mstate)(chunk2mem(msp));
+    memset(m, 0, msize);
+    (void)INITIAL_LOCK(&m->mutex);
+    msp->head = (msize | INUSE_BITS);
+    m->seg.base = m->least_addr = tbase;
+    m->seg.size = m->footprint = m->max_footprint = tsize;
+    m->magic = mparams.magic;
+    m->release_checks = MAX_RELEASE_CHECK_RATE;
+    m->mflags = mparams.default_mflags;
+    m->extp = 0;
+    m->exts = 0;
+    disable_contiguous(m);
+    init_bins(m);
+    mn = next_chunk(mem2chunk(m));
+    init_top(m, mn, (size_t)((tbase + tsize) - (char*)mn) - TOP_FOOT_SIZE);
+    check_top_chunk(m, m->top);
+    return m;
 }
 
 mspace create_mspace(size_t capacity, int locked) {
-  mstate m = 0;
-  size_t msize;
-  ensure_initialization();
-  msize = pad_request(sizeof(struct malloc_state));
-  if (capacity < (size_t) -(msize + TOP_FOOT_SIZE + mparams.page_size)) {
-    size_t rs = ((capacity == 0)? mparams.granularity :
-                 (capacity + TOP_FOOT_SIZE + msize));
-    size_t tsize = granularity_align(rs);
-    char* tbase = (char*)(CALL_MMAP(tsize));
-    if (tbase != CMFAIL) {
-      m = init_user_mstate(tbase, tsize);
-      m->seg.sflags = USE_MMAP_BIT;
-      set_lock(m, locked);
+    mstate m = 0;
+    size_t msize;
+    ensure_initialization();
+    msize = pad_request(sizeof(struct malloc_state));
+    if (capacity < (size_t)-(msize + TOP_FOOT_SIZE + mparams.page_size)) {
+        size_t rs = ((capacity == 0) ? mparams.granularity :
+            (capacity + TOP_FOOT_SIZE + msize));
+        size_t tsize = granularity_align(rs);
+        char* tbase = (char*)(CALL_MMAP(tsize));
+        if (tbase != CMFAIL) {
+            m = init_user_mstate(tbase, tsize);
+            m->seg.sflags = USE_MMAP_BIT;
+            set_lock(m, locked);
+        }
     }
-  }
-  return (mspace)m;
+    return (mspace)m;
 }
 
 mspace create_mspace_with_base(void* base, size_t capacity, int locked) {
-  mstate m = 0;
-  size_t msize;
-  ensure_initialization();
-  msize = pad_request(sizeof(struct malloc_state));
-  if (capacity > msize + TOP_FOOT_SIZE &&
-      capacity < (size_t) -(msize + TOP_FOOT_SIZE + mparams.page_size)) {
-    m = init_user_mstate((char*)base, capacity);
-    m->seg.sflags = EXTERN_BIT;
-    set_lock(m, locked);
-  }
-  return (mspace)m;
+    mstate m = 0;
+    size_t msize;
+    ensure_initialization();
+    msize = pad_request(sizeof(struct malloc_state));
+    if (capacity > msize + TOP_FOOT_SIZE &&
+        capacity < (size_t)-(msize + TOP_FOOT_SIZE + mparams.page_size)) {
+        m = init_user_mstate((char*)base, capacity);
+        m->seg.sflags = EXTERN_BIT;
+        set_lock(m, locked);
+    }
+    return (mspace)m;
 }
 
 int mspace_track_large_chunks(mspace msp, int enable) {
-  int ret = 0;
-  mstate ms = (mstate)msp;
-  if (!PREACTION(ms)) {
-    if (!use_mmap(ms)) {
-      ret = 1;
-    }
-    if (!enable) {
-      enable_mmap(ms);
-    } else {
-      disable_mmap(ms);
+    int ret = 0;
+    mstate ms = (mstate)msp;
+    if (!PREACTION(ms)) {
+        if (!use_mmap(ms)) {
+            ret = 1;
+        }
+        if (!enable) {
+            enable_mmap(ms);
+        }
+        else {
+            disable_mmap(ms);
+        }
+        POSTACTION(ms);
     }
-    POSTACTION(ms);
-  }
-  return ret;
+    return ret;
 }
 
 size_t destroy_mspace(mspace msp) {
-  size_t freed = 0;
-  mstate ms = (mstate)msp;
-  if (ok_magic(ms)) {
-    msegmentptr sp = &ms->seg;
-    (void)DESTROY_LOCK(&ms->mutex); /* destroy before unmapped */
-    while (sp != 0) {
-      char* base = sp->base;
-      size_t size = sp->size;
-      flag_t flag = sp->sflags;
-      (void)base; /* placate people compiling -Wunused-variable */
-      sp = sp->next;
-      if ((flag & USE_MMAP_BIT) && !(flag & EXTERN_BIT) &&
-          CALL_MUNMAP(base, size) == 0)
-        freed += size;
+    size_t freed = 0;
+    mstate ms = (mstate)msp;
+    if (ok_magic(ms)) {
+        msegmentptr sp = &ms->seg;
+        (void)DESTROY_LOCK(&ms->mutex); /* destroy before unmapped */
+        while (sp != 0) {
+            char* base = sp->base;
+            size_t size = sp->size;
+            flag_t flag = sp->sflags;
+            (void)base; /* placate people compiling -Wunused-variable */
+            sp = sp->next;
+            if ((flag & USE_MMAP_BIT) && !(flag & EXTERN_BIT) &&
+                CALL_MUNMAP(base, size) == 0)
+                freed += size;
+        }
     }
-  }
-  else {
-    USAGE_ERROR_ACTION(ms,ms);
-  }
-  return freed;
+    else {
+        USAGE_ERROR_ACTION(ms, ms);
+    }
+    return freed;
 }
 
 /*
@@ -5493,476 +5545,476 @@ size_t destroy_mspace(mspace msp) {
 */
 
 void* mspace_malloc(mspace msp, size_t bytes) {
-  mstate ms = (mstate)msp;
-  if (!ok_magic(ms)) {
-    USAGE_ERROR_ACTION(ms,ms);
-    return 0;
-  }
-  if (!PREACTION(ms)) {
-    void* mem;
-    size_t nb;
-    if (bytes <= MAX_SMALL_REQUEST) {
-      bindex_t idx;
-      binmap_t smallbits;
-      nb = (bytes < MIN_REQUEST)? MIN_CHUNK_SIZE : pad_request(bytes);
-      idx = small_index(nb);
-      smallbits = ms->smallmap >> idx;
-
-      if ((smallbits & 0x3U) != 0) { /* Remainderless fit to a smallbin. */
-        mchunkptr b, p;
-        idx += ~smallbits & 1;       /* Uses next bin if idx empty */
-        b = smallbin_at(ms, idx);
-        p = b->fd;
-        assert(chunksize(p) == small_index2size(idx));
-        unlink_first_small_chunk(ms, b, p, idx);
-        set_inuse_and_pinuse(ms, p, small_index2size(idx));
-        mem = chunk2mem(p);
-        check_malloced_chunk(ms, mem, nb);
-        goto postaction;
-      }
+    mstate ms = (mstate)msp;
+    if (!ok_magic(ms)) {
+        USAGE_ERROR_ACTION(ms, ms);
+        return 0;
+    }
+    if (!PREACTION(ms)) {
+        void* mem;
+        size_t nb;
+        if (bytes <= MAX_SMALL_REQUEST) {
+            bindex_t idx;
+            binmap_t smallbits;
+            nb = (bytes < MIN_REQUEST) ? MIN_CHUNK_SIZE : pad_request(bytes);
+            idx = small_index(nb);
+            smallbits = ms->smallmap >> idx;
+
+            if ((smallbits & 0x3U) != 0) { /* Remainderless fit to a smallbin. */
+                mchunkptr b, p;
+                idx += ~smallbits & 1;       /* Uses next bin if idx empty */
+                b = smallbin_at(ms, idx);
+                p = b->fd;
+                assert(chunksize(p) == small_index2size(idx));
+                unlink_first_small_chunk(ms, b, p, idx);
+                set_inuse_and_pinuse(ms, p, small_index2size(idx));
+                mem = chunk2mem(p);
+                check_malloced_chunk(ms, mem, nb);
+                goto postaction;
+            }
 
-      else if (nb > ms->dvsize) {
-        if (smallbits != 0) { /* Use chunk in next nonempty smallbin */
-          mchunkptr b, p, r;
-          size_t rsize;
-          bindex_t i;
-          binmap_t leftbits = (smallbits << idx) & left_bits(idx2bit(idx));
-          binmap_t leastbit = least_bit(leftbits);
-          compute_bit2idx(leastbit, i);
-          b = smallbin_at(ms, i);
-          p = b->fd;
-          assert(chunksize(p) == small_index2size(i));
-          unlink_first_small_chunk(ms, b, p, i);
-          rsize = small_index2size(i) - nb;
-          /* Fit here cannot be remainderless if 4byte sizes */
-          if (SIZE_T_SIZE != 4 && rsize < MIN_CHUNK_SIZE)
-            set_inuse_and_pinuse(ms, p, small_index2size(i));
-          else {
-            set_size_and_pinuse_of_inuse_chunk(ms, p, nb);
-            r = chunk_plus_offset(p, nb);
-            set_size_and_pinuse_of_free_chunk(r, rsize);
-            replace_dv(ms, r, rsize);
-          }
-          mem = chunk2mem(p);
-          check_malloced_chunk(ms, mem, nb);
-          goto postaction;
+            else if (nb > ms->dvsize) {
+                if (smallbits != 0) { /* Use chunk in next nonempty smallbin */
+                    mchunkptr b, p, r;
+                    size_t rsize;
+                    bindex_t i;
+                    binmap_t leftbits = (smallbits << idx) & left_bits(idx2bit(idx));
+                    binmap_t leastbit = least_bit(leftbits);
+                    compute_bit2idx(leastbit, i);
+                    b = smallbin_at(ms, i);
+                    p = b->fd;
+                    assert(chunksize(p) == small_index2size(i));
+                    unlink_first_small_chunk(ms, b, p, i);
+                    rsize = small_index2size(i) - nb;
+                    /* Fit here cannot be remainderless if 4byte sizes */
+                    if (SIZE_T_SIZE != 4 && rsize < MIN_CHUNK_SIZE)
+                        set_inuse_and_pinuse(ms, p, small_index2size(i));
+                    else {
+                        set_size_and_pinuse_of_inuse_chunk(ms, p, nb);
+                        r = chunk_plus_offset(p, nb);
+                        set_size_and_pinuse_of_free_chunk(r, rsize);
+                        replace_dv(ms, r, rsize);
+                    }
+                    mem = chunk2mem(p);
+                    check_malloced_chunk(ms, mem, nb);
+                    goto postaction;
+                }
+
+                else if (ms->treemap != 0 && (mem = tmalloc_small(ms, nb)) != 0) {
+                    check_malloced_chunk(ms, mem, nb);
+                    goto postaction;
+                }
+            }
         }
-
-        else if (ms->treemap != 0 && (mem = tmalloc_small(ms, nb)) != 0) {
-          check_malloced_chunk(ms, mem, nb);
-          goto postaction;
+        else if (bytes >= MAX_REQUEST)
+            nb = MAX_SIZE_T; /* Too big to allocate. Force failure (in sys alloc) */
+        else {
+            nb = pad_request(bytes);
+            if (ms->treemap != 0 && (mem = tmalloc_large(ms, nb)) != 0) {
+                check_malloced_chunk(ms, mem, nb);
+                goto postaction;
+            }
         }
-      }
-    }
-    else if (bytes >= MAX_REQUEST)
-      nb = MAX_SIZE_T; /* Too big to allocate. Force failure (in sys alloc) */
-    else {
-      nb = pad_request(bytes);
-      if (ms->treemap != 0 && (mem = tmalloc_large(ms, nb)) != 0) {
-        check_malloced_chunk(ms, mem, nb);
-        goto postaction;
-      }
-    }
 
-    if (nb <= ms->dvsize) {
-      size_t rsize = ms->dvsize - nb;
-      mchunkptr p = ms->dv;
-      if (rsize >= MIN_CHUNK_SIZE) { /* split dv */
-        mchunkptr r = ms->dv = chunk_plus_offset(p, nb);
-        ms->dvsize = rsize;
-        set_size_and_pinuse_of_free_chunk(r, rsize);
-        set_size_and_pinuse_of_inuse_chunk(ms, p, nb);
-      }
-      else { /* exhaust dv */
-        size_t dvs = ms->dvsize;
-        ms->dvsize = 0;
-        ms->dv = 0;
-        set_inuse_and_pinuse(ms, p, dvs);
-      }
-      mem = chunk2mem(p);
-      check_malloced_chunk(ms, mem, nb);
-      goto postaction;
-    }
+        if (nb <= ms->dvsize) {
+            size_t rsize = ms->dvsize - nb;
+            mchunkptr p = ms->dv;
+            if (rsize >= MIN_CHUNK_SIZE) { /* split dv */
+                mchunkptr r = ms->dv = chunk_plus_offset(p, nb);
+                ms->dvsize = rsize;
+                set_size_and_pinuse_of_free_chunk(r, rsize);
+                set_size_and_pinuse_of_inuse_chunk(ms, p, nb);
+            }
+            else { /* exhaust dv */
+                size_t dvs = ms->dvsize;
+                ms->dvsize = 0;
+                ms->dv = 0;
+                set_inuse_and_pinuse(ms, p, dvs);
+            }
+            mem = chunk2mem(p);
+            check_malloced_chunk(ms, mem, nb);
+            goto postaction;
+        }
 
-    else if (nb < ms->topsize) { /* Split top */
-      size_t rsize = ms->topsize -= nb;
-      mchunkptr p = ms->top;
-      mchunkptr r = ms->top = chunk_plus_offset(p, nb);
-      r->head = rsize | PINUSE_BIT;
-      set_size_and_pinuse_of_inuse_chunk(ms, p, nb);
-      mem = chunk2mem(p);
-      check_top_chunk(ms, ms->top);
-      check_malloced_chunk(ms, mem, nb);
-      goto postaction;
-    }
+        else if (nb < ms->topsize) { /* Split top */
+            size_t rsize = ms->topsize -= nb;
+            mchunkptr p = ms->top;
+            mchunkptr r = ms->top = chunk_plus_offset(p, nb);
+            r->head = rsize | PINUSE_BIT;
+            set_size_and_pinuse_of_inuse_chunk(ms, p, nb);
+            mem = chunk2mem(p);
+            check_top_chunk(ms, ms->top);
+            check_malloced_chunk(ms, mem, nb);
+            goto postaction;
+        }
 
-    mem = sys_alloc(ms, nb);
+        mem = sys_alloc(ms, nb);
 
-  postaction:
-    POSTACTION(ms);
-    return mem;
-  }
+    postaction:
+        POSTACTION(ms);
+        return mem;
+    }
 
-  return 0;
+    return 0;
 }
 
 void mspace_free(mspace msp, void* mem) {
-  if (mem != 0) {
-    mchunkptr p  = mem2chunk(mem);
+    if (mem != 0) {
+        mchunkptr p = mem2chunk(mem);
 #if FOOTERS
-    mstate fm = get_mstate_for(p);
-    (void)msp; /* placate people compiling -Wunused */
+        mstate fm = get_mstate_for(p);
+        (void)msp; /* placate people compiling -Wunused */
 #else /* FOOTERS */
-    mstate fm = (mstate)msp;
+        mstate fm = (mstate)msp;
 #endif /* FOOTERS */
-    if (!ok_magic(fm)) {
-      USAGE_ERROR_ACTION(fm, p);
-      return;
-    }
-    if (!PREACTION(fm)) {
-      check_inuse_chunk(fm, p);
-      if (RTCHECK(ok_address(fm, p) && ok_inuse(p))) {
-        size_t psize = chunksize(p);
-        mchunkptr next = chunk_plus_offset(p, psize);
-        if (!pinuse(p)) {
-          size_t prevsize = p->prev_foot;
-          if (is_mmapped(p)) {
-            psize += prevsize + MMAP_FOOT_PAD;
-            if (CALL_MUNMAP((char*)p - prevsize, psize) == 0)
-              fm->footprint -= psize;
-            goto postaction;
-          }
-          else {
-            mchunkptr prev = chunk_minus_offset(p, prevsize);
-            psize += prevsize;
-            p = prev;
-            if (RTCHECK(ok_address(fm, prev))) { /* consolidate backward */
-              if (p != fm->dv) {
-                unlink_chunk(fm, p, prevsize);
-              }
-              else if ((next->head & INUSE_BITS) == INUSE_BITS) {
-                fm->dvsize = psize;
-                set_free_with_pinuse(p, psize, next);
-                goto postaction;
-              }
-            }
-            else
-              goto erroraction;
-          }
+        if (!ok_magic(fm)) {
+            USAGE_ERROR_ACTION(fm, p);
+            return;
         }
-
-        if (RTCHECK(ok_next(p, next) && ok_pinuse(next))) {
-          if (!cinuse(next)) {  /* consolidate forward */
-            if (next == fm->top) {
-              size_t tsize = fm->topsize += psize;
-              fm->top = p;
-              p->head = tsize | PINUSE_BIT;
-              if (p == fm->dv) {
-                fm->dv = 0;
-                fm->dvsize = 0;
-              }
-              if (should_trim(fm, tsize))
-                sys_trim(fm, 0);
-              goto postaction;
-            }
-            else if (next == fm->dv) {
-              size_t dsize = fm->dvsize += psize;
-              fm->dv = p;
-              set_size_and_pinuse_of_free_chunk(p, dsize);
-              goto postaction;
+        if (!PREACTION(fm)) {
+            check_inuse_chunk(fm, p);
+            if (RTCHECK(ok_address(fm, p) && ok_inuse(p))) {
+                size_t psize = chunksize(p);
+                mchunkptr next = chunk_plus_offset(p, psize);
+                if (!pinuse(p)) {
+                    size_t prevsize = p->prev_foot;
+                    if (is_mmapped(p)) {
+                        psize += prevsize + MMAP_FOOT_PAD;
+                        if (CALL_MUNMAP((char*)p - prevsize, psize) == 0)
+                            fm->footprint -= psize;
+                        goto postaction;
+                    }
+                    else {
+                        mchunkptr prev = chunk_minus_offset(p, prevsize);
+                        psize += prevsize;
+                        p = prev;
+                        if (RTCHECK(ok_address(fm, prev))) { /* consolidate backward */
+                            if (p != fm->dv) {
+                                unlink_chunk(fm, p, prevsize);
+                            }
+                            else if ((next->head & INUSE_BITS) == INUSE_BITS) {
+                                fm->dvsize = psize;
+                                set_free_with_pinuse(p, psize, next);
+                                goto postaction;
+                            }
+                        }
+                        else
+                            goto erroraction;
+                    }
+                }
+
+                if (RTCHECK(ok_next(p, next) && ok_pinuse(next))) {
+                    if (!cinuse(next)) {  /* consolidate forward */
+                        if (next == fm->top) {
+                            size_t tsize = fm->topsize += psize;
+                            fm->top = p;
+                            p->head = tsize | PINUSE_BIT;
+                            if (p == fm->dv) {
+                                fm->dv = 0;
+                                fm->dvsize = 0;
+                            }
+                            if (should_trim(fm, tsize))
+                                sys_trim(fm, 0);
+                            goto postaction;
+                        }
+                        else if (next == fm->dv) {
+                            size_t dsize = fm->dvsize += psize;
+                            fm->dv = p;
+                            set_size_and_pinuse_of_free_chunk(p, dsize);
+                            goto postaction;
+                        }
+                        else {
+                            size_t nsize = chunksize(next);
+                            psize += nsize;
+                            unlink_chunk(fm, next, nsize);
+                            set_size_and_pinuse_of_free_chunk(p, psize);
+                            if (p == fm->dv) {
+                                fm->dvsize = psize;
+                                goto postaction;
+                            }
+                        }
+                    }
+                    else
+                        set_free_with_pinuse(p, psize, next);
+
+                    if (is_small(psize)) {
+                        insert_small_chunk(fm, p, psize);
+                        check_free_chunk(fm, p);
+                    }
+                    else {
+                        tchunkptr tp = (tchunkptr)p;
+                        insert_large_chunk(fm, tp, psize);
+                        check_free_chunk(fm, p);
+                        if (--fm->release_checks == 0)
+                            release_unused_segments(fm);
+                    }
+                    goto postaction;
+                }
             }
-            else {
-              size_t nsize = chunksize(next);
-              psize += nsize;
-              unlink_chunk(fm, next, nsize);
-              set_size_and_pinuse_of_free_chunk(p, psize);
-              if (p == fm->dv) {
-                fm->dvsize = psize;
-                goto postaction;
-              }
-            }
-          }
-          else
-            set_free_with_pinuse(p, psize, next);
-
-          if (is_small(psize)) {
-            insert_small_chunk(fm, p, psize);
-            check_free_chunk(fm, p);
-          }
-          else {
-            tchunkptr tp = (tchunkptr)p;
-            insert_large_chunk(fm, tp, psize);
-            check_free_chunk(fm, p);
-            if (--fm->release_checks == 0)
-              release_unused_segments(fm);
-          }
-          goto postaction;
+        erroraction:
+            USAGE_ERROR_ACTION(fm, p);
+        postaction:
+            POSTACTION(fm);
         }
-      }
-    erroraction:
-      USAGE_ERROR_ACTION(fm, p);
-    postaction:
-      POSTACTION(fm);
     }
-  }
 }
 
 void* mspace_calloc(mspace msp, size_t n_elements, size_t elem_size) {
-  void* mem;
-  size_t req = 0;
-  mstate ms = (mstate)msp;
-  if (!ok_magic(ms)) {
-    USAGE_ERROR_ACTION(ms,ms);
-    return 0;
-  }
-  if (n_elements != 0) {
-    req = n_elements * elem_size;
-    if (((n_elements | elem_size) & ~(size_t)0xffff) &&
-        (req / n_elements != elem_size))
-      req = MAX_SIZE_T; /* force downstream failure on overflow */
-  }
-  mem = internal_malloc(ms, req);
-  if (mem != 0 && calloc_must_clear(mem2chunk(mem)))
-    memset(mem, 0, req);
-  return mem;
+    void* mem;
+    size_t req = 0;
+    mstate ms = (mstate)msp;
+    if (!ok_magic(ms)) {
+        USAGE_ERROR_ACTION(ms, ms);
+        return 0;
+    }
+    if (n_elements != 0) {
+        req = n_elements * elem_size;
+        if (((n_elements | elem_size) & ~(size_t)0xffff) &&
+            (req / n_elements != elem_size))
+            req = MAX_SIZE_T; /* force downstream failure on overflow */
+    }
+    mem = internal_malloc(ms, req);
+    if (mem != 0 && calloc_must_clear(mem2chunk(mem)))
+        memset(mem, 0, req);
+    return mem;
 }
 
 void* mspace_realloc(mspace msp, void* oldmem, size_t bytes) {
-  void* mem = 0;
-  if (oldmem == 0) {
-    mem = mspace_malloc(msp, bytes);
-  }
-  else if (bytes >= MAX_REQUEST) {
-    MALLOC_FAILURE_ACTION;
-  }
+    void* mem = 0;
+    if (oldmem == 0) {
+        mem = mspace_malloc(msp, bytes);
+    }
+    else if (bytes >= MAX_REQUEST) {
+        MALLOC_FAILURE_ACTION;
+    }
 #ifdef REALLOC_ZERO_BYTES_FREES
-  else if (bytes == 0) {
-    mspace_free(msp, oldmem);
-  }
+    else if (bytes == 0) {
+        mspace_free(msp, oldmem);
+    }
 #endif /* REALLOC_ZERO_BYTES_FREES */
-  else {
-    size_t nb = request2size(bytes);
-    mchunkptr oldp = mem2chunk(oldmem);
+    else {
+        size_t nb = request2size(bytes);
+        mchunkptr oldp = mem2chunk(oldmem);
 #if ! FOOTERS
-    mstate m = (mstate)msp;
+        mstate m = (mstate)msp;
 #else /* FOOTERS */
-    mstate m = get_mstate_for(oldp);
-    if (!ok_magic(m)) {
-      USAGE_ERROR_ACTION(m, oldmem);
-      return 0;
-    }
+        mstate m = get_mstate_for(oldp);
+        if (!ok_magic(m)) {
+            USAGE_ERROR_ACTION(m, oldmem);
+            return 0;
+        }
 #endif /* FOOTERS */
-    if (!PREACTION(m)) {
-      mchunkptr newp = try_realloc_chunk(m, oldp, nb, 1);
-      POSTACTION(m);
-      if (newp != 0) {
-        check_inuse_chunk(m, newp);
-        mem = chunk2mem(newp);
-      }
-      else {
-        mem = mspace_malloc(m, bytes);
-        if (mem != 0) {
-          size_t oc = chunksize(oldp) - overhead_for(oldp);
-          memcpy(mem, oldmem, (oc < bytes)? oc : bytes);
-          mspace_free(m, oldmem);
+        if (!PREACTION(m)) {
+            mchunkptr newp = try_realloc_chunk(m, oldp, nb, 1);
+            POSTACTION(m);
+            if (newp != 0) {
+                check_inuse_chunk(m, newp);
+                mem = chunk2mem(newp);
+            }
+            else {
+                mem = mspace_malloc(m, bytes);
+                if (mem != 0) {
+                    size_t oc = chunksize(oldp) - overhead_for(oldp);
+                    memcpy(mem, oldmem, (oc < bytes) ? oc : bytes);
+                    mspace_free(m, oldmem);
+                }
+            }
         }
-      }
     }
-  }
-  return mem;
+    return mem;
 }
 
 void* mspace_realloc_in_place(mspace msp, void* oldmem, size_t bytes) {
-  void* mem = 0;
-  if (oldmem != 0) {
-    if (bytes >= MAX_REQUEST) {
-      MALLOC_FAILURE_ACTION;
-    }
-    else {
-      size_t nb = request2size(bytes);
-      mchunkptr oldp = mem2chunk(oldmem);
+    void* mem = 0;
+    if (oldmem != 0) {
+        if (bytes >= MAX_REQUEST) {
+            MALLOC_FAILURE_ACTION;
+        }
+        else {
+            size_t nb = request2size(bytes);
+            mchunkptr oldp = mem2chunk(oldmem);
 #if ! FOOTERS
-      mstate m = (mstate)msp;
+            mstate m = (mstate)msp;
 #else /* FOOTERS */
-      mstate m = get_mstate_for(oldp);
-      (void)msp; /* placate people compiling -Wunused */
-      if (!ok_magic(m)) {
-        USAGE_ERROR_ACTION(m, oldmem);
-        return 0;
-      }
+            mstate m = get_mstate_for(oldp);
+            (void)msp; /* placate people compiling -Wunused */
+            if (!ok_magic(m)) {
+                USAGE_ERROR_ACTION(m, oldmem);
+                return 0;
+            }
 #endif /* FOOTERS */
-      if (!PREACTION(m)) {
-        mchunkptr newp = try_realloc_chunk(m, oldp, nb, 0);
-        POSTACTION(m);
-        if (newp == oldp) {
-          check_inuse_chunk(m, newp);
-          mem = oldmem;
+            if (!PREACTION(m)) {
+                mchunkptr newp = try_realloc_chunk(m, oldp, nb, 0);
+                POSTACTION(m);
+                if (newp == oldp) {
+                    check_inuse_chunk(m, newp);
+                    mem = oldmem;
+                }
+            }
         }
-      }
     }
-  }
-  return mem;
+    return mem;
 }
 
 void* mspace_memalign(mspace msp, size_t alignment, size_t bytes) {
-  mstate ms = (mstate)msp;
-  if (!ok_magic(ms)) {
-    USAGE_ERROR_ACTION(ms,ms);
-    return 0;
-  }
-  if (alignment <= MALLOC_ALIGNMENT)
-    return mspace_malloc(msp, bytes);
-  return internal_memalign(ms, alignment, bytes);
+    mstate ms = (mstate)msp;
+    if (!ok_magic(ms)) {
+        USAGE_ERROR_ACTION(ms, ms);
+        return 0;
+    }
+    if (alignment <= MALLOC_ALIGNMENT)
+        return mspace_malloc(msp, bytes);
+    return internal_memalign(ms, alignment, bytes);
 }
 
 void** mspace_independent_calloc(mspace msp, size_t n_elements,
-                                 size_t elem_size, void* chunks[]) {
-  size_t sz = elem_size; /* serves as 1-element array */
-  mstate ms = (mstate)msp;
-  if (!ok_magic(ms)) {
-    USAGE_ERROR_ACTION(ms,ms);
-    return 0;
-  }
-  return ialloc(ms, n_elements, &sz, 3, chunks);
+    size_t elem_size, void* chunks[]) {
+    size_t sz = elem_size; /* serves as 1-element array */
+    mstate ms = (mstate)msp;
+    if (!ok_magic(ms)) {
+        USAGE_ERROR_ACTION(ms, ms);
+        return 0;
+    }
+    return ialloc(ms, n_elements, &sz, 3, chunks);
 }
 
 void** mspace_independent_comalloc(mspace msp, size_t n_elements,
-                                   size_t sizes[], void* chunks[]) {
-  mstate ms = (mstate)msp;
-  if (!ok_magic(ms)) {
-    USAGE_ERROR_ACTION(ms,ms);
-    return 0;
-  }
-  return ialloc(ms, n_elements, sizes, 0, chunks);
+    size_t sizes[], void* chunks[]) {
+    mstate ms = (mstate)msp;
+    if (!ok_magic(ms)) {
+        USAGE_ERROR_ACTION(ms, ms);
+        return 0;
+    }
+    return ialloc(ms, n_elements, sizes, 0, chunks);
 }
 
 size_t mspace_bulk_free(mspace msp, void* array[], size_t nelem) {
-  return internal_bulk_free((mstate)msp, array, nelem);
+    return internal_bulk_free((mstate)msp, array, nelem);
 }
 
 #if MALLOC_INSPECT_ALL
 void mspace_inspect_all(mspace msp,
-                        void(*handler)(void *start,
-                                       void *end,
-                                       size_t used_bytes,
-                                       void* callback_arg),
-                        void* arg) {
-  mstate ms = (mstate)msp;
-  if (ok_magic(ms)) {
-    if (!PREACTION(ms)) {
-      internal_inspect_all(ms, handler, arg);
-      POSTACTION(ms);
+    void(*handler)(void* start,
+        void* end,
+        size_t used_bytes,
+        void* callback_arg),
+    void* arg) {
+    mstate ms = (mstate)msp;
+    if (ok_magic(ms)) {
+        if (!PREACTION(ms)) {
+            internal_inspect_all(ms, handler, arg);
+            POSTACTION(ms);
+        }
+    }
+    else {
+        USAGE_ERROR_ACTION(ms, ms);
     }
-  }
-  else {
-    USAGE_ERROR_ACTION(ms,ms);
-  }
 }
 #endif /* MALLOC_INSPECT_ALL */
 
 int mspace_trim(mspace msp, size_t pad) {
-  int result = 0;
-  mstate ms = (mstate)msp;
-  if (ok_magic(ms)) {
-    if (!PREACTION(ms)) {
-      result = sys_trim(ms, pad);
-      POSTACTION(ms);
+    int result = 0;
+    mstate ms = (mstate)msp;
+    if (ok_magic(ms)) {
+        if (!PREACTION(ms)) {
+            result = sys_trim(ms, pad);
+            POSTACTION(ms);
+        }
     }
-  }
-  else {
-    USAGE_ERROR_ACTION(ms,ms);
-  }
-  return result;
+    else {
+        USAGE_ERROR_ACTION(ms, ms);
+    }
+    return result;
 }
 
 #if !NO_MALLOC_STATS
 void mspace_malloc_stats(mspace msp) {
-  mstate ms = (mstate)msp;
-  if (ok_magic(ms)) {
-    internal_malloc_stats(ms);
-  }
-  else {
-    USAGE_ERROR_ACTION(ms,ms);
-  }
+    mstate ms = (mstate)msp;
+    if (ok_magic(ms)) {
+        internal_malloc_stats(ms);
+    }
+    else {
+        USAGE_ERROR_ACTION(ms, ms);
+    }
 }
 #endif /* NO_MALLOC_STATS */
 
 size_t mspace_footprint(mspace msp) {
-  size_t result = 0;
-  mstate ms = (mstate)msp;
-  if (ok_magic(ms)) {
-    result = ms->footprint;
-  }
-  else {
-    USAGE_ERROR_ACTION(ms,ms);
-  }
-  return result;
+    size_t result = 0;
+    mstate ms = (mstate)msp;
+    if (ok_magic(ms)) {
+        result = ms->footprint;
+    }
+    else {
+        USAGE_ERROR_ACTION(ms, ms);
+    }
+    return result;
 }
 
 size_t mspace_max_footprint(mspace msp) {
-  size_t result = 0;
-  mstate ms = (mstate)msp;
-  if (ok_magic(ms)) {
-    result = ms->max_footprint;
-  }
-  else {
-    USAGE_ERROR_ACTION(ms,ms);
-  }
-  return result;
+    size_t result = 0;
+    mstate ms = (mstate)msp;
+    if (ok_magic(ms)) {
+        result = ms->max_footprint;
+    }
+    else {
+        USAGE_ERROR_ACTION(ms, ms);
+    }
+    return result;
 }
 
 size_t mspace_footprint_limit(mspace msp) {
-  size_t result = 0;
-  mstate ms = (mstate)msp;
-  if (ok_magic(ms)) {
-    size_t maf = ms->footprint_limit;
-    result = (maf == 0) ? MAX_SIZE_T : maf;
-  }
-  else {
-    USAGE_ERROR_ACTION(ms,ms);
-  }
-  return result;
+    size_t result = 0;
+    mstate ms = (mstate)msp;
+    if (ok_magic(ms)) {
+        size_t maf = ms->footprint_limit;
+        result = (maf == 0) ? MAX_SIZE_T : maf;
+    }
+    else {
+        USAGE_ERROR_ACTION(ms, ms);
+    }
+    return result;
 }
 
 size_t mspace_set_footprint_limit(mspace msp, size_t bytes) {
-  size_t result = 0;
-  mstate ms = (mstate)msp;
-  if (ok_magic(ms)) {
-    if (bytes == 0)
-      result = granularity_align(1); /* Use minimal size */
-    if (bytes == MAX_SIZE_T)
-      result = 0;                    /* disable */
-    else
-      result = granularity_align(bytes);
-    ms->footprint_limit = result;
-  }
-  else {
-    USAGE_ERROR_ACTION(ms,ms);
-  }
-  return result;
+    size_t result = 0;
+    mstate ms = (mstate)msp;
+    if (ok_magic(ms)) {
+        if (bytes == 0)
+            result = granularity_align(1); /* Use minimal size */
+        if (bytes == MAX_SIZE_T)
+            result = 0;                    /* disable */
+        else
+            result = granularity_align(bytes);
+        ms->footprint_limit = result;
+    }
+    else {
+        USAGE_ERROR_ACTION(ms, ms);
+    }
+    return result;
 }
 
 #if !NO_MALLINFO
 struct mallinfo mspace_mallinfo(mspace msp) {
-  mstate ms = (mstate)msp;
-  if (!ok_magic(ms)) {
-    USAGE_ERROR_ACTION(ms,ms);
-  }
-  return internal_mallinfo(ms);
+    mstate ms = (mstate)msp;
+    if (!ok_magic(ms)) {
+        USAGE_ERROR_ACTION(ms, ms);
+    }
+    return internal_mallinfo(ms);
 }
 #endif /* NO_MALLINFO */
 
 size_t mspace_usable_size(const void* mem) {
-  if (mem != 0) {
-    mchunkptr p = mem2chunk(mem);
-    if (is_inuse(p))
-      return chunksize(p) - overhead_for(p);
-  }
-  return 0;
+    if (mem != 0) {
+        mchunkptr p = mem2chunk(mem);
+        if (is_inuse(p))
+            return chunksize(p) - overhead_for(p);
+    }
+    return 0;
 }
 
 int mspace_mallopt(int param_number, int value) {
-  return change_mparam(param_number, value);
+    return change_mparam(param_number, value);
 }
 
 #endif /* MSPACES */
@@ -6277,4 +6329,4 @@ History:
       * Based loosely on libg++-1.2X malloc. (It retains some of the overall
          structure of old version,  but most details differ.)
 
-*/
+*/
\ No newline at end of file
